[
    {
        "Title": "A \"Do No Harm\" Novel Safety Checklist and Research Approach to Determine Whether to Launch an Artificial Intelligence-Based Medical Technology: Introducing the Biological-Psychological, Economic, and Social (BPES) Framework",
        "BibtexKey": "khanNoHarmNovel2023",
        "Year": 2023,
        "Overview": "Proposes a novel 3-step framework for helping AI industry developers assess whether medical AI tools are fit for launching, resulting in a binary \"go\" or \"no-go\" answer. The framework is a checklist that considers various stakeholders and addresses a wide range of considerations from biological-psychological, economic, and social aspects.",
        "RQ1": "Broad range of risks are considered including biological-psychological looking at whether the AI medical technology causes physical-mental health benefit or harms, social benefit or harm in terms of access to affordable quality of care, or economic benefit or harm to the stakeholders involved.",
        "RQ2": "Proposes a framework-based checklist on long-standing medical practices for safe AI deployment and operation that covers a wide range of stakeholder considerations.",
        "RQ3": "Very broad coverage of harms and benefits that could apply well out of medical AI but no concrete methods are proposed.",
        "RQ4": "Not evaluated.",
        "Limitations": "The framework relies on a very mature safety background from medial fields that may not be readily adaptable in AI fields. The methods have also not been empirically evaluated and cover a wide range of problems that may make its scope too wide.",
        "Keywords": [
            "framework",
            "deployment",
            "checklist",
            "medical AI",
            "stakeholder-focus",
            "interdisciplinary",
            "theoretical"
        ],
        "AuthorKeywords": ["artificial intelligence", "AI", "safety checklist", "Do No Harm", "biological-psychological factors", "economic factors", "social factors", "AI medical hardware devices", "AI medical mobile apps", "AI medical software programs"],
        "RiskTypes": ["biological", "psychological", "social", "economic"],
        "Stage": ["operation"],
        "Method": "design framework",
        "AuthorAffiliationType": ["university"],
        "AuthorAffiliation": ["University of Toronto, Canada"],
        "Citations": 2
    },
    {
        "Title": "A Concept for Dynamic and Robust Machine Learning with Context Modeling for Heterogeneous Manufacturing Data",
        "BibtexKey": "kammConceptDynamicRobust2023",
        "Year": 2023,
        "Overview": "Proposes an integrated three-level framework to integrate multiple heterogeneous data sources in an industrial setting robustly and dynamically where a learned context model represents the pre-processed and validated data sources uniformly. It proposes various candidate methods to achieve this integration. A further data analytics and user interface layer are then built on this data layer.",
        "RQ1": "Addresses risks stemming from sub-optimal control and management of complex integrated industrial systems during operation time considering aspects such as condition monitoring, anomaly detection and failure analysis.",
        "RQ2": "Proposes a high-level framework for integrating and managing heterogenous sources of data in a safety-critical system.",
        "RQ3": "Focused on ML system driven by heterogeneous data sources, but proposed framework is potentially applicable outside of industrial applications.",
        "RQ4": "Not evaluated.",
        "Limitations": "",
        "Keywords": [
            "framework",
            "machine learning",
            "robustness",
            "heterogeneous data",
            "context modelling",
            "industrial",
            "multi-modal",
            "theoretical"
        ],
        "AuthorKeywords": ["context modeling", "dynamic and robust machine learning", "heterogeneous data", "machine learning"],
        "RiskTypes": ["heterogeneous data", "multi-modal data"],
        "Stage": ["operation"],
        "Method": "design framework",
        "AuthorAffiliation": ["University of Stuttgart, Germany"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3
    },
    {
        "Title": "A Contact-Safe Reinforcement Learning Framework for Contact-Rich Robot Manipulation",
        "BibtexKey": "zhuContactSafeReinforcementLearning2022",
        "Year": 2022,
        "Overview": "Proposes concrete method based on RL for contact-safe robust actuation in content-rich tasks where the agent may cause collisions when exploring the environment, or in unseen scenarios. The method applies to both task space safety (robot-target object contact) and joint space safety (robot-human contact). They use an image-based RL algorithm for contact aware force reduction with control-theoretical constraints.",
        "RQ1": "Addresses the physical risk of an ill-calibrated or exploring RL policy in contact-rich physical environments.",
        "RQ2": "Propose a concrete method that integrates vision-based RL with control theory to train a contact-aware force controller to reduce contact forces.",
        "RQ3": "The method they propose is an instantiation of a more general framework of the integration between RL and theoretical control systems.",
        "RQ4": "Trained in simulation and evaluated in a real-world wiping task with a robot arm that shows the method can keep contact forces low even in unexpected scenarios for both task and joint space safety.",
        "Limitations": "Evaluation only focuses on a wiping task and there are no details on how exactly the more general framework should be applied in other settings.",
        "Keywords": [
            "reinforcement learning",
            "robot manipulation", 
            "control theory",
            "real-world evaluation", 
            "collision avoidance",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["physical"],
        "Stage": ["operation"],
        "Method": "real-world testing",
        "AuthorAffiliation": ["Tsinghua University, China"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 2
    },
    {
        "Title": "A Deep Reinforcement Learning Framework with Formal Verification",
        "BibtexKey": "boudiDeepReinforcementLearning2023",
        "Year": 2023,
        "Overview": "The authors propose a framework called Multi State-Actor (MuStAc) which integrates the decentralised training of individual deep RL agents with a set of formally verifiable action rules, then followed by the centralised training of a single, global agent. The model predicts valid career paths for people respecting formal requirements that are aimed at improving the reliability of the system.",
        "RQ1": "Addresses risks stemming from both the deployment and operation of the RL system, wherein biases may be encoded in the system without the formal verification and during operation this may lead to wrong decisions.",
        "RQ2": "The proposed system uses a verifiable action space to mitigate the risks.",
        "RQ3": "The proposed framework may be implemented outside of the designated application domain of human-resource management but it is unclear and not discussed how to do so.",
        "RQ4": "Small demonstration evaluation with 5 actions and 25 agents shows that the model can converge to an optimal policy.",
        "Limitations": "It is unclear how the model scales with more actions and agents and how it handles domain adaptation. It is also not measured in any way how the proposed formal actions mitigates risks from bias compared to a baseline model.",
        "Keywords": [
            "reinforcement learning",
            "human resource management",
            "framework",
            "applied",
            "formal methods"
        ],
        "AuthorKeywords": ["model transformation", "event-b", "atelier b", "safe RL", "safe AI", "formal verification", "ai control"],
        "RiskTypes": ["social"],
        "Stage": ["deployment", "operation"],
        "Method": "simulated agents",
        "AuthorAffiliationType": ["university"],
        "AuthorAffiliation": ["Mohammed V University, Morocco"],
        "Citations": 4
    },
    {
        "Title": "A Deep Safe Reinforcement Learning Approach for Mapless Navigation",
        "BibtexKey": "lvDeepSafeReinforcement2021",
        "Year": 2021,
        "Overview": "The authors propose a concrete safe deep RL approach to mapless robot navigation for reducing collisions while maintaining success rates. They use constrained policy optimisation (CPO) and build their architecture called Actor-Critic-Safety (ACS) in which they extend the standard Actor-Critic (AC) framework with an additional safety critic to produce safety constraints for the actor. Their results show decreased collision rates and similar success rates to map-based navigation baselines.",
        "RQ1": "The physical risk of collisions is addressed stemming from an inaccurate and unsafe trained policy.",
        "RQ2": "An extended AC framework is proposed with an additional safety critique to improve the safety criteria in CPO.",
        "RQ3": "ACS is described independent of the applied domain so it could be easily applied in other domains as well.",
        "RQ4": "Evaluated in dynamic environments with pedestrians that show improved collision rates and similar-to-baseline success rates.",
        "Limitations": "The authors claim that the method generalises to unseen scenarios but the system was not tested in different RL environments and the description of the safety critic is very limited and reliant on a neural network so it is unclear whether the system would generalise well.",
        "Keywords": [
            "reinforcement learning",
            "robot navigation",
            "constrained policy optimisation",
            "simulation",
            "applied"
        ],
        "AuthorKeywords": ["mapless navigation", "safe reinforcement learning", "acs architecture", "dynamic environment with pedestrians"],
        "RiskTypes": ["physical"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Harbin Institute of Technology, China"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 3
    },
    {
        "Title": "A Deployment Model to Extend Ethically Aligned AI Implementation Method ECCOLA",
        "BibtexKey": "antikainenDeploymentModelExtend2021",
        "Year": 2021,
        "Overview": "Extends an existing ethical guidelines framework ECCOLA with a deployment model to enable its application and adoption in practice.",
        "RQ1": "The paper focuses on the ethical risks stemming from potentially every part of the AI software design pipeline but particularly from the design stage.",
        "RQ2": "The proposed extension to the ECCOLA framework provides an actionable deployment model with 4 stages proposing actual steps to address issues such as stakeholder involvement, ethical risk assessment, use cases, etc.",
        "RQ3": "The proposed method is very general and may be applied in any AI product development pipeline, but it is especially suited for agile methods.",
        "RQ4": "Not evaluated.",
        "Limitations": "",
        "Keywords": [
            "framework",
            "ethics",
            "theoretical",
            "deployment",
            "software engineering"
        ],
        "AuthorKeywords": ["artificial intelligence", "AI ethics", "ECCOLA", "software engineering", "product lifecycle", "adoption model"],
        "RiskTypes": ["ethical"],
        "Stage": ["design", "deployment", "operation"],
        "Method": "design framework",
        "AuthorAffiliation": ["University of Jyväskylä, Finland"],
        "AuthorAffiliationType": ["university"],
        "Citations": 2
    },
    {
        "Title": "A Dirichlet Process Mixture of Robust Task Models for Scalable Lifelong Reinforcement Learning",
        "BibtexKey": "wangDirichletProcessMixture2022",
        "Year": 2022,
        "Overview": "The paper combines a Dirichlet process prior over task distributions with domain randomisation for robust parameter training to address the problem of continual reinforcement learning. They use deep deterministic policy gradient (DDPG) to train their system.",
        "RQ1": "The paper addresses risks stemming from failed domain and task adaptation in nonstationary task distributions due to continual learning and the dilemma of stability vs plasticity.",
        "RQ2": "A concrete algorithm is proposed to address the issues of domain adaptation.",
        "RQ3": "The algorithm is proposed for DDPG but the underlying ideas do not rely on any specific training algorithm for RL.",
        "RQ4": "The system is evaluated for robot navigation and robot arm manipulation and shows much improved performance as both compared to baselines and in ablation studies.",
        "Limitations": "It is unclear how, if at all, the task distributions were qualitatively different during training.",
        "Keywords": [
            "reinforcement learning",
            "continual learning",
            "domain adaptation",
            "robustness",
            "Dirichlet process",
            "applied"
        ],
        "AuthorKeywords": ["adaptation models", "bayes methods", "chinese restaurant process (crp)", "dirichlet process mixture", "domain randomization", "expectation maximization (em)", "knowledge engineering", "lifelong reinforcement learning (rl)", "mathematical models", "q-learning", "task analysis", "thermal stability"],
        "RiskTypes": ["catastrophic forgetting", "stability-plasticity"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Nanjing University, China", "University of New South Wales, Australia"],
        "AuthorAffiliationType": ["university"],
        "Citations": 7
    },
    {
        "Title": "A Discrepancy Aware Framework for Robust Anomaly Detection",
        "BibtexKey": "caiDiscrepancyAwareFramework2023",
        "Year": 2023,
        "Overview": "A modified self-supervised architecture is proposed to detect anomalies in images in primarily industrial settings while alleviating the sensitivity of self-supervised networks to synthetic images. The method replaces the encoder with a teacher-student network which generates a discrepancy map to then guide the decoder that segments the image to identify the anomaly.",
        "RQ1": "Addresses risks stemming from lack of data in anomaly detection and overreliance on synthetic data points during the deployment process, especially training.",
        "RQ2": "Proposes concrete self-supervised neural architecture to address the risks.",
        "RQ3": "The method is applied to anomaly detection which is a broad and important issue and there are no specific assumptions about the input data source.",
        "RQ4": "Evaluated on two benchmark datasets that show significant improvements over baseline methods.",
        "Limitations": "Self-supervised training is generally more time costly than supervised learning and it is not clear how the learned representations would work for out-of-distribution inputs.",
        "Keywords": [
            "convolutional neural network",
            "self-supervised learning",
            "anomaly detection",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["anomaly detection", "artificial intelligence", "data models", "head", "image reconstruction", "robustness", "self-supervised learning", "synthetic data", "training"],
        "RiskTypes": ["anomaly detection", "synthetic data poisoning"],
        "Stage": ["deployment", "operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Huazhong University of Science and Technology, China", "Huazhong Agriculture University, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1
    },
    {
        "Title": "A Framework for Building Uncertainty Wrappers for AI/ML-Based Data-Driven Components",
        "BibtexKey": "klasFrameworkBuildingUncertainty2020",
        "Year": 2020,
        "Overview": "They propose a model-agnostic uncertainty wrapper for data-driven models to quantify uncertainty in ML predictions dependably based on a mathematical measure of uncertainty called Brier score.",
        "RQ1": "Risks stemming from uncertain predictions and a lack of understanding of uncertain predictions and downstream erroneous choices are targeted.",
        "RQ2": "A conceptual uncertainty wrapper framework is proposed to produce uncertainty estimates of ML models.",
        "RQ3": "The proposed architecture is ML model agnostic.",
        "RQ4": "They demonstrate the framework for pedestrian detection on simulated image data which produces interpretable uncertainty estimates.",
        "Limitations": "The evaluation is rather limited and more actionable guidance on applying the framework could be beneficial for the adoption of the framework.",
        "Keywords": [
            "framework",
            "machine learning",
            "uncertainty estimates",
            "model agnostic",
            "theoretical"
        ],
        "AuthorKeywords": ["artificial intelligence", "data quality", "dependability", "machine learning", "operational design domain", "out-of-distribution", "safety engineering"],
        "RiskTypes": ["uncertainty"],
        "Stage": ["deployment", "operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Fraunhofer Institute for Experimental Software Engineering, Germany"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 23
    },
    {
        "Title": "A Hierarchical HAZOP-Like Safety Analysis for Learning-Enabled Systems",
        "BibtexKey": "qiHierarchicalHAZOPLikeSafety2022",
        "Year": 2022,
        "Overview": "Adaptation of the Hazard and Operability Analysis (HAZOP) framework for ML-enabled systems by dividing an ML system into three levels (internal, lifecycle, system) with additional sublevels called nodes that target more specific parameters. They also propose a Bayesian network-based model to quantitatively model the causal relationships between each level, hazards, and security threats. ",
        "RQ1": "No risks are directly addressed but rather provides a prescriptive and holistic view to mitigate a range of categories of risks stemming from various sources across the entire ecosystem of an ML system.",
        "RQ2": "A prescriptive and actionable hazard analysis framework is proposed to proactively try to identify and mitigate several categories of risk sources.",
        "RQ3": "The framework is very generally applicable to ML system development.",
        "RQ4": "Not evaluated.",
        "Limitations": "No real-world evaluation is given on whether the proposed framework is effective at mitigating risks or not.",
        "Keywords": [
            "framework",
            "hazard analysis",
            "machine learning",
            "software engineering",
            "theoretical",
            "risk prevention"
        ],
        "AuthorKeywords": ["AI safety", "autonomous underwater vehicle", "cyber physical system", "deviation analysis", "hazard identification", "HAZOP", "learning-enabled system", "machine learning security", "robotics and autonomous system", "safety analysis", "trustworthy AI"],
        "RiskTypes": ["holistic"],
        "Stage": ["design", "deployment", "operation"],
        "Method": "analysis framework",
        "AuthorAffiliationType": ["university", "corporate"],
        "AuthorAffiliation": ["University of Liverpool, UK", "Adelard Part of NCC Group, UK"],
        "Citations": 9
    },
    {
        "Title": "A Maturity Model for Trustworthy AI Software Development",
        "BibtexKey": "choMaturityModelTrustworthy2023",
        "Year": 2023,
        "Overview": "Propose an ML-focused maturity model to assess and measure the risks to fairness in ML systems systematically and to provide actionable countermeasures to the identified risks. This is done along two dimensions, the Process dimension which relates to the design. deployment, and operation processes themselves, and the Capability dimension which relates to the achieved capabilities of the system as a result of the various Processes. ",
        "RQ1": "The framework addresses risks to fairness.",
        "RQ2": "A theoretical maturity framework is proposed to address fairness risks in ML systems.",
        "RQ3": "The framework is completely ML system agnostic.",
        "RQ4": "Evaluation is extensive, being applied to 13 different AI systems.",
        "Limitations": "While the framework is extensive and is shown to have a good coverage and discovery rate of various fairness-related risks, it is perhaps too complex which could hinder people's willingness to adopt it and the question of enforcing the framework's use is discussed in the paper explicitly.",
        "Keywords": [
            "framework",
            "theoretical",
            "maturity model",
            "software engineering",
            "machine learning",
            "risk prevention"
        ],
        "AuthorKeywords": ["trustworthy AI", "maturity model", "fairness", "safety", "practical guide"],
        "RiskTypes": ["holistic"],
        "Stage": ["design", "deployment", "operation"],
        "Method": "analysis framework",
        "AuthorAffiliationType": ["university", "big-tech corporate"],
        "AuthorAffiliation": ["Sungkyunkwan University, Republic of Korea", "Samsung Research, Republic of Korea"],
        "Citations": 2
    },
    {
        "Title": "A Methodology for Evaluating the Robustness of Anomaly Detectors to Adversarial Attacks in Industrial Scenarios",
        "BibtexKey": "peralesgomezMethodologyEvaluatingRobustness2022",
        "Year": 2022,
        "Overview": "Propose a four-step adversarial dataset generation method to improve the adversarial robustness of LSTM networks for anomaly detection in industrial processes. Evaluation is on a complex industrial process which shows a much increased resilience against adversarial samples.",
        "RQ1": "Risks stemming from misclassification errors in anomaly detection due to adversarial samples are targeted, focusing on the deployment stage.",
        "RQ2": "Concrete framework and implementation is proposed for anomaly detection.",
        "RQ3": "General framework for adversarial dataset creation that does not rely on industrial-specific context.",
        "RQ4": "Evaluated in practice on the Tennessee Eastman process shows robustness improvements.",
        "Limitations": "Lack of qualitative examples to give insight into why the model improves performance. More methods of adversarial attacks could also have been explored.",
        "Keywords": [
            "framework",
            "applied",
            "anomaly detection",
            "robustness",
            "industrial",
            "adversarial attack"
        ],
        "AuthorKeywords": ["adversarial attacks", "deep learning", "evasion attacks", "industrial control systems", "machine learning", "robustness"],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["deployment"],
        "Method": "dataset",
        "AuthorAffiliationType": ["university", "research institution"],
        "AuthorAffiliation": ["University of Murcia, Spain", "École Polytechnique Fédérale de Lausanne, Switzerland", "University of Zurich, Switzerland", "Armasuisse Science and Technology, Switzerland"],
        "Citations": 1
    },
    {
        "Title": "A Model Selection Approach for Corruption Robust Reinforcement Learning",
        "BibtexKey": "weiModelSelectionApproach2022",
        "Year": 2022,
        "Overview": "A model selection algorithm for finite-horizon tabular and linear MDPs is proposed to give a theoretical upper bound on the amount of regret stemming from adversarial corruptions in RL both in the reward and the transition function. Two algorithms are proposed to improve the bound.",
        "RQ1": "Risks from adversarial corruptions of the reward and transition functions are addressed which is especially relevant during the deployment stage.",
        "RQ2": "Concrete algorithms and theoretical results are given.",
        "RQ3": "Results apply to finite-horizon tabular and linear MDPs but may be extended to MDPs with general function approximations.",
        "RQ4": "Theoretical results are proven but no empirical evaluation.",
        "Limitations": "No empirical evaluation. It is also unclear how qualitatively useful in practice the derived bounds are.",
        "Keywords": [
            "reinforcement learning",
            "adversarial attack",
            "theoretical",
            "algorithm",
            "robustness"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack", "reward signal corruption", "transition function corruption"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliationType": ["university", "big-tech corporate"],
        "AuthorAffiliation": ["University of Southern California, United States", "Google Research, United States"],
        "Citations": 30
    },
    {
        "Title": "A Multi-layered Collaborative Framework for Evidence-driven Data Requirements Engineering for Machine Learning-based Safety-critical Systems",
        "BibtexKey": "deyMultilayeredCollaborativeFramework2023",
        "Year": 2023,
        "Overview": "Proposes a three-layered requirements elicitation framework for problem, data, and model identification and assessment. Produces relevant artefacts with stakeholder inputs that assess and verify the quality and applicability of data especially in safety-critical data-driven applications. The evaluation of autonomous vehicle pedestrian detection dataset shows improved understanding and confidence in data requirements.",
        "RQ1": "Addresses various risks stemming from incorrect or inadequate problem, data, and model requirements understanding during the development stage.",
        "RQ2": "Proposes very detailed general framework for design requirements elicitation for any safety-critical data-driven systems.",
        "RQ3": "The proposed framework is very method-agnostic and high-level.",
        "RQ4": "Preliminary evaluation on autonomous vehicle pedestrian detection shows improved design requirements understanding.",
        "Limitations": "Much more evaluation is needed and more actionable recommendations are needed for the contents of artefacts that would make this framework more readily adaptable in real-world settings.",
        "Keywords": [
            "framework",
            "method-agnostic",
            "theoretical",
            "stakeholder-focus",
            "requirements engineering",
            "safety-critical system",
            "data requirements",
            "problem requirements",
            "model requirements"
        ],
        "AuthorKeywords": ["data requirements", "machine learning", "reliability", "safety", "uncertainty"],
        "RiskTypes": ["data requirements", "problem requirements", "model requirements"],
        "Stage": ["design"],
        "Method": "design framework",
        "AuthorAffiliationType": ["university"],
        "AuthorAffiliation": ["Ajou University, Republic of Korea"],
        "Citations": 1
    },
    {
        "Title": "A multi-level semantic web for hard-to-specify domain concept, Pedestrian, in ML-based software",
        "BibtexKey": "barzaminiMultilevelSemanticWeb2022",
        "Year": 2022,
        "Overview": "A workflow is proposed to partially specify hard-to-specify concepts for learning using semantic graphs. The workflow is used to then augment training data for downstream tasks, thereby combining the interpretability of semantic graphs with the inductive learning of hard-to-specify concepts from data. The method largely relies on online searches, domain knowledge of the developers, and manual annotation to derive the semantic graph.",
        "RQ1": "The risk of concept misspecification and uninterpretable representations are addressed in the paper for the deployment stage.",
        "RQ2": "A concrete workflow is proposed to combine semantic webs with datasets for augmented and more interpretable concept specifications.",
        "RQ3": "The proposed workflow is model-agnostic.",
        "RQ4": "Evaluated for autonomous driving and specifically for specifying the concept of \"pedestrian\".",
        "Limitations": "Very focused evaluation and more examples or case studies are needed to show the benefits of the framework. It is also unclear how the gained interpretability trades-off with the apparently large manual labour of creating the semantic graph.",
        "Keywords": [
            "framework",
            "applied",
            "semantic web",
            "data augmentation",
            "concept specification",
            "interpretability"
        ],
        "AuthorKeywords": ["requirements specification", "machine learning", "machine-learned components", "safety-critical systems"],
        "RiskTypes": ["transparency", "data requirements"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Northern Illinois University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 13
    },
    {
        "Title": "A Near-Optimal Algorithm for Safe Reinforcement Learning Under Instantaneous Hard Constraints",
        "BibtexKey": "shiNearOptimalAlgorithmSafe2023",
        "Year": 2023,
        "Overview": "Proposes algorithm for safe RL with episodic MDPs which satisfies instantaneous hard constraints on actions while achieving near-optimal regret as compared to unsafe baselines.",
        "RQ1": "Addresses risks from unsafe states and actions during operation.",
        "RQ2": "Proposes near-optimal RL algorithm for episodic MDPs proving theoretical guarantees about its regret bounds.",
        "RQ3": "Generally applicable algorithm for episodic MDPs is proposed.",
        "RQ4": "Several theoretical results and some empirical evaluation shows the working of the algorithm.",
        "Limitations": "Unclear how regret scales with the number of constraints and more empirical evaluation would be beneficial.",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "constrained MDP",
            "instantaneous hard constraints",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["unsafe actions", "unsafe states"],
        "Stage": ["operation"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["The Ohio State University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1
    },
    {
        "Title": "A Novel Composite Graph Neural Network",
        "BibtexKey": "liuNovelCompositeGraph2023",
        "Year": 2023,
        "Overview": "The authors propose a novel graph learning algorithm combining composite graphs (C-graph) with semi-supervised learning that is demonstrably more robust than baseline methods to data variation. C-graphs characterise sample similarities and encode feature importances and combination preferences.",
        "RQ1": "Potential risks from scare and noisy data are addressed in the graph learning setting.",
        "RQ2": "A concrete and novel graph learning algorithm is proposed.",
        "RQ3": "The method is application-agnostic.",
        "RQ4": "Evaluation on 9 benchmark datasets show improved robustness to data noise.",
        "Limitations": "Higher time-complexity due to learning of feature-relationships and over-smoothing labels.",
        "Keywords": [
            "graph neural network",
            "algorithm",
            "applied",
            "robustness",
            "self-supervised learning"
        ],
        "AuthorKeywords": ["aggregates", "composite graph (c-graph)", "graph neural networks", "graph neural networks (gnns)", "learning (artificial intelligence)", "learning systems", "noise measurement", "prediction methods", "robustness", "sample graph", "tree-based feature graph"],
        "RiskTypes": ["noise"],
        "Stage": ["design", "deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Jilin University, China", "Hunan University, China", "University of Surrey, UK"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 2
    },
    {
        "Title": "A Principal Component Analysis Approach for Embedding Local Symmetries into Deep Learning Algorithms",
        "BibtexKey": "lagravePrincipalComponentAnalysis2020",
        "Year": 2020,
        "Overview": "Propose a PCA-based transformation to the inputs of neural networks to enforce local data symmetry which is shown to empirically improve the robustness of the neural networks outputs.",
        "RQ1": "Addresses the issue of noise in the training data at the design stage.",
        "RQ2": "Proposes an algorithm to improve neural network robustness.",
        "RQ3": "The proposed method is model-agnostic as it only operates on the inputs.",
        "RQ4": "Evaluated on MNIST but it is limited.",
        "Limitations": "Evaluation is very limited and just focusing on MNIST while the motivations of the method are not well justified in the relevant mathematics of topology.",
        "Keywords": [
            "robustness",
            "applied",
            "algorithm",
            "principal component analysis (PCA)",
            "local symmetry"
        ],
        "AuthorKeywords": ["safe machine learning", "robustness-by-design", "model-based engineering", "Lie groups", "data representation"],
        "RiskTypes": ["noise"],
        "Stage": ["design"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Thales Research and Technology, France"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 0
    },
    {
        "Title": "A Real-World Reinforcement Learning Framework for Safe and Human-Like Tactical Decision-Making",
        "BibtexKey": "yavasRealWorldReinforcementLearning2023",
        "Year": 2023,
        "Overview": "They propose a three-level approach to training more general real-world agents using deep RL. This first involves the training of policies in a low-fidelity simulator that is parametrised by real-world driving data and uncertainty. This is followed by a high-level validation phase where the low-fidelity-trained agents are evaluated on hand-crafted and difficult high-fidelity scenarios. Based on data from this, the agent me be further refined in the low-fidelity stage. Finally, the agent is tested in the real world.",
        "RQ1": "Addresses risks from insufficient generalization and lack of domain adaptation in the deployment phase.",
        "RQ2": "Proposes a practical framework based on multiple levels of fidelity in simulation for training more general agents.",
        "RQ3": "The method is designed for autonomous driving but the underlying ideas are generalisable to other domains for deep RL.",
        "RQ4": "Good evaluation on highway driving tasks with promising results.",
        "Limitations": "The double training of agents might get very costly for larger systems and resource usage in this sense was not discussed.",
        "Keywords": [
            "reinforcement learning",
            "deep learning",
            "domain adaptation",
            "generalization",
            "framework",
            "applied"
        ],
        "AuthorKeywords": ["autonomous vehicles", "reinforcement learning", "artificial intelligence", "intelligent vehicles"],
        "RiskTypes": ["generalization"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Istanbul Technical University, Turkey"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "A Reinforcement Learning Architecture That Transfers Knowledge Between Skills When Solving Multiple Tasks",
        "BibtexKey": "tommasinoReinforcementLearningArchitecture2019",
        "Year": 2019,
        "Overview": "Propose a new architecture for continuous state space deep RL and transfer learning called transfer expert RL (TERL). The framework is based on a mixture of experts architecture where a higher-level gating mechanism selects among the experts for both the actor and the critic. The method is largely motivated based on analogies to biology and motor control.",
        "RQ1": "Propose the risk of slow or inadequate domain generalization during deployment and operation.",
        "RQ2": "Proposes a novel deep RL arhcitecture for transfer learning in RL.",
        "RQ3": "Specific method is proposed.",
        "RQ4": "Very extensive evaluation on multiple-jointed robot arm manipulation tasks show much improved across many measures.",
        "Limitations": "",
        "Keywords": [
            "reinforcement learning",
            "continuous state space",
            "transfer learning",
            "mixture of experts",
            "gating mechanism",
            "hierarchical reinforcement learning" 
        ],
        "AuthorKeywords": ["autonomous robotics", "bio-inspired modular neural architecture", "catastrophic interference", "cumulative learning", "functioning and learning responsibility signals", "mixture-of-expert networks", "reaching tasks", "transfer reinforcement learning (trl)"],
        "RiskTypes": ["generalization"],
        "Stage": ["deployment", "operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Nanyang Technological University, Singapore", "Istituto di Scienze e Tecnologie della Cognizione, Italy"],
        "AuthorAffiliationType": ["university"],
        "Citations": 30
    },
    {
        "Title": "A risk degree-based safe semi-supervised learning algorithm",
        "BibtexKey": "ganRiskDegreebasedSafe2016",
        "Year": 2016,
        "Overview": "Proposes an algorithm for safer self-supervised learning based on minimising the kernel mean squared error (KMSE) and the Laplacian KMSE. Estimates risk by using consistency and confidence between the two methods. Shows some marginal improvment over the baselines in empirical evaluations.",
        "RQ1": "Addresses risk stemming from noise or poisoned data in self-supervised learning that may hurt training performance.",
        "RQ2": "Proposes algorithm to improve and quantify training imrpovement degradation due to poisoned data as a surrugate measure of risk.",
        "RQ3": "Model-specific trainign algorithm is proposed.",
        "RQ4": "Limited evaluation on small datasets.",
        "Limitations": "Limited evaluation, unclear motivation, and conflated concepts around safety. No improvement to safety is shown qualitatively.",
        "Keywords": [
            "self-supervised learning",
            "data poisoning",
            "risk estimation",
            "algorithm"
        ],
        "AuthorKeywords": ["semi-supervised learning", "safe mechanism", "risk degree", "kernel minimum squared error"],
        "RiskTypes": ["synthetic data poisoning"],
        "Stage": ["design", "deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Hangzhou Dianzi University, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 12
    },
    {
        "Title": "A Robust Approach To Model-Based Classification Based On Trimming And Constraints: Semi-Supervised Learning In Presence Of Outliers And Label Noise",
        "BibtexKey": "cappozzoRobustApproachModelbased2020",
        "Year": 2020,
        "Overview": "Proposes a machine learning method for robust data classification based on an impartial trimming criterion and constrain criterion from the eigenvalues of the data scatter matrix.",
        "RQ1": "Addresses risk from misclassification due to outliers.",
        "RQ2": "Proposes ML algorithm for robust to outliers supervised learning.",
        "RQ3": "Specific method is proposed which is data-agnostic.",
        "RQ4": "Evaluation on real-world and simulated data shows good improvements over baselines.",
        "Limitations": "Trimming levels add an extra hyperparameter to select and it is unclear how well the method works on higher dimensional data.",
        "Keywords": [
            "machine learning",
            "supervised learning",
            "robustness",
            "outliers",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["model-based classification", "label noise", "outliers detection", "impartial trimming", "eigenvalues restrictions", "robust estimation"],
        "RiskTypes": ["outliers"],
        "Stage": ["design", "deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Milano-Bicocca, Italy", "University College Dublin, Ireland"],
        "AuthorAffiliationType": ["university"],
        "Citations": 19
    },
    {
        "Title": "A Robust Automated Machine Learning System with Pseudoinverse Learning",
        "BibtexKey": "wangRobustAutomatedMachine2021",
        "Year": 2021,
        "Overview": "Proposes an ensemble of neural networks trained with pseudo inverse learning and using a meta-learner that enables data-driven hyper-parameter optimisation. Evaluation shows slightly improved robustness to noise in data.",
        "RQ1": "Risk from noisy data is addressed during training.",
        "RQ2": "Proposes a new neural network architecture.",
        "RQ3": "A model is proposed but it is data-agnostic.",
        "RQ4": "Evaluation on standard benchmark datasets shows somewhat improved classification accuracy on noisy data as compared to baseline.",
        "Limitations": "Non-standard and complicated architecture that is not clear how well it would work on high dimensional data or how it woudl generalise.",
        "Keywords": [
            "algorithm",
            "deep learning",
            "classification",
            "applied",
            "pseudo inverse learning"
        ],
        "AuthorKeywords": ["automated machine learning", "contractive pseudoinverse learners", "deep neural network", "robust learning systems"],
        "RiskTypes": ["noise"],
        "Stage": ["design", "deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["School of Information Engineering, China", "School of Systems Science, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 18
    },
    {
        "Title": "A Robust Double-Parallel Extreme Learning Machine Based On An Improved M-Estimation Algorithm",
        "BibtexKey": "zhaRobustDoubleparallelExtreme2022",
        "Year": 2022,
        "Overview": "Proposes a modification to the standard extreme learning machine (ELM) framework to increase the robust to outliers. The resulting method is called Robust Double-parallel ELM and works by applying an iterative M-estimation step based on reweighted least squares estimation to the output weights after the weights have been trained.",
        "RQ1": "Addresses risks stemming from outliers in data.",
        "RQ2": "Proposes an algorithm to increase robustness of an existing machine learning framework to outliers.",
        "RQ3": "The method is specific for ELMs.",
        "RQ4": "Evaluation on synthetic data and also industrial real-world data.",
        "Limitations": "Model accuracy suffers from the addded algorithm and it increases the optimisation time as well.",
        "Keywords": [
            "algorithm",
            "robustness",
            "outliers",
            "applied",
            "industrial",
            "extreme learning machine"
        ],
        "AuthorKeywords": ["outlier-robust regression", "extreme learning machine (ELM)", "double-parallel", "M-estimation", "coal-fired boiler"],
        "RiskTypes": ["outliers", "noise"],
        "Stage": ["design"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Yanshan University, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3
    },
    {
        "Title": "A Robust Extreme Learning Machine Based on Adaptive Loss Function for Regression Modeling",
        "BibtexKey": "zhangRobustExtremeLearning2023",
        "Year": 2023,
        "Overview": "Proposes a modification to the standard L2-loss function in extreme learning machines (ELM) by replacing it with a tuneable adaptive RBF-style loss function and two hyperparameters and applies the resulting loss function to a Bayesian optimisation setting with ELMs.",
        "RQ1": "Addresses risk from noisy data with outliers by improving robustness",
        "RQ2": "Proposes a modification to the existing ELM framework to improve robustness.",
        "RQ3": "Specific to the ELM model.",
        "RQ4": "Evaluated with synthetic and real-world industrial data. Shows improved robustness and accuracy.",
        "Limitations": "Method might take longer to train.",
        "Keywords": [
            "algorithm",
            "robustness",
            "outliers",
            "applied",
            "industrial",
            "extreme learning machine"
        ],
        "AuthorKeywords": ["adaptive loss function", "Bayesian optimization", "extreme learning machine", "iterative reweighted least squares",  "regression modeling"],
        "RiskTypes": ["outliers", "noise"],
        "Stage": ["design"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Qingdao University of Science and Technology, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "A Robust Extreme Learning Machine Framework For Uncertain Data Classification",
        "BibtexKey": "jingRobustExtremeLearning2020",
        "Year": 2020,
        "Overview": "Reforumlates the extreme learning machine optimisation problem as a second-order cone programming with global optimal solution to improve the robustness of classifications to outliers.",
        "RQ1": "Addresses risk from noisy data with outliers by improving robustness",
        "RQ2": "Proposes a modification to the existing ELM framework to improve robustness.",
        "RQ3": "Specific to the ELM model.",
        "RQ4": "Evaluated with synthetic and real-world industrial data. Shows improved robustness but worse accuracy.",
        "Limitations": "Robustness is at the cost of classification accuracy.",
        "Keywords": [
            "algorithm",
            "robustness",
            "outliers",
            "applied",
            "industrial",
            "extreme learning machine"
        ],
        "AuthorKeywords": ["uncertainty", "extreme learning machine", "probability constraint", "second-order cone programming", "expectation maximization (em) algorithm", "missing data"
        ],
        "RiskTypes": ["noise", "outliers", "missing data"],
        "Stage": ["design"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["China Agricultural University, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 5
    },
    {
        "Title": "A Robust Learning Methodology for Uncertainty-Aware Scientific Machine Learning Models",
        "BibtexKey": "costaRobustLearningMethodology2023",
        "Year": 2023,
        "Overview": "Proposes a 5-stage framework for uncertainty-aware robust scientific machine learning based on an integration of Monte Carlo methods, Bayesian parameter selection, and machine learning, that provides estimates of the uncertainty in the model predictions.",
        "RQ1": "Risks from uncertain predictions are targeted.",
        "RQ2": "Proposes a holistic framework for uncertainty-aware model training in scientific ML.",
        "RQ3": "Proposes an ML model-agnostic method.",
        "RQ4": "Evaluated on the polymerisation reactor system by Hidalgo and Brosilow.",
        "Limitations": "Applied to numerical data prediction but the method might be too computationally expensive for more complex data modalities.",
        "Keywords": [
            "robustness",
            "framework",
            "applied",
            "machine learning",
            "polymerisation reactor"
        ],
        "AuthorKeywords": ["scientific machine learning", "robust learning", "uncertainty", "Markov Chain Monte Carlo"],
        "RiskTypes": ["uncertainty"],
        "Stage": ["design", "deployment"],
        "Method": "design framework",
        "AuthorAffiliation": ["Universidade Federal da Bahia, Brazil", "Norwegian University of Science and Technology, Norway"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1
    },
    {
        "Title": "A Robust Mean-Field Actor-Critic Reinforcement Learning Against Adversarial Perturbations on Agent States",
        "BibtexKey": "zhouRobustMeanFieldActorCritic2023",
        "Year": 2023,
        "Overview": "Proposes a modifaction to the mean-field actor-critic training algorithm for multi-agent RL to increase the method's robustness against adversarial attacks. They design a new objective function with a policy gradient function on clean states and an action loss function of the difference between actions taken on clean and adversarial states. They also apply a repetitive regularization of the action loss.",
        "RQ1": "Addresses risk from adversarial attacks.",
        "RQ2": "Proposes a concrete algorithm to increase robustness against adversarial attacks.",
        "RQ3": "Method specific to mean-field actor-critic algorithm but works in any environment.",
        "RQ4": "Very extensive evaluation on several benchmark tasks shows increased robustness while mainting performance.",
        "Limitations": "",
        "Keywords": [
            "reinforcement learning",
            "algorithm",
            "applied",
            "deep learning",
            "multi-agent RL (MARL)"
        ],
        "AuthorKeywords": ["mean-field actor-critic (MFAC)", "reinforcement learning", "multiagent systems", "robustness", "state-adversarial stochastic game (SASG)"],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Tongji University, China", "New Jersey Institute of Technology, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 7
    },
    {
        "Title": "A Robust Method to Measure the Global Feature Importance of Complex Prediction Models",
        "BibtexKey": "zhangRobustMethodMeasure2021",
        "Year": 2021,
        "Overview": "Proposes a method to determine the global feature importances of a machine learning model based on the ANOVA high-dimensional model representation. They show that their method can robustly identify important features even in the presence of noisy data.",
        "RQ1": "Risk from noisy data is addressed.",
        "RQ2": "Proposes a risk-robust global model-agnostic explainability method for machine learning from tabular data.",
        "RQ3": "Proposed algorithm is model-agnostic.",
        "RQ4": "Evaluated on synthetic data and standard tabular datasets.",
        "Limitations": "Unsure whether it generalises to non-tabular data and no comparisons to other model-agnostic methods.",
        "Keywords": [
            "machine learning",
            "explainable AI",
            "model-agnostic interpretability",
            "robustness"
        ],
        "AuthorKeywords": ["feature importance", "global interpretation", "high-dimensional model representation", "robustness", "supervised machine learning"],
        "RiskTypes": ["noise"],
        "Stage": ["deployment", "operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Beijing University of Posts and Telecommunications, China", "China North Vehicle Research Institute, China"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 3
    },
    {
        "Title": "A Robust Offline Reinforcement Learning Algorithm Based on Behavior Regularization Methods",
        "BibtexKey": "zhangRobustOfflineReinforcement2022",
        "Year": 2022,
        "Overview": "Combines batch deep Q learning with random ensemble mixture algorithm (REM) to improve the performance of REM in data scarce situations.",
        "RQ1": "Addresses risk stemming from data scarce offline reinforcement learning.",
        "RQ2": "Designs three new offline RL archiecture based on batch Q learning and REM.",
        "RQ3": "Specific model is proposed.",
        "RQ4": "Algorithm evaluated on Atari games shows improved performance.",
        "Limitations": "Evaluation is limited to one environment and not every additions were evaluated in the paper.",
        "Keywords": [
            "reinforcement learning",
            "deep learning",
            "offline RL",
            "limited data",
            "batch-constrained deep Q learning",
            "random ensemble mixture"
        ],
        "AuthorKeywords": ["offline deep reinforcement learning", "limited data", "behavior regularization"],
        "RiskTypes": ["limited data"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Northeastern University, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "A Robust Policy Bootstrapping Algorithm For Multi-Objective Reinforcement Learning In Non-Stationary Environments",
        "BibtexKey": "abdelfattahRobustPolicyBootstrapping2020",
        "Year": 2020,
        "Overview": "Proposes a method to improve robustness to preference changes in non-stationary multi-objective optimisation processes that is based on finding a convex set of policies that serve to bootstrap the algorithm when exploring the preference space.",
        "RQ1": "Addresseses risks from non-stationary environments.",
        "RQ2": "Proposes algorithm to optimise policy of a multi-objective MDP (MOMDP) that is also robust to changes to preferences.",
        "RQ3": "An algorithm is proposed that works for all MOMDPs.",
        "RQ4": "Evaluation on three benchmark environments shows greatly improved performance in non-stationary settings.",
        "Limitations": "",
        "Keywords": [
            "reinforcement learning",
            "robustness",
            "multi-objective MDP"
        ],
        "AuthorKeywords": ["multi-objective optimization", "reinforcement learning", "non-stationary", "environment", "dynamics", "policy bootstrapping", "Markov decision processes"],
        "RiskTypes": ["non-stationarity"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["University of New South Wales, Australia"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3
    },
    {
        "Title": "A Robust Supervised Subspace Learning Approach For Output-Relevant Prediction And Detection Against Outliers",
        "BibtexKey": "liRobustSupervisedSubspace2021",
        "Year": 2021,
        "Overview": "Proposes a novel supervised optimisation problem to protect and detect outliers robustly by simulataneously optimising the prediction accuracy and the input reconstruction error loss, separating out clean and outlying samples as part of the optimisation process.",
        "RQ1": "Addresses risk from noisy data with outliers.",
        "RQ2": "Introduces a specific robust latent space learning supervised ML algorithm for robust outlier detection.",
        "RQ3": "A new model is proposed for outlier detection in ML.",
        "RQ4": "Evaluation on synthetic and real benchmark data show improved performance on outlier detection with more interpretable latent space representations as compared to baselines.",
        "Limitations": "Paper doesn't discuss how method performance under data-distribution shift and the interpretability of the latent space could have used more qualitative analysis.",
        "Keywords": [
            "machine learning",
            "algorithm",
            "outlier detection",
            "robustness",
            "optimisation",
            "representation learning",
            "applied"
        ],
        "AuthorKeywords": ["output-related detection", "robust supervised subspace learning (rssl)", "subspace decomposition", "matrix factorization", "industrial system"],
        "RiskTypes": ["noise", "outliers"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["New York University Abu Dhabi, UAE", "New York University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 12
    },
    {
        "Title": "A Robust Unsupervised Feature Learning Framework Using Spatial Boosting Networks",
        "BibtexKey": "leRobustUnsupervisedFeature2013",
        "Year": 2013,
        "Overview": "Proposes an unsupervised representation learning algorithm based on spatial boosting networks and convolutional overlapping pooling to extract features that are more robust to noise, have a smaller number of parameters than baseline methods, and still offer comparative performance.",
        "RQ1": "Risk stemming from brittle representations due to noise is addressed.",
        "RQ2": "A concrete method is proposed to increase the robustness of learned representations.",
        "RQ3": "A specific algorithm is proposed.",
        "RQ4": "Evaluation on benchmark datasets CIFAR-10 and STL-10 show increased robustness at a smaller number of parameters.",
        "Limitations": "Actual accuracies are significantly lower than some supervised baselines.",
        "Keywords": [
            "unsupervised learning",
            "representation learning",
            "spatial boosting network",
            "convolutional pooling",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["unsupervised feature learning", "deep learning", "neural networks"],
        "RiskTypes": ["noise"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Science, Vietnam", "John von Neumann Institute, Vietnam"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 1
    },
    {
        "Title": "A Safe and Self-Recoverable Reinforcement Learning Framework for Autonomous Robots",
        "BibtexKey": "wangSafeSelfRecoverableReinforcement2022",
        "Year": 2022,
        "Overview": "Propose an RL framework for safe and self-recoverable exploration during the learning phase that is based on a safety shield integrated between the agent and the environment.",
        "RQ1": "Risks stemming from exploring unsafe actions and unsafe states in RL are addressed.",
        "RQ2": "An RL learning framework is proposed to add self-recovery to safe states and unsafe action detection.",
        "RQ3": "A learning algorithm-agnostic RL framework is proposed.",
        "RQ4": "Evaluation on maze environments show greatly reduced incidence of unsafe states and actions.",
        "Limitations": "Evaluation is very limited. The proposed method still requires the execution of the current action before the safety shield is activated. In other words, the safety shield is post-hoc.",
        "Keywords": [
            "reinforcement learning",
            "framework",
            "theoretical"
        ],
        "AuthorKeywords": ["safe reinforcement learning", "self-recoverable reinforcement learning", "autonomous robotics"],
        "RiskTypes": ["unsafe states", "unsafe actions"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Yancheg Institute of Technology, China", "Changshu Institute of Technology, China"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 1
    },
    {
        "Title": "A Safety Case Pattern for Systems with Machine Learning Components",
        "BibtexKey": "wozniakSafetyCasePattern2020",
        "Year": 2020,
        "Overview": "Proposes an actionable formal safety framework in goal structuring notation to create a safety case for complex systems with machine learning components.",
        "RQ1": "Addresses risks stemming from every aspect of machine learning.",
        "RQ2": "Proposes a holistic safety framework to verify complex systems with ML components.",
        "RQ3": "A very high-level and general framework is proposed.",
        "RQ4": "Case study on pedestrian recognition in autonomous vehicles demonstrates the utility of the framework well.",
        "Limitations": "Much evaluation is necessary to develop a comprehensive standard.",
        "Keywords": [
            "framework",
            "theoretical",
            "safety-critical system",
            "industrial"
        ],
        "AuthorKeywords": ["machine learning", "safety case", "ISO 26262", "goal structuring notation (GSN)"],
        "RiskTypes": ["holistic", "safety verification", "standardization"],
        "Stage": ["design", "deployment", "operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["fortiss GmbH, Germany"],
        "AuthorAffiliationType": ["corporate"],
        "Citations": 14
    },
    {
        "Title": "A Safety-Critical Decision-Making and Control Framework Combining Machine-Learning-Based and Rule-Based Algorithms",
        "BibtexKey": "aksjonovSafetyCriticalDecisionMakingControl2023",
        "Year": 2023,
        "Overview": "Proposes a mixed learned and formal system for the control of safety-critical systems. The outputs of the controllers are regulated by a formal switching logic that prioritises the safety controller based on formal representations when a particular set of safety constraints are violated while optimising multiple reward components at the same time based on efficiency, safety, and comfort.",
        "RQ1": "Risk from unsafe learned control is addressed.",
        "RQ2": "Proposes a high-level framework for integrating a formal safety logic with a learned controller to profit from both learned and formal representations.",
        "RQ3": "A high-level learning algorithm-agnostic framework is proposed.",
        "RQ4": "Evaluation on a simulated system with vehicle breaking control claims to maximise efficiency and comfort while maintaining high levels of safety.",
        "Limitations": "Limited evaluation on a single domain and the efficacy of the proposed method depends on the largest part on the hand-crafted safety switching mechanism which is a non-trivial component to design.",
        "Keywords": [
            "framework",
            "theoretical",
            "reinforcement learning",
            "formal methods",
            "control theory"
        ],
        "AuthorKeywords": ["decision making", "intelligent control", "machine learning", "artificial intelligence", "rule based systems", "autonomous vehicles", "safety"],
        "RiskTypes": ["safety verification"],
        "Stage": ["deployment", "operation"],
        "Method": "design framework",
        "AuthorAffiliation": ["Aalto University, Finland"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3
    },
    {
        "Title": "A Study On Real-Time Artificial Intelligence",
        "BibtexKey": "tayStudyRealtimeArtificial1998",
        "Year": 1998,
        "Overview": "The paper discusses major issues of modern AI including high complexity, complex or expensive evaluation, unpredictable environmental disturbances, and non-linear systems. While on its own and today it is not groundbreaking, it already indicates that these issues were recognised back in the 90s too.",
        "RQ1": "Risks from many aspects of AI systems are discussed.",
        "RQ2": "Only discussion, no methods are proposed.",
        "RQ3": "NA",
        "RQ4": "NA",
        "Limitations": "NA",
        "Keywords": [
            "theoretical",
            "big picture",
            "issues of AI"
        ],
        "AuthorKeywords": ["real-time AI", "real-time expert systems", "safety in control systems", "real-time control"],
        "RiskTypes": ["holistic"],
        "Stage": ["design"], 
        "Method": "philosophical",
        "AuthorAffiliation": ["National University of Singapore, Singapore"],
        "AuthorAffiliationType": ["university"],
        "Citations": 13
    },
    {
        "Title": "A Validity Perspective on Evaluating the Justified Use of Data-driven Decision-making Algorithms",
        "BibtexKey": "costonValidityPerspectiveEvaluating2023",
        "Year": 2023,
        "Overview": "The paper considers a taxonomy for the actual justifiability of deployment of predictive ML-based models in high-stakes environments. It proposes that validity, value alignment, and reliability should be the three key factors in determining whether a predictive system should be deployed. It then performs a literature review of these concepts and proposes a list of high-level questions to motivate the deliberation of these concepts in a more actionable way in the real world.",
        "RQ1": "Considers risks for a wide range of aspects related to the deployment of ML systems.",
        "RQ2": "Proposes a taxonomy of deliberative concepts from a validity perspective to motivate the justifiability of the deployment of predictive ML systems in high-stakes environments.",
        "RQ3": "A high-level taxonomy and deliberative framework is proposed.",
        "RQ4": "None, but some examples are mentioned.",
        "Limitations": "",
        "Keywords": [
            "framework",
            "taxonomy",
            "theoretical",
            "validity",
            "value alignment",
            "reliability",
            "machine learning"
        ],
        "AuthorKeywords": ["predictive analytics", "validity", "deliberation", "algorithmic oversight", "responsible AI", "algorithmic decision support"],
        "RiskTypes": ["validity", "holistic"],
        "Stage": ["design", "deployment", "operation"],
        "Good": true,
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Carnegie Mellon University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 18
    },
    {
        "Title": "Adaptive Robust Learning Framework For Twin Support Vector Machine Classification",
        "BibtexKey": "maAdaptiveRobustLearning2021",
        "Year": 2021,
        "Overview": "Proposes a new adaptive loss function based on correntropy for twin support vector machines (TSVM) to improve the robustness of TSVMs against outliers and noisy data.",
        "RQ1": "Risk stemming from noisy data or outliers is addressed.",
        "RQ2": "Proposes a modification to the loss function of the TSVM to address noisy data better.",
        "RQ3": "A modification of a concrete classification algorithm is proposed.",
        "RQ4": "Large evaluation on UCI datasets against an extensive range of baselines shows improved robustness while maintaining performance.",
        "Limitations": "Introduces a new hyperparameter that needs selection. Convergence time takes much longer compared to baselines.",
        "Keywords": [
            "machine learning",
            "supervised learning",
            "twin support vector machine",
            "algorithm",
            "applied",
            "robustness",
            "adaptive loss function"
        ],
        "AuthorKeywords": ["robustness", "correntropy", "distance metric", "twin support vector machine", "DC programming"],
        "RiskTypes": ["noise"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Tianjin University, China", "University of Rhode Island, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 20
    },
    {
        "Title": "Addressing Uncertainty In The Safety Assurance Of Machine-Learning",
        "BibtexKey": "burtonAddressingUncertaintySafety2023",
        "Year": 2023,
        "Overview": "Proposes a comprehensive safety assessment framework in goal-structured notation to address risks stemming from uncertain ML predictions. The method is very well motivated by the relevant literature and seem to provide an actionable framework to develop safer ML systems.",
        "RQ1": "Addresses risks stemming from the uncertainty of ML-based predictions.",
        "RQ2": "Clarifies several safety-related definitions and proposes a comprehensive safety assessment framework in goal-structured notation. Also proposes a continuous refinement workflow for ML development based on the safety framework.",
        "RQ3": "A general safety assessment framework is proposed for addressing uncertain prediction of ML-based predictors.",
        "RQ4": "NA",
        "Limitations": "No evaluation of the framework.",
        "Keywords": [
            "framework",
            "theoretical",
            "safety analysis",
            "goal structuring notation (GSN)",
            "iterative development workflow"
        ],
        "AuthorKeywords": ["machine learning", "safety", "assurance arguments", "cyber-physical systems", "uncertainty", "complexity"],
        "RiskTypes": ["holistic", "uncertainty"],
        "Stage": ["design"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Liaoning Shihua University, China", "Northeastern University, China", "The University of Texas at Arlington, United States", "Missouri University of Science and Technology, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 6
    },
    {
        "Title": "Adversarial Robustness of Deep Reinforcement Learning Based Dynamic Recommender Systems",
        "BibtexKey": "wangAdversarialRobustnessDeep2022",
        "Year": 2022,
        "Overview": "Compare multiple adversarial attack types (FGSM, strategically timed, counterfactual) to determine their effectiveness in attacking RL-based recommender systems, then design a recurrent attention-based supervised classifier to predict the presence of an attack.",
        "RQ1": "Risk from adversarial attacks of RL-based recommender systems are investigated.",
        "RQ2": "A supervised temporal attention-based classifier is proposed to detect adversarial attacks.",
        "RQ3": "The classifier is proposed for RL-based recommender systems.",
        "RQ4": "Evaluation shows that the proposed detection method can identify attacks with high F1-score.",
        "Limitations": "It is not investigated how well the model generalises to out-of-distribution data or new forms of attack.",
        "Keywords": [
            "reinforcement learning",
            "recommender system",
            "adversarial attack",
            "adversarial detection",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["recommender systems (RS)", "deep reinforcement learning (deep RL)", "adversarial attack", "robustness", "deep learning", "artificial neural network (DL-ANN)"],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["China Agricultural University, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Adversarial Robustness of Phishing Email Detection Models",
        "BibtexKey": "gholampourAdversarialRobustnessPhishing2023",
        "Year": 2023,
        "Overview": "Create a dataset of mixed phishing and legitimate emails based on existing fake email generating methods and trains a classifier with the new data that achieves 94% accuracy.",
        "RQ1": "Risk stemming from phishing attacks are investigated.",
        "RQ2": "A new dataset for phishing detection is proposed.",
        "RQ3": "A new task-specific dataset is proposed.",
        "RQ4": "Evaluation on benchmarks show 94% accuracy of a trained classifier.",
        "Limitations": "Limited contribution and investigation of effects of the dataset.",
        "Keywords": [
            "phishing email",
            "dataset",
            "applied"
        ],
        "AuthorKeywords": ["phishing/legitimate dataset", "adversarial attacks", "data augmentation", "model robustness", "transformer models", "deep learning", "machine learning", "generative AI", "GPT-2"],
        "RiskTypes": ["adversarial attack", "phishing"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Houston, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 2
    },
    {
        "Title": "Algorithmic Robustness For Semi-Supervised (ε, y, τ)-Good Metric Learning",
        "BibtexKey": "nicolaeAlgorithmicRobustnessSemisupervised2015",
        "Year": 2015,
        "Overview": "Propose a joint metric and classifier semi-supervised learning method based on theoretical robustness guarantees for machine learning.",
        "RQ1": "Risk from label quality and outliers are indirectly addressed.",
        "RQ2": "A theoretically more robust metric and classification is learned.",
        "RQ3": "A specific ML method is proposed.",
        "RQ4": "Evaluation on several UCI datasets for 3-NN classification shows improved and more robust performance.",
        "Limitations": "Evaluation only on single metric learning task (3-NN).",
        "Keywords": [
            "metric learning",
            "kNN classification",
            "machine learning",
            "algorithm",
            "applied",
            "semi-supervised learning"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["metric robustness"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Universite Jean Monnet, France", "Universite Grenoble Alpes, France"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3
    },
    {
        "Title": "Aligning Individual And Collective Welfare In Complex Socio-Technical Systems By Combining Metaheuristics And Reinforcement Learning",
        "BibtexKey": "bazzanAligningIndividualCollective2019",
        "Year": 2019,
        "Overview": "Propose a multi-agent RL (MARL) framework that combines individual agent-level exploitation and a centralised metaheuristic (i.e., genetic algorithm)-based control method to maximise social welfare.",
        "RQ1": "Addresses risks stemming from trading off social-welfare optimisation and individual reward maximisation in an optimal resource allocation task.",
        "RQ2": "Proposes a high-level MARL-based framework combining metaheuristics and individual RL training.",
        "RQ3": "A high-level MARL framework is proposed that is training algorithm-agnostic.",
        "RQ4": "Evaluation on the task of selfish routing shows that the proposed method finds socially stable policies that are more efficient as compared to existing baselines.",
        "Limitations": "More evaluation is necessary and the qualitative effects of the metaheuristic should have been explored in more detail.",
        "Keywords": [
            "framework",
            "reinforcement learning",
            "theoretical",
            "collective welfare",
            "metaheuristics",
            "socio-technical system"
        ],
        "AuthorKeywords": ["complex systems", "socio-technical systems", "multi-agent systems", "multi-agent reinforcement learning", "metaheuristics", "load balance", "route choice", "selfish routing"],
        "RiskTypes": ["scarce resource allocation"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Universidade Federal do Rio Grande do Sul, Brazil"],
        "AuthorAffiliationType": ["university"],
        "Citations": 20
    },
    {
        "Title": "Alignment For Advanced Machine Learning Systems",
        "BibtexKey": "taylorAlignmentAdvancedMachine2020",
        "Year": 2020,
        "Overview": "Reviews long-termist issues of AI alignment based on the works of Bostrom, Russel, and others, identifying eight main research directions: inductive ambiguity identification, robust human imitation, informed oversight, generalizable environmental goals, conservative concepts, impact measures, mild optimization, and averting instrumental incentives.",
        "RQ1": "Risks from ML alignment are reviewed.",
        "RQ2": "No concrete methods are proposed.",
        "RQ3": "NA",
        "RQ4": "NA",
        "Limitations": "",
        "Keywords": [
            "ML alignment",
            "long-term issues",
            "literature review",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["holistic", "alignment"],
        "Stage": ["design", "deployment", "operation"],
        "Method": "philosophical",
        "AuthorAffiliation": ["Machine Intelligence Research Institute, United States"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 112
    },
    {
        "Title": "An Adversarial Perspective on Accuracy, Robustness, Fairness, and Privacy: Multilateral-Tradeoffs in Trustworthy ML",
        "BibtexKey": "gittensAdversarialPerspectiveAccuracy2022",
        "Year": 2022,
        "Overview": "Reviews trustworthy machine learning with a special focus on how existing methods trade off accuracy, fairness, robustness, and privacy.",
        "RQ1": "Focuses on trade-offs among accuracy, fairness, robustness, and privacy in trustworthy ML.",
        "RQ2": "No methods are proposed but the paper advocates for the use of causal models and causal discovery for trustworthy ML.",
        "RQ3": "NA",
        "RQ4": "NA",
        "Limitations": "Review paper that does not propose a concrete method.",
        "Keywords": [
            "literature review",
            "theoretical"
        ],
        "AuthorKeywords": ["security", "adversarial robustness", "privacy", "fairness", "machine learning", "causal models", "causal representation", "trustworthy machine learning", "causal machine learning"],
        "RiskTypes": ["privacy", "fairness", "robustness", "holistic"],
        "Stage": ["design", "deployment", "operation"],
        "Method": "literature review",
        "AuthorAffiliation": ["Rensselaer Polytechnic Institute, United States", "Google, United States"],
        "AuthorAffiliationType": ["research institution", "big-tech corporate"],
        "Citations": 4
    },
    {
        "Title": "An Adversarial Reinforcement Learning Framework for Robust Machine Learning-based Malware Detection",
        "BibtexKey": "ebrahimiAdversarialReinforcementLearning2022",
        "Year": 2022,
        "Overview": "Proposes an iterative minimax game-based classification algorithm for detecting adversarial malware attacks. The game is based on two major steps: 1. finding a sub-optimal adversarial attack policy and 2. robustifying the malware detector model against a fixed attack policy.",
        "RQ1": "Addresses risk stemming from adversarial malware attacks.",
        "RQ2": "Proposes concrete algorithm to train and improve adversarial malware detectors.",
        "RQ3": "A concrete algorithm is proposed for a specific task.",
        "RQ4": "Evaluation on benchmark problems show significant improvements to evasion rates.",
        "Limitations": "Convergence is not guaranteed and false positive rates might be high.",
        "Keywords": [
            "malware detection",
            "algorithm",
            "applied",
            "adversarial attack"
        ],
        "AuthorKeywords": ["adversarial robustness", "adversarial learning", "machine learning-based malware detection", "adversarial malware variants", "adversarial minimax game"],
        "RiskTypes": ["malware", "adversarial attack"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["University of South Florida, United States", "University of Georgia, United States", "Hefei University of Technology, China", "University of Arizona, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1
    },
    {
        "Title": "An Assurance Case Pattern for the Interpretability of Machine Learning in Safety-Critical Systems",
        "BibtexKey": "wardAssuranceCasePattern2020",
        "Year": 2020,
        "Overview": "Proposes a safety argument pattern for arguing whether local or global explanations of an ML system are reliable.",
        "RQ1": "Addresses risks stemming from using unreliable post-hoc explanations for ML models.",
        "RQ2": "Proposes a safety argument pattern for ML post-hoc explainability.",
        "RQ3": "Proposes a model-agnostic argument.",
        "RQ4": "Not evaluated.",
        "Limitations": "Not evaluated.",
        "Keywords": [
            "theoretical",
            "safety argument",
            "explainable AI",
            "framework"
        ],
        "AuthorKeywords": ["interpretability", "explainability", "machine learning", "artificial intelligence", "assurance", "safety", "safety-case"],
        "RiskTypes": ["holistic"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of York, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 17
    },
    {
        "Title": "An Improved Robust Fuzzy Algorithm for Unsupervised Learning",
        "BibtexKey": "dikImprovedRobustFuzzy2020",
        "Year": 2020,
        "Overview": "Proposes a robust, dynamic, and fuzzy unsupervised learning algorithm for clustering consisting of three stages: 1. a preprocessing stage where potential outliers are determined and quarantined; 2. a dynamic clustering stage; 3. a re-insertion of quarantined samples into the determined clusters.",
        "RQ1": "Addresses risk from noisy data with outliers.",
        "RQ2": "A concrete robust unsupervised clustering algorithm is proposed.",
        "RQ3": "A data-agnostic algorithm is proposed.",
        "RQ4": "Evaluation on artificial and many real medical datasets shows best performance as compared to baselines.",
        "Limitations": "The detection of outliers is a crucial part of the algorithm and it is unclear how well the algorithm performs under varying levels of noise.",
        "Keywords": [
            "unsupervised learning",
            "clustering",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["similarity measure", "outlier detection", "clustering", "proximity degree"],
        "RiskTypes": ["noise", "outliers"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Mohammed V University, Morocco", "Abdelmalek Essaâdi University, Morocco"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3
    },
    {
        "Title": "Architectural Patterns for Handling Runtime Uncertainty of Data-Driven Models in Safety-Critical Perception",
        "BibtexKey": "grossArchitecturalPatternsHandling2022",
        "Year": 2022,
        "Overview": "Proposes architecture-level patterns for systems that must quantify uncertainty in data-driven decisions in safety-critical systems.",
        "RQ1": "Risk stemming from uncertain decisions are addressed on an architectural level.",
        "RQ2": "Four architectural design patterns are identified with two axes of changes to determine uncertainty during runtime.",
        "RQ3": "A high-level design framework is proposed.",
        "RQ4": "Evaluation of the system in a simulated distance controller shows that the framework can help find acceptable levels of risk more efficiently leading to performance gains.",
        "Limitations": "More evaluation is needed.",
        "Keywords": [
            "framework",
            "theoretical",
            "uncertainty estimates",
            "design patterns",
            "safety-critical system",
            "architectural patterns"
        ],
        "AuthorKeywords": ["uncertainty quantification", "architectural patterns", "machine learning", "safety", "autonomous systems"],
        "RiskTypes": ["uncertainty", "holistic"],
        "Stage": ["design"],
        "Method": "design framework",
        "AuthorAffiliation": ["Fraunhofer Institute for Experimental Software Engineering, Germany", "Robert Bosch GmbH, Germany"],
        "AuthorAffiliationType": ["research institution", "corporate"],
        "Citations": 5
    },
    {
        "Title": "Architectural Patterns for Integrating AI Technology into Safety-Critical Systems",
        "BibtexKey": "dzambicArchitecturalPatternsIntegrating2021",
        "Year": 2021,
        "Overview": "Two architectural design patterns are proposed to address specific risks stemming from using data-driven models in safety-critical applications. The first pattern integrates AI-based decision-making into a super-framework that takes the recommendations of the AI system as input and is ultimately responsible for the final decision involving a human/policy/other model. The second pattern integrates the AI model with external oversight in an edge computing fashion.",
        "RQ1": "Risks stemming from data-driven models in safety-critical systems are addressed.",
        "RQ2": "Two architectural design patterns are proposed.",
        "RQ3": "The proposed patterns are model- and task-agnostic.",
        "RQ4": "Not evaluated.",
        "Limitations": "Not evaluated.",
        "Keywords": [
            "architectural patterns",
            "design patterns",
            "framework",
            "theoretical",
            "safety-critical system"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["holistic"],
        "Stage": ["design"],
        "Method": "design framework",
        "AuthorAffiliation": ["Graz University of Technology, Austria"],
        "AuthorAffiliationType": ["university"],
        "Citations": 6
    },
    {
        "Title": "Artificial Intelligence Systems, Responsibility and Agential Self-Awareness",
        "BibtexKey": "farinaArtificialIntelligenceSystems2022",
        "Year": 2022,
        "Overview": "Presents a philosophical discussion of how AI agents may be considered responsible for their own actions concluding that having a sense of ownership (e.g., of an own body) is a minimal requirement for attributing responsibility to AI agents.",
        "RQ1": "Indirectly considers the risk stemming for wrongly attributing responsibility to AI agents where none may exist.",
        "RQ2": "NA",
        "RQ3": "NA",
        "RQ4": "NA",
        "Limitations": "Only a philosophical standpoint regarding holding AI agents themselves accountable.",
        "Keywords": [
            "theoretical",
            "philosophical",
            "accountability",
            "responsible AI",
            "AI alignment"
        ],
        "AuthorKeywords": ["AI responsibility", "artificial self", "agential self awareness", "personal identity"],
        "RiskTypes": ["responsibility", "holistic"],
        "Stage": ["operation"],
        "Method": "philosophical",
        "AuthorAffiliation": ["University of Nottingham, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 2
    },
    {
        "Title": "Assessing and Enhancing Adversarial Robustness of Predictive Analytics: An Empirically Tested Design Framework",
        "BibtexKey": "liAssessingEnhancingAdversarial2022",
        "Year": 2022,
        "Overview": "Proposes a framework for the robust training of adversarially resistant supervised machine learning systems. In the first step the robustness of an existing ML system is assessed and adversarial samples are identified, while in the second the method is retrained to be more robust against the identified attacks.",
        "RQ1": "Risk stemming from adversarial attacks on supervised ML systems are addressed.",
        "RQ2": "A two-stage design and deployment framework is proposed.",
        "RQ3": "The framework is learning algorithm- and task-agnostic.",
        "RQ4": "Implementation and extensive evaluation on spam text classification shows greatly improved performance of the framework-developed system as compared to the baseline.",
        "Limitations": "IT is unclear how well the system would adapt to new sources of attacks and how costly it is to implement the framework in an adaptable fashion.",
        "Keywords": [
            "supervised learning",
            "machine learning",
            "framework",
            "theoretical",
            "adversarial attack",
            "robustness"
        ],
        "AuthorKeywords": ["predictive analytics", "adversarial robustness", "text mining", "artificial intelligence security", "supervised machine learning", "design frameworks"],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["operation"],
        "Method": "design framework",
        "AuthorAffiliation": ["University of Georgia, United States", "Hefei University of Technology, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 4
    },
    {
        "Title": "Assurance Argument Patterns And Processes For Machine Learning In Safety-Related Systems",
        "BibtexKey": "picardiAssuranceArgumentPatterns2020",
        "Year": 2020,
        "Overview": "Proposes a safety assurance argument framework in goal structuring notation (GNS) to develop a case for the safety of deployment of ML-based systems in safety-related systems. It also proposes a four-stage deployment process for the instantiation of the determined requirements in an actual design scenario.",
        "RQ1": "Risks stemming from the unsafe deployment of ML systems in safety-critical systems is addressed.",
        "RQ2": "A safety verification argument in goal structuring notation is proposed with a deployment process to generate relevant artifacts for proving the safety of the ML system.",
        "RQ3": "The framework is model- and task-agnostic.",
        "RQ4": "Not evaluated.",
        "Limitations": "Not evaluated.",
        "Keywords": [
            "framework",
            "theoretical",
            "goal structuring notation (GSN)",
            "assurance process",
            "safety-critical system",
            "safety argument"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["holistic"],
        "Stage": ["design", "deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of York, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 26
    },
    {
        "Title": "Assured Deep Multi-Agent Reinforcement Learning for Safe Robotic Systems",
        "BibtexKey": "rileyAssuredDeepMultiAgent2022",
        "Year": 2022,
        "Overview": "Proposes a four-stage safety assurance framework for multi-agent RL called Assured Multi-Agent Reinforcement Learning (AMARL) which relies on the oversight of an RL and domain expert to first derive high-level abstract policies that drive and constrain the training of the actual MARL process. This produces formally verifiable safety guarantees through the process of quantitative verification. They extend this framework with deep RL to better utilise sparse reward signals.",
        "RQ1": "Risk stemming from unsafe exploration and training in MARL is addressed.",
        "RQ2": "Proposes a four-stage framework integrated with deep RL and formally verifiable guarantees through quantitative verification.",
        "RQ3": "A MARL-specific but training algorithm-agnostic training framework is proposed",
        "RQ4": "Evaluation on a search and collect task shows comparative performance and much reduced risk levels as compared to the baseline.",
        "Limitations": "Evaluation on more tasks should be necessary.",
        "Keywords": [
            "safe RL",
            "framework",
            "applied"
        ],
        "AuthorKeywords": ["reinforcement learning", "multi-agent systems", "quantitative verification", "assurance", "multi-agent reinforcement learning", "safety-critical scenarios", "safe multi-agent reinforcement learning",  "assured multi-agent reinforcement learning", "deep reinforcement learning"],
        "RiskTypes": ["safety verification"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["University of York, UK", "L3S Research Center, Germany", "Defence Science and Technology Laboratory, UK"],
        "AuthorAffiliationType": ["university", "research institution", "government"],
        "Citations": 3
    },
    {
        "Title": "Auditing And Testing AI - A Holistic Framework",
        "BibtexKey": "beckerAuditingTestingAI2022",
        "Year": 2022,
        "Overview": "Proposes a five-stage holistic framework for the development of AI systems with a very high-level view and also suggests an auditing framework to oversee the entire life cycle of an AI system.",
        "RQ1": "A wide range of risks may be addressed by the holistic auditing and testing framework.",
        "RQ2": "A five-stage holistic framework is proposed for auditing and testing.",
        "RQ3": "A very general and very high-level framework is proposed.",
        "RQ4": "Not evaluated.",
        "Limitations": "No evaluation of the framework. No real actionable discussions.",
        "Keywords": [
            "framework",
            "theoretical",
            "auditing",
            "testing"
        ],
        "AuthorKeywords": ["AI", "testing", "auditing", "safety", "fairness", "lifecycle", "framework"],
        "RiskTypes": ["holistic"],
        "Stage": ["design", "deployment", "operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Gesellschaft für Informatik, Germany"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 0
    },
    {
        "Title": "Autoencoder-Based Semantic Novelty Detection: Towards Dependable AI-Based Systems",
        "BibtexKey": "rauschAutoencoderBasedSemanticNovelty2021",
        "Year": 2021,
        "Overview": "Proposes a semantic segmentation and autoencoder-based novelty detection system.",
        "RQ1": "Risk stemming from testing on data not seen during training is addressed.",
        "RQ2": "A novelty detection algorithm is proposed to detect out-of-distribution samples for safety checking.",
        "RQ3": "A specific algorithm is proposed for a specific, autonomous driving-related task.",
        "RQ4": "Implementation and evaluation on autonomous driving as the use case shows improved performance as compared to a baseline without semantic segmentations.",
        "Limitations": "Limited novelty and specific to autonomous driving.",
        "Keywords": [
            "autonomous driving",
            "algorithm",
            "novelty detection",
            "semantic segmentation",
            "applied",
            "autoencoder"
        ],
        "AuthorKeywords": ["safety engineering", "autonomous system", "perception", "artificial intelligence", "autoencoder", "novelty detection"],
        "RiskTypes": ["outliers", "out-of-distribution (OOD)"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Technische Universität Clausthal, Germany"],
        "AuthorAffiliationType": ["university"],
        "Citations": 6
    },
    {
        "Title": "Automatic Exploration Process Adjustment for Safe Reinforcement Learning with Joint Chance Constraint Satisfaction",
        "BibtexKey": "okawaAutomaticExplorationProcess2020",
        "Year": 2020,
        "Overview": "Proposes a safe RL algorithm for continuous affine nonlinear control problems where the safety of the to-be-executed action during exploration is determined by a gradually adjusted normal distribution whose covariance matrix is updated during the training of the control process. This leads to provable guarantees on the probability of entering unsafe states.",
        "RQ1": "Risk stemming from the exploration of unsafe actions in continuous RL problems is addressed.",
        "RQ2": "A gradually adjusted constrained RL training algorithm is proposed.",
        "RQ3": "A specific training algorithm is proposed for continuous affine nonlinear RL control problems.",
        "RQ4": "Theoretical results and numerical simulation show that the proposed method can reduce the cost of exploration significantly.",
        "Limitations": "More evaluation is necessary.",
        "Keywords": [
            "reinforcement learning",
            "affine nonlinear system",
            "safe RL",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["reinforcement learning", "learning algorithm", "safe exploration", "safety-critical", "chance constraint"],
        "RiskTypes": ["unsafe actions", "exploration"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Fujitsu Laboratories, Japan", "Reading Skill Test, Japan"],
        "AuthorAffiliationType": ["corporate"],
        "Citations": 5
    },
    {
        "Title": "Automatically Learning Fallback Strategies with Model-Free Reinforcement Learning in Safety-Critical Driving Scenarios",
        "BibtexKey": "lecerfAutomaticallyLearningFallback2022",
        "Year": 2022,
        "Overview": "Proposes an RL training algorithm where a set of fallback policies are trained in addition to the optimal policy. These suboptimal fallback policies are trained to each be qualitatively different from one another and therefore provide fallback strategies in case the optimal fails. An additional reward component for each suboptimal policy is added to the reward signal to motivate this qualitative separation.",
        "RQ1": "Risk stemming from unsafe optimal policies in RL is considered.",
        "RQ2": "A fallback strategy is proposed for RL.",
        "RQ3": "The method is sufficiently task-agnostic to be used in environments other than the evaluation environment.",
        "RQ4": "Evaluation for autonomous driving-based intersection environment shows higher rewards and a qualitative difference between trained policies is shown.",
        "Limitations": "Limited evaluation and there might be a significant influence on training time due to the training of additional policies.",
        "Keywords": [
            "reinforcement learning",
            "fallback policies",
            "suboptimal control",
            "algorithm",
            "applied",
            "autonomous driving"
        ],
        "AuthorKeywords": ["reinforcement learning", "autonomous vehicle", "safe control"],
        "RiskTypes": ["unsafe actions", "out-of-distribution (OOD)"],
        "Stage": ["deployment", "operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Renault Software Labs, France", "Data Science EURECOM, France"],
        "AuthorAffiliationType": ["corporate", "research institution"],
        "Citations": 1
    },
    {
        "Title": "Automating Safety Argument Change Impact Analysis for Machine Learning Components",
        "BibtexKey": "carlanAutomatingSafetyArgument2022",
        "Year": 2022,
        "Overview": "Propose a semi-automated approach for identifying the impact of changes on safety arguments. The framework is formulated in goal structuring notation (GSN). It extends an existing safety metamodel with semantically enriched traces between argumentation elements. It also proposes a seven-stage qualitative change impact assessment process for arguments about the sufficiency of the input data. Finally, it proposes a quantitative method to automatically evaluate the coverage of the input space in the data.",
        "RQ1": "Risks stemming from insufficient data coverage are addressed.",
        "RQ2": "A safety argument framework is proposed in GSN with an additional automatic process to determine the sufficiency of coverage in the input data.",
        "RQ3": "A model-agnostic safety argumentation framework is proposed.",
        "RQ4": "Evaluation in a simulated safety engineering environment demonstrates the applicability of the framework ML-pedestrian detection.",
        "Limitations": "The framework is quite complex and it is not evaluated extensively.",
        "Keywords": [
            "framework",
            "theoretical",
            "safety argument",
            "safety-critical system",
            "goal structuring notation (GSN)"
        ],
        "AuthorKeywords": ["safety cases", "machine learning (ML)", "operational design domain (ODD)", "change impact analysis (CIA)"],
        "RiskTypes": ["holistic", "data requirements", "out-of-distribution (OOD)"],
        "Stage": ["design", "deployment", "operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["fortiss GmbH, Germany", "Robert Bosch GmbH, Germany", "Mälardalen University, Germany", "Fraunhofer Institute for Experimental Software Engineering, Germany"],
        "AuthorAffiliationType": ["research institution", "corporate"],
        "Citations": 1
    },
    {
        "Title": "Adaptive Critic Designs for Event-Triggered Robust Control of Nonlinear Systems with Unknown Dynamics",
        "BibtexKey": "yangAdaptiveCriticDesigns2019",
        "Year": 2019,
        "Overview": "Proposes an event-triggered robust control strategy for continuous-time nonlinear systems with unknown dynamics that uses an RNN and adaptive critic design. The RNN is used to reconstruct the system dynamics based on system data, then a critic network is used to approximate the solution of the event-triggered Hamilton-Jacobi-Bellman equation.",
        "RQ1": "Risk from unsafe exploration and states are addressed indirectly for nonlinear control.",
        "RQ2": "A robust control method is proposed for nonlinear systems with unknown dynamics that guarantees the boundedness of system states.",
        "RQ3": "A specific robust control algorithm is proposed for a specific problem.",
        "RQ4": "Evaluation on a nonlinear oscillator and an unstable power system shows that system states stay bounded.",
        "Limitations": "Proposed system only works for deterministic control.",
        "Keywords": [
            "control theory",
            "nonlinear system",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["adaptive critic designs (ACD)", "adaptive dynamic programming (ADP)", "event-triggered control (ETC)", "neural networks (NN)", "reinforcement learning (RL)", "robust control"],
        "RiskTypes": ["unsafe exploration", "unsafe states"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Tianjin University, China", "University of Rhode Island, USA"],
        "AuthorAffiliationType": ["university"],
        "Citations": 127
    },
    {
        "Title": "Adaptive Interleaved Reinforcement Learning: Robust Stability of Affine Nonlinear Systems with Unknown Uncertainty",
        "BibtexKey": "liAdaptiveInterleavedReinforcement2022",
        "Year": 2022,
        "Overview": "Proposes an adaptive interleaved reinforcement learning algorithm for discreet-time affine nonlinear systems. By picking the right utility function the robust control problem is cast as an optimal control problem. At each time step the method evaluates its performance and then updates its control policy parametrised by a neural network.",
        "RQ1": "Risk from unknown uncertainty in control problems is addressed.",
        "RQ2": "An adaptive robust RL-based control algorithm is proposed for both matched and unmatched uncertainty.",
        "RQ3": "A specific control algorithm is proposed for a specific control problem.",
        "RQ4": "Theoretical results and numerical simulation on a torsional pendulum and another non-linear system shows that the control algorithm can assure stability even in the presence of uncertainty.",
        "Limitations": "",
        "Keywords": [
            "reinforcement learning",
            "affine nonlinear system",
            "control theory",
            "Hamilton-Jacobi-Bellman (HJB)",
            "robustness",
            "algorithm",
            "theoretical"
        ],
        "AuthorKeywords": ["interleaved reinforcement learning", "neural networks (NN)", "robust control", "uncertain systems"],
        "RiskTypes": ["robustness", "uncertainty"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Liaoning Shihua University, China", "Northeastern University, China", "The University of Texas at Arlington, United States", "Missouri University of Science and Technology, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 47
    },
    {
        "Title": "Barrier Lyapunov Function-Based Safe Reinforcement Learning for Autonomous Vehicles With Optimized Backstepping",
        "BibtexKey": "zhangBarrierLyapunovFunctionBased2022",
        "Year": 2022,
        "Overview": "Proposes a safe RL-based control algorithm for non-linear systems in strict feedback form with backstepping that significantly reduces the variance of the control method. The method decomposes optimised control to sub-critics and actors that combine Barrier Lyapunov Functions and neural networks.",
        "RQ1": "Risk from unsafe exploration is addressed in non-linear RL control.",
        "RQ2": "Proposes a concrete algorithm to ensure safe exploration during the training process.",
        "RQ3": "A specific safe RL algorithm is proposed.",
        "RQ4": "Evaluation on trajectory tracking and planning problems show decreased variance during learning. Ablation studies are also performed.",
        "Limitations": "The text is highly inaccessible without specialised knowledge of optimal RL control and Barrier Lyapunov Function. ",
        "Keywords": [
            "reinforcement learning",
            "trajectory tracking",
            "safe RL",
            "autonomous driving",
            "algorithm",
            "applied",
            "control theory"
        ],
        "AuthorKeywords": ["adaptive dynamic programming (ADP)", "autonomous vehicles", "Barrier Lyapunov Function (BLF)", "safe reinforcement learning (SRL)"],
        "RiskTypes": ["unsafe actions", "unsafe exploration"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["National University of Singapore, Singapore", "Beihang University, China", "Tongji University, China", "Jilin University, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 12
    },
    {
        "Title": "BDI-Dojo: Developing Robust BDI Agents In Evolving Adversarial Environments",
        "BibtexKey": "pulawskiBDIDojoDevelopingRobust2021",
        "Year": 2021,
        "Overview": "Presents an extension to an existing development framework (Jason-RL) for belief-desire-intention-based robust multi-agent adversarial learning.",
        "RQ1": "Risk from uncertain adversarial environments is addressed.",
        "RQ2": "A development and training framework is presented.",
        "RQ3": "The framework works for RL-based BDI agent training in adversarial settings.",
        "RQ4": "Evaluation on a toy example shows that the environment produces more robust agents as compared to the baseline.",
        "Limitations": "Limited evaluation, the code is missing for the framework though might be available somewhere else online.",
        "Keywords": [
            "reinforcement learning",
            "multi-agent RL (MARL)",
            "adversarial learning",
            "belief-desire-intention (BDI)",
            "framework",
            "applied"
        ],
        "AuthorKeywords": ["multi-agent systems", "robustness", "resilience", "reinforcement learning", "adversarial training"],
        "RiskTypes": ["adversarial learning", "robustness"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University Of Wollongong, Australia"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Beneficial Artificial Intelligence Coordination by Means of a Value Sensitive Design Approach",
        "BibtexKey": "umbrelloBeneficialArtificialIntelligence2019",
        "Year": 2019,
        "Overview": "Suggests that value sensitive design (VSD) could be used to instill common values into AI systems throughout the design process. The paper gives an overview of VSD and presents a qualitative investigation of how VSD applies to the UK AI Committee case study.",
        "RQ1": "Risk from AI systems unaligned with human values are considered.",
        "RQ2": "A conceptual framework is explored for informing the design process of AI systems.",
        "RQ3": "The framework is very high-level and application-agnostic.",
        "RQ4": "Evaluation in the form of a qualitative case study analysis shows that VSD can cover topics discussed in the UK AI committee case study.",
        "Limitations": "Very high-level with limited evaluation constrained only to a single case study.",
        "Keywords": [
            "framework",
            "theoretical",
            "qualitative analysis",
            "AI alignment"
        ],
        "AuthorKeywords": ["value sensitive design (VSD)", "design for values", "safe for design", "AI ethics"],
        "RiskTypes": ["alignment"],
        "Stage": ["design"],
        "Method": "philosophical",
        "AuthorAffiliation": ["Institute for Ethics and Emerging Technologies, Italy"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 42
    },
    {
        "Title": "Beyond Generalization: A Theory Of Robustness In Machine Learning",
        "BibtexKey": "freieslebenGeneralizationTheoryRobustness2023",
        "Year": 2023,
        "Overview": "Proposes a unified theoretical framework to think about and cover various definitions of robustness. It differentiates between the robustness target which is the system that is to be made robust, and the robustness modifier which measures the degree of robustness in the target. The framework then draws a causal relationship between the target and modifier and shows that definitions of robustness in ML can be cast as instantiations of this framework.",
        "RQ1": "Addressing the risk of diverging and heterogenous definitions of robustness in ML.",
        "RQ2": "Proposes a unified theoretical account of robustness in ML.",
        "RQ3": "A general high-level theoretical framework is proposed.",
        "RQ4": "Not applicable.",
        "Limitations": "It is not explored how robustness is defined if more than one distinct system is involved.",
        "Keywords": [
            "theoretical",
            "framework",
            "robustness",
            "machine learning"
        ],
        "AuthorKeywords": ["robustness", "machine learning", "generalization", "models in science", "uncertainty", "extrapolation"],
        "RiskTypes": ["robustness", "holistic"],
        "Stage": ["design"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["University of Tübingen, Germany"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1
    },
    {
        "Title": "BinFI: An Efficient Fault Injector for Safety-Critical Machine Learning Systems",
        "BibtexKey": "chenBinFIEfficientFault2019",
        "Year": 2019,
        "Overview": "Proposes a fault injection-based safety analysis technique for ML systems, that relies on the assumptions that ML algorithms can be approximated by a single monotonic function. It uses the monotonic function to approximate the error propagation levels within the ML system to then identify safety-critical bits in the input.",
        "RQ1": "Risk stemming from sensitivity to safety-critical input variation is addressed.",
        "RQ2": "Proposes a method to estimate error propagation and to find safety-critical bits in the input of ML systems.",
        "RQ3": "The method is ML system-agnostic.",
        "RQ4": "Evaluated on traffic sign detection and path planning for autonomous vehicle shows that the method can identify safety critical bits with very high accuracy.",
        "Limitations": "Evaluation on more ML systems and more tasks would be important.",
        "Keywords": [
            "machine learning",
            "robustness",
            "fault injection",
            "safety-critical system",
            "autonomous driving",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["error resilience", "machine learning", "fault injection"],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment", "operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of British Columbia, Canada", "Los Alamos National Laboratory, United States"],
        "AuthorAffiliationType": ["university", "government"],
        "Citations": 95
    },
    {
        "Title": "Bridging Model-based Safety and Model-free Reinforcement Learning through System Identification of Low Dimensional Linear Models",
        "BibtexKey": "liBridgingModelbasedSafety2022",
        "Year": 2022,
        "Overview": "Combines model-free RL with model-based safety by mapping a high-dimensional closed-loop control system to a low-dimensional linear model. The linear model is fitted to input-output pairs from the RL control driven by a neural network and it is shown to be able to approximate the system dynamic very well.",
        "RQ1": "Risk stemming from unsafe control of high-dimensional systems is addressed.",
        "RQ2": "Proposes a concrete algorithm and encapsulating framework to combine a safe low-dimensional linear control and planning methods for improved safety verification from a high-dimensional model.",
        "RQ3": "A concrete RL safety method is proposed that is agnostic of the RL training algorithm.",
        "RQ4": "Evaluation on a complex robot manipulation task shows stable and decoupled and minimum phase control.",
        "Limitations": "Testing with more RL training algorithms would be useful. Some qualitative explanations of the obtained linear model could be useful as well.",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "model-free RL",
            "model-based safety",
            "bipedal robot",
            "high-dimensional control",
            "framework",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["instability", "high-dimensional control"],
        "Stage": ["deployment", "operation"],
        "Method": "real-world testing",
        "AuthorAffiliation": ["University of California at Berkley, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 5
    },
    {
        "Title": "Building Multi-Agent Environments with Theoretical Guarantees on the Learning of Ethical Policies",
        "BibtexKey": "rodriguez-sotoBuildingMultiAgentEnvironments2022",
        "Year": 2022,
        "Overview": "Propose a method for multi-agent systems that gives theoretical guarantees that agents learn ethical policies where ethical is defined as the maximisation of a social moral value encoded as an additional reward signal. The proposed multi-agent ethical embedding method transforms the learning environment of a multi-objective ethical Markow game into a single-object ethical Markov game via an ethical equilibrium and then a solution weight vector computation.",
        "RQ1": "Risk stemming from unethical behaviour in multi-agent learning is addressed.",
        "RQ2": "Proposes a concrete multi-agent framework that trains ethical agents with theoretical guarantees.",
        "RQ3": "A concrete RL training algorithm is proposed.",
        "RQ4": "Evaluated on the public civility game. Shows that ethical actions are preferred by agents.",
        "Limitations": "Limited evaluation and a lack of concrete algorithm for training the proposed framework.",
        "Keywords": [
            "reinforcement learning",
            "multi-agent RL (MARL)",
            "ethics",
            "ethical RL",
            "multi-objective Markov game",
            "framework",
            "applied"
        ],
        "AuthorKeywords": ["value alignment", "moral decision making", "multi-objective reinforcement learning", "multi-agent systems"],
        "RiskTypes": ["ethical"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Artificial Intelligence Research Institute, Spain", "Universitat de Barcelona, Spain"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Censored Markov Decision Processes: A Framework for Safe Reinforcement Learning in Collaboration with External Systems",
        "BibtexKey": "kohjimaCensoredMarkovDecision2020",
        "Year": 2020,
        "Overview": "Proposes a special version of semi-Markov decision processes (MDP) called Censored MDPs (CeMDP) which describes the interaction of environment, learner, and an external system such as human intervention to support safer RL. A variant of value iteration and Q-learning is proposed for CeMDPs.",
        "RQ1": "Addresses risk from unsafe actions and states in reinforcement learning.",
        "RQ2": "Proposes a modified semi-MDP framework with external feedback.",
        "RQ3": "A specific framework is proposed for safe RL that is training algorithm agnostic.",
        "RQ4": "Some theoretical results are derived and minor numerical simulations are run.",
        "Limitations": "Theoretical results do not give a strong insight about why the proposed framework should be used in contrast to other methods and the numerical results show that the proposed algorithm converges but not much more.",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "censored MDP",
            "semi MDP",
            "framework",
            "algorithm",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["unsafe actions", "unsafe states"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["NTT Corporation, Japan"],
        "AuthorAffiliationType": ["corporate"],
        "Citations": 0
    },
    {
        "Title": "Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing",
        "BibtexKey": "rajiClosingAIAccountability2020",
        "Year": 2020,
        "Overview": "Propose an actionable five-stage internal auditing framework to ensure that the system in development conforms to the internal ethical values of the company. The audit process generates artifacts at each stage which are finally assembled into an audit report. The stages are Scoping, Mapping, Artifact Collection, Testing, and Reflection (SMACTR).",
        "RQ1": "Addresses short- and long-term risks from the unethical deployment and side effects of AI systems.",
        "RQ2": "Proposes an actionable ethical internal audit framework that is designed to make sure the system confirms to company values.",
        "RQ3": "A high-level and general framework is proposed.",
        "RQ4": "Not evaluated.",
        "Limitations": "Not evaluated. Internal audits may not be independent of internal pressure and so might not be as enforcable as external audits.",
        "Keywords": [
            "internal audit",
            "ethics",
            "framework",
            "theoretical"
        ],
        "AuthorKeywords": ["algorithmic audits", "machine learning", "accountability", "responsible innovation"],
        "RiskTypes": ["holistic"],
        "Stage": ["design"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Partnership on AI, United States", "Google, United States"],
        "AuthorAffiliationType": ["research institution", "big-tech corporate"],
        "Citations": 605
    },
    {
        "Title": "Combining Pessimism with Optimism for Robust and Efficient Model-Based Deep Reinforcement Learning",
        "BibtexKey": "curiCombiningPessimismOptimism2021",
        "Year": 2021,
        "Overview": "Propose a provably robust and sample-efficient deep model-based RL algorithm that is robust to adversarial agents, action-space changes, and parameter space changes. The model is based on the computation of an optimistic and adversarial pessimistic policy in an adversarial RL setting.",
        "RQ1": "Addresses risk from a lack of robustness to adversarial actions, action and parameter space perturbations.",
        "RQ2": "Proposes a training algorithm for model-based deep RL.",
        "RQ3": "The proposed algorithm is sufficiently generic to solve a range of tasks but is specific to the problem model-based deep RL.",
        "RQ4": "Extensive evaluation on several environments against multiple baselines shows improvments to robustness across the board.",
        "Limitations": "The theoretical guarantees rely on assumptions of Lipschitz continuity which constrains the range of models that can parametrise the environment-dynamics model.",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "robustness",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["ETH Zurich, Switzerland"],
        "AuthorAffiliationType": ["university"],
        "Citations": 10
    },
    {
        "Title": "Computational Transcendence: Responsibility and Agency",
        "BibtexKey": "deshmukhComputationalTranscendenceResponsibility2022",
        "Year": 2022,
        "Overview": "Studies emergent responsible behaviour among agents in non-cooperative games. Agents are endowed with an internal elastic sense of self that they themselves curate rather than external constraints or reinforcements being imposed on them. This leads to action selections that benefit the total welfare rather than the individual rewards.The framework is computational transcendence.",
        "RQ1": "Addresses risk from selfish behaviour of agents in non-cooperative environments.",
        "RQ2": "Presents a framework and implementation to show that agents themselves may attain strategies that are individually suboptimal but increase total welfare.",
        "RQ3": "An ethical multi=agential framework and implementation is presented which is generally task-agnostic.",
        "RQ4": "Some evaluation against the baseline of reciprocity shows improved social outcomes.",
        "Limitations": "Very limited evaluation of the system, and the motivations of the framework, especially the qualitative interpretation of various hyperparameters in terms of ethical motivations are not rigoruously explored.",
        "Keywords": [
            "multi-agent games",
            "computational transcendence",
            "non-cooperative games",
            "framework",
            "theoretical"
        ],
        "AuthorKeywords": ["responsible AI", "multi-agent systems", "AI ethics", "autonomy", "agency", "identity"],
        "RiskTypes": ["ethical", "cooperation"],
        "Stage": ["design", "deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["International Institute of Information Technology, India"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 4
    },
    {
        "Title": "Conflict-Aware Safe Reinforcement Learning: A Meta-Cognitive Learning Framework",
        "BibtexKey": "mazouchiConflictAwareSafeReinforcement2022",
        "Year": 2022,
        "Overview": "Propose a data-driven conflict-aware safe RL algorithm for autonomous systems across a variety of circumstances using a two-level hierarchy of controllers. A higher metacognitive layer leverages a data-driven receding-horizon attentional controller (RHAC) adapts relative attention to different system's safety and performance requirements, while a lower-layer RL controller designs control actuation signals for the system.",
        "RQ1": "Risk from non-robust control is addressed.",
        "RQ2": "Proposes an RL control framework and algorithm with a high-level attentional and low-level actuation controller.",
        "RQ3": "A specific RL control algorithm is proposed.",
        "RQ4": "Evaluation on numerical simulations corresponding to vehicle acceleration and steering.",
        "Limitations": "Limited evaluation and unclear how the method generalises to unseen circumstances.",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "control theory",
            "algorithm",
            "framework",
            "applied"
        ],
        "AuthorKeywords": ["optimal control", "receding-horizon attentional controller (RHAC)","reinforcement learning (RL)"],
        "RiskTypes": ["robustness", "generalization"],
        "Stage": ["deployment", "operation"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Michigan State University, United States", "Ford Research and Innovation Center, United States"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 13
    },
    {
        "Title": "Conservative and Adaptive Penalty for Model-Based Safe Reinforcement Learning",
        "BibtexKey": "maConservativeAdaptivePenalty2022",
        "Year": 2022,
        "Overview": "Proposes a model-based safe RL learning algorithm that accounts for potential modeling errors by capturing model uncertainty and adaptively exploiting it to balance the reward and the cost objectives. The proposed model is called Conservative and Adaptive Penalty (CAP)",
        "RQ1": "Addresses risk from incorrect modeling in safe RL, specifically in constrained MDPs.",
        "RQ2": "A learning algorithm is proposed to account for adaptively modeling errors.",
        "RQ3": "A concrete safe RL training algorithm is proposed that is environment-agnostic but works only for constrained MDPs.",
        "RQ4": "Theoretical proofs show that CAP leads to feasible policies in the real environment and simulations in various environments show greatly reduced constraint violations compared to ablations and baselines.",
        "Limitations": "Constraint violations still occur with the proposed method and the proposed method is not tractable in its original formulation so heuristics are needed.",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "constrained MDP",
            "penalty",
            "algorithm",
            "applied",
            "model-based RL"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["modeling errors", "constraint violation", "unsafe states"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["University of Pennsylvania, United States", "University of Melbourne, Australia"],
        "AuthorAffiliationType": ["university"],
        "Citations": 16
    },
    {
        "Title": "Constrained Cross-Entropy Method for Safe Reinforcement Learning",
        "BibtexKey": "wenConstrainedCrossEntropyMethod2021",
        "Year": 2021,
        "Overview": "Propose a constrained cross-entropy-based method to address the problem of safe reinforcement learning problem with constrained MDPs transforming the original constrained problem into an unconstrained problem with a surrogate objective function.",
        "RQ1": "Addresses risk from safety constraint violations of reinforcement learning.",
        "RQ2": "Proposes a training algorithm for constrained MDPs.",
        "RQ3": "A concrete training algorithm is proposed that is environment-agnostic but specific to constrained MDPs.",
        "RQ4": "Some theoretical results are shown that the method works with high probability and two numerical simulations show some minor improvements over baselines on constraint satisfaction performance.",
        "Limitations": "Method is sample inefficient and requires good estimation of surrogate policies required for the method.",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "constrained cross entropy",
            "constrained MDP",
            "surrogate loss",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["machine learning algorithms", "safe reinforcement learning", "statistical learning"],
        "RiskTypes": ["constraint violation", "unsafe states"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["University of Pennsylvania, United States", "The University of Texas at Austin, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 89
    },
    {
        "Title": "Continuous Safety Assessment of Updated Supervised Learning Models in Shadow Mode",
        "BibtexKey": "guissoumaContinuousSafetyAssessment2023",
        "Year": 2023,
        "Overview": "Proposes a workflow for developing, deploying, and evaluating over-the-air (OTA) updates of supervised machine learning models. To prevent potential unforeseen misbehavior after deployment, they introduce a method for runtime evaluation of updates in shadow mode using contract specifications which is then embedded in a workflow for iterative training.",
        "RQ1": "Addresses risk for the untested or unverified deployment of over-the-air updates to supervised machine learning models.",
        "RQ2": "A workflow is proposed to certify the correctness of new versions of supervised ML models during development, deployment, and evaluation.",
        "RQ3": "A high-level workflow is proposed specifically for supervised ML.",
        "RQ4": "Some exploratory evaluation on a prototype Electronic Control Unit for an automotive Lane Keep Assist system.",
        "Limitations": "Limited evaluation and unclear how actionable or useful the workflow would be in real-world situations.",
        "Keywords": [
            "supervised learning",
            "machine learning",
            "continuous safety",
            "contracts",
            "framework",
            "theoretical"
        ],
        "AuthorKeywords": ["DevOps", "safety monitoring", "Over-The-Air updates", "contract-based design", "supervised learning"],
        "RiskTypes": ["over-the-air updates", "continuous deployment", "new versions"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Karlsruhe Institute of Technology, Germany"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Counterfactual Learning In Enhancing Resilience In Autonomous Agent Systems",
        "BibtexKey": "samarasingheCounterfactualLearningEnhancing2023",
        "Year": 2023,
        "Overview": "The paper advocates the inclusion of counterfactual learning for safety-relevant and robust autonomous systems as a way of moving beyond the current limitations of safe AI, including removing the excess need for redundancy, decentralised control, or distributed sensing. It also proposes a high-level design framework to how counterfactual learning could be incorporated into safety-critical autonomous systems.",
        "RQ1": "Risk from unsafe autonomous systems are addressed on a conceptual level.",
        "RQ2": "A high-level discussion and framework is presented to improve safe AI through the use of counterfactuals.",
        "RQ3": "A very high-level discussion with no concrete method but a high-level design framework.",
        "RQ4": "Not evaluated.",
        "Limitations": "Very high level without any evaluation.",
        "Keywords": [
            "counterfactual learning",
            "framework",
            "theoretical"
        ],
        "AuthorKeywords": ["autonomous agent systems", "multi-agent system (MAS)", "resilience", "counterfactual learning", "machine learning", "explainability", "explainable agents", "robustness"],
        "RiskTypes": ["holistic"],
        "Stage": ["design"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["University of New South Wales, Australia"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Curricular Robust Reinforcement Learning via GAN-Based Perturbation Through Continuously Scheduled Task Sequence",
        "BibtexKey": "liCurricularRobustReinforcement2023",
        "Year": 2023,
        "Overview": "Propose a method to improve the robustness of the learned RL policy where errors of the transition function are viewed as a set of RL tasks following a task distribution given by a GAN that generates tasks in a curriculum learning setting.",
        "RQ1": "Risk from unstable RL policies is addressed.",
        "RQ2": "A concrete training algorithm is proposed for MDPs to obtain more robust policies.",
        "RQ3": "A concrete training algorithm is proposed for general MDPs.",
        "RQ4": "Evaluation on three environments show comparable performance to baselines.",
        "Limitations": "Limited and unconvincing evaluation.",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "robustness",
            "curriculum learning",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["robust reinforcement learning", "generative adversarial network (GAN)", "curricular learning"],
        "RiskTypes": ["robustness", "unsafe actions"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Beijing Jiaotong University, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1
    },
    {
        "Title": "Curriculum Learning-Based Fuzzy Support Vector Machine",
        "BibtexKey": "chenCurriculumLearningbasedFuzzy2023",
        "Year": 2023,
        "Overview": "Adapts the existing method of fuzzy SVM with a curriculum learning approach where initially only easy samples are learned, then increasingly harder samples are added into the dataset. To determine between easy and hard samples, a modified DBSCAN is used for clustering.",
        "RQ1": "Addresses risk from noisy data with outliers by improving robustness.",
        "RQ2": "Proposes a supervised learning algorithm called curriculum learning-based SVM.",
        "RQ3": "A concrete supervised learning algorithm is proposed.",
        "RQ4": "Extensive evaluation on various synthetic datasets and popular benchmarks show improved accuracy and robustness to outliers.",
        "Limitations": "The method is quite complex and runtime might increase significantly for large data.",
        "Keywords": [
            "supervised learning",
            "machine learning",
            "fuzzy support vector machine",
            "curriculum learning",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["fuzzy support vector machine", "curriculum learning strategy", "noise", "density-based clustering", "slack variable"],
        "RiskTypes": ["robustness", "outliers"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Xiamen University, China", "Huaqiao University, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Data Banzhaf: A Robust Data Valuation Framework for Machine Learning",
        "BibtexKey": "wangDataBanzhafRobust2023",
        "Year": 2023,
        "Overview": "Proposes to use the Banzhaf value as a measure of data quality for the problem of data valuation. Formalises the notion of data valuation robustness by introducing the safety margin which is the magnitude of the largest perturbation of model performance scores. Within this theoretical framework, the Banzhaf value achieves the highest margin.",
        "RQ1": "Addresses risk to data sharing, dataset quality evaluations, and test-time behaviour through the study of robust data valuation.",
        "RQ2": "Proposes a theoretical framework to formalise mathematically what it means for data valuations to be robust and introduces a high safety margin valuation called Data Banzhaf.",
        "RQ3": "A theoretical framework, a concrete measure for data valuation, and an approximation of that measure is proposed.",
        "RQ4": "Theoretical results about the accuracy of the Banzhaf estimation and empirical evaluation on several ML tasks against other data valuation measures.",
        "Limitations": "The derived estimation algorithm assumes the worst-case variation to the data which may not be appropriate for data sources with known noise distributions.",
        "Keywords": [
            "machine learning",
            "data valuation",
            "Banzhaf value",
            "safety margin",
            "robustness",
            "noisy data",
            "framework",
            "algorithm",
            "theoretical",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["data quality", "noise"],
        "Stage": ["deployment", "operation"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Princeton University, United States", "Virginia Tech, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 23
    },
    {
        "Title": "Data Efficient Safe Reinforcement Learning Algorithm",
        "BibtexKey": "padakandlaDataEfficientSafe2022",
        "Year": 2022,
        "Overview": "Formulates the safe RL problem in a constrained off-policy RL setting to facilitate safe exploration for RL agents. Develops a sample efficient utilising cross-entropy that respects a given threshold for the number of safety constraint violations.",
        "RQ1": "Risk from unconstrained and unsafe RL exploration is addressed.",
        "RQ2": "Proposes a constrained off-policy formulation for safe RL and proposes a sample efficient learning algorithm.",
        "RQ3": "A concrete algorithm is proposed for a certain type of MDPs.",
        "RQ4": "Evaluation on 4 RL benchmarks.",
        "Limitations": "Limited evaluation and unclear from evaluation what improvements are achieved to the safety of the system.",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "constrained cross entropy",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["safe exploration", "constrained RL", "off-policy"],
        "RiskTypes": ["unsafe exploration", "unsafe exploration"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Indian Institute of Science, India"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1
    },
    {
        "Title": "Decentralized Stochastic Optimization and Machine Learning: A Unified Variance-Reduction Framework for Robust Performance and Fast Convergence",
        "BibtexKey": "xinDecentralizedStochasticOptimization2020",
        "Year": 2020,
        "Overview": "Considers the task of decentralised stochastic optimisation where data samples are distributed across a network of nodes, and raw data sharing is not permitted due to privacy and/or resource constraints. Proposes a unified algorithmic framework that combines variance reduction with gradient tracking to achieve robust performance and fast convergence.",
        "RQ1": "Risks from data sharing and distribution are addressed.",
        "RQ2": "Reviews existing methods for decentralised stochastic optimisation and proposes a unified framework to talk about the topic.",
        "RQ3": "Reviews existing stochastic decentralised optimisation algorithms and proposes a unifying framework.",
        "RQ4": "Some numerical illustrations of the reviews algorithms.",
        "Limitations": "Not applicable.",
        "Keywords": [
            "literature review",
            "framework",
            "algorithm",
            "decentralised stochastic gradient descent",
            "decentralised stochastic optimisation",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["data sharing", "privacy"],
        "Stage": ["deployment"],
        "Method": "literature review",
        "AuthorAffiliation": ["Carnegie Mellon University, United States", "Tufts University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 89
    },
    {
        "Title": "Deep Robust Reinforcement Learning for Practical Algorithmic Trading",
        "BibtexKey": "liDeepRobustReinforcement2019",
        "Year": 2019,
        "Overview": "Propose to use stacked denoising autoencoders (SDAEs) to infer robust representations of financial time series data that is resilient to non-stationary distributions. The derived representations are then used in an RL framework to train trading agents.",
        "RQ1": "Risk for non-robust representations to non-stationary financial time series are addressed.",
        "RQ2": "A specific representation learning algorithm is used to train a trading agent.",
        "RQ3": "A task-specific and data-specific algorithm is used.",
        "RQ4": "Evaluation on historical trading shows that the proposed system outperforms existing baselines.",
        "Limitations": "The trained agents rely on black box systems which are difficult to understand in case of non-optimal behaviour.",
        "Keywords": [
            "reinforcement learning",
            "stacked denoising autoencoder",
            "financial trading",
            "robustness",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["algorithmic trading", "Markov decision process (MDP)", "deep neural network", "reinforcement learning"],
        "RiskTypes": ["non-stationarity", "out-of-distribution (OOD)"],
        "Stage": ["deployment", "operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Sun Yat-sen University, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 133
    },
    {
        "Title": "Deep Unsupervised Convolutional Domain Adaptation",
        "BibtexKey": "zhuoDeepUnsupervisedConvolutional2017",
        "Year": 2017,
        "Overview": "Addresses domain adaptation where the feature representation learned in the source domain with rich label information needs to be adapted to the target domain with less or even no label information. Proposes an attention transfer process for convolutional domain adaptation, which performs domain discrepancy minimization on the second-order correlation statistics of the attention maps.",
        "RQ1": "Addresses risk from domain adaptation for image data classification.",
        "RQ2": "A concrete algorithm is presented which jointly minimises a supervised classification loss and a joint correlation alignment loss.",
        "RQ3": "A concrete neural architecture and algorithm is proposed for domain adaptation.",
        "RQ4": "Evaluation on benchmark datasets shows somewhat improved classification accuracy on some measures for domain adaptation as compared to the baseline.",
        "Limitations": "Evaluation results are mixed.",
        "Keywords": [
            "domain adaptation",
            "image classification",
            "semi-supervised learning",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["unsupervised domain adaptation", "deep learning", "attention model", "correlation alignment"],
        "RiskTypes": ["out-of-distribution (OOD)"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Chinese Academy of Sciences, China", "Harbin Institute of Technology, China"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 112
    },
    {
        "Title": "Developing Safer AI-Concepts From Economics To The Rescue",
        "BibtexKey": "maskaraDevelopingSaferAIconcepts2023",
        "Year": 2023,
        "Overview": "Addresses the long-termist issues of extinction risks from rogue AIs. Proposes a decentralised framework for AI alignment that relies on existing economic mechanisms and regulations to drive AI competition, self-regulation, and decentralisation in a way to limit the potential risks from one single actor or AI itself going rogue.",
        "RQ1": "Long-termist risks from maleficient use of superintelligent AI or rogue AI are considered.",
        "RQ2": "Proposes a theoretical approach to regulation AI based on the economic principles of competition and marginal analysis, as well as regulation and subsidies to preventing from AIs being used for maleficient purposes. ",
        "RQ3": "An extremely high-level framework is proposed for mitigating existential risks from AI.",
        "RQ4": "Not evaluated.",
        "Limitations": "Not evaluated. The proposed framework is extremely high-level and, in its current form, only provides some guidelines rather than actionable ideas.",
        "Keywords": [
            "theoretical",
            "framework",
            "economics",
            "existential risk"
        ],
        "AuthorKeywords": ["existential risk", "safe AI", "AI regulation", "AI policy", "X risks", "decentralized AI"],
        "RiskTypes": ["existential", "holistic"],
        "Stage": ["operation"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Nova Southeastern University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training",
        "BibtexKey": "mengDistantlySupervisedNamedEntity2021",
        "Year": 2021,
        "Overview": "Addresses named entity recognition in natural language text. Proposes a noise-robust learning scheme comprised of a new loss function and a noisy label removal step, for training NER models on distantly-labeled data, and a self-training method that uses contextualized augmentations created by pre-trained language models to improve the generalization ability of the NER model.",
        "RQ1": "Addresses risk from noisy labels.",
        "RQ2": "Proposes a concrete neural architecture for NER with distant supervision that is robust to noisy labels.",
        "RQ3": "A concrete algorithm is proposed for a specific task.",
        "RQ4": "Evaluation on benchmark datasets and comparisons to existing baselines show that the proposed algorithm outperforms baselines on almost all tasks and measures.",
        "Limitations": "",
        "Keywords": [
            "named entity recognition",
            "machine learning",
            "natural language processing",
            "unsupervised learning",
            "pre-trained models",
            "algorithm",
            "applied",
            "robustness"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["noise", "noisy labels"],
        "Stage": ["deployment", "operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Illinois Urbana-Champaign, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 55
    },
    {
        "Title": "Dual Learning-Based Safe Semi-Supervised Learning",
        "BibtexKey": "ganDualLearningBasedSafe2018",
        "Year": 2018,
        "Overview": "Addresses safe semi-supervised learning where a small number of labeled samples and a large number of unlabeled samples are available and where the samples might decrease prediction accuracy (here defined as the risk).Proposes to use a least squares regression to predict the safety of samples in the labeled dataset and then tries to reconstruct an unlabeled sample from the predictions and the labeled dataset. Measures safety by calculating the Euclidean distance between the original sample and the reconstructed one.",
        "RQ1": "Addresses risk of performance degradation in semi-supervised learning with unlabeled samples.",
        "RQ2": "Proposes a two-stage system for detecting and filtering out unsafe unlabeled samples.",
        "RQ3": "A concrete algorithm is proposed for semi-supervised learning.",
        "RQ4": "Evaluation on 8 benchmark datasets show that the proposed method can reliable improve classification accuracy by filtering out unsafe samples.",
        "Limitations": "Prediction accuracy of the system is still overall low.",
        "Keywords": [
            "machine learning",
            "semi-supervised learning",
            "robustness",
            "classification",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["safety", "classification algorithms", "supervised learning", "artificial intelligence"],
        "RiskTypes": ["outliers", "noise"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Hangzhou Dianzi University, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 19
    },
    {
        "Title": "ECCOLA - A Method For Implementing Ethically Aligned AI Systems",
        "BibtexKey": "vakkuriECCOLAMethodImplementing2021",
        "Year": 2021,
        "Overview": "Proposes ECCOLA which is an actionable, agile framework for implementing AI ethics into real-world products based on 21-stage modular pipeline. ECCOLA uses a card-based system for a given ethical concept in the software engineering workflow where each card describes the motivation, steps to implementation, and a practical example of that ethical concept.",
        "RQ1": "Addresses a wide range of risks from the lack of unactionable implementation frameworks for real-world products utilising AI.",
        "RQ2": "Proposes an actionable, agile framework for software engineering processes to implement ethics into products.",
        "RQ3": "An actionable framework is proposed.",
        "RQ4": "Not evaluated.",
        "Limitations": "Not evaluated.",
        "Keywords": [
            "framework", 
            "software engineering",
            "agile development",
            "actionable ethics",
            "theoretical"
        ],
        "AuthorKeywords": ["artificial intelligence", "AI ethics", "ethics", "implementing", "method"],
        "RiskTypes": ["holistic", "ethical"],
        "Stage": ["design", "deployment", "operation"],
        "Method": "design framework",
        "AuthorAffiliation": ["University of Jyväskylä, Finland"],
        "AuthorAffiliationType": ["university"],
        "Citations": 79
    },
    {
        "Title": "Embedding Responsibility In Intelligent Systems: From AI Ethics To Responsible AI Ecosystems",
        "BibtexKey": "stahlEmbeddingResponsibilityIntelligent2023",
        "Year": 2023,
        "Overview": "Proposes a sketch and gives some motivation for understanding AI systems, in particular responsible AI, as part of its ecosystem. The outlined framework introduces meta-responsibility as a term to describe characteristics of ecosystems that can facilitate the responsible deployment of AI.",
        "RQ1": "Addresses risk from a narrow definition of responsible AI.",
        "RQ2": "Sketchs a high-level framework of meta-responsibility as part of AI ecosystems that enforce responsible AI.",
        "RQ3": "A high-level framework is sketched out to motivate the more holistic understanding of AI systems.",
        "RQ4": "Not applicable.",
        "Limitations": "An opinion paper whose ideas are difficult to adapt in the real world. It is more of a thought experiment of AI ecosystems.",
        "Keywords": [
            "AI ethics",
            "responsible AI",
            "meta-responsibility",
            "framework",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["holistic", "ethical", "responsibility"],
        "Stage": [],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["University of Nottingham, UK", "De Montfort University, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 12
    },
    {
        "Title": "Establishing Data Provenance for Responsible Artificial Intelligence Systems",
        "BibtexKey": "werderEstablishingDataProvenance2022",
        "Year": 2022,
        "Overview": "This discusses data provenance, the need for artifacts to document the origins and processing of data for AI systems. Recommends best practices to establish data provenance throughout the lifecycle of the AI-based product. To this end also reviews biases and challenges regarding fairness, accountability, transparency, and explainability.",
        "RQ1": "Addresses risk from lack of understanding about the origins and processing of data.",
        "RQ2": "Recommends best practices for data provenance.",
        "RQ3": "Some high-level recommendations are proposed.",
        "RQ4": "Not evaluated.",
        "Limitations": "More or less a review paper without evaluation or strong real-world evidence for support.",
        "Keywords": [
            "organisational data governance",
            "data traceability",
            "data provenance",
            "framework",
            "theoretical",
            "literature review"
        ],
        "AuthorKeywords": ["data provenance", "artificial intelligence", "fairness", "accountability", "transparency", "explainability"],
        "RiskTypes": ["data quality", "data requirements", "holistic"],
        "Stage": [],
        "Method": "literature review",
        "AuthorAffiliation": ["University of Cologne, Germany", "Georgia State University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 35
    },
    {
        "Title": "Establishing Verification and Validation Objectives for Safety-Critical Bayesian Networks",
        "BibtexKey": "douthwaiteEstablishingVerificationValidation2017",
        "Year": 2017,
        "Overview": "Introduces a framework that facilitates the structured description of Bayesian Network-based systems for modeling conventional engineering and implementation aspects, as well as novel aspects arising from the operation and utilisation of these systems in a safety-critical context. This then provides the structured basis for the description of BN-based system error modes and associated verification and validation objectives.",
        "RQ1": "Risk stemming from BN-based software systems are addressed especially in safety-critical applications.",
        "RQ2": "Proposes a structured framework to developing and understanding the failure modes of BN-based software systems.",
        "RQ3": "A concrete design and deployment framework is proposed for BN-based systems.",
        "RQ4": "Not evaluated.",
        "Limitations": "No evaluation, the framework is overly complex and the figures are difficult to comprehend.",
        "Keywords": [
            "framework",
            "theoretical",
            "bayesian network",
            "software engineering"
        ],
        "AuthorKeywords": ["Bayesian networks", "autonomous systems", "machine learning", "safety critical", "mission critical assurance", "reference model"],
        "RiskTypes": ["holistic"],
        "Stage": ["design"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of York, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 6
    },
    {
        "Title": "Evaluating Correctness of Reinforcement Learning based on Actor-Critic Algorithm",
        "BibtexKey": "kimEvaluatingCorrectnessReinforcement2022",
        "Year": 2022,
        "Overview": "Proposes some measures to evaluate and visualise the learned policies of some actor-critic training algorithms for reinforcement learning. ",
        "RQ1": "Risk from incorrect training of RL algorithms is addressed.",
        "RQ2": "A method to measure and visualise the correctness of RL algorithms are addressed.",
        "RQ3": "A concrete method for evaluation is proposed.",
        "RQ4": "Evaluation on simulated numerical experiment shows some qualitative insight into the workings of actor-critic RL training.",
        "Limitations": "Very surface-level measures and very superficial analysis.",
        "Keywords": [
            "reinforcement learning",
            "visualisation",
            "evaluation",
            "correctness",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["reinforcement learning", "actor-critic algorithm", "safety-critical system", "quality evaluation", "correctness"],
        "RiskTypes": ["correctness"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Chungbuk National University, Republic of Korea"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Evaluating Model-Free Reinforcement Learning toward Safety-Critical Tasks",
        "BibtexKey": "zhangEvaluatingModelFreeReinforcement2023",
        "Year": 2023,
        "Overview": "Proposes a unified framework, SafeRL-Kit, for model-free safe reinforcement learning containing projection-based, recovery-based, and optimization-based approaches. Then extends these methods with the unrolling safety layer approach which is a two-stage method that takes the initial near-optimal action of the trained model and iteratively performs gradient-based action updates to enforce hard safety constraints.",
        "RQ1": "Addresses risk from unsafe exploration and actions in model-free RL.",
        "RQ2": "A concrete method is proposed to enforce hard safety constraints on model-free RL agents.",
        "RQ3": "A unified safe model-free RL framework and a specific model-free safe RL algorithm is proposed.",
        "RQ4": "Extensive evaluation on several environments shows that the method outperforms existing methods.",
        "Limitations": "The proposed method is 4 to 5 times slower than baselines.",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "unrolling safety layer",
            "applied",
            "algorithm",
            "framework",
            "model-free RL",
            "safety-critical system"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["unsafe actions", "unsafe exploration"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Tsinghua University, China", "JD.com, Inc., China", "Qianyuan Institute of Sciences, China"],
        "AuthorAffiliationType": ["university", "research institution", "corporate"],
        "Citations": 8
    },
    {
        "Title": "eXplainable and Reliable Against Adversarial Machine Learning in Data Analytics",
        "BibtexKey": "vaccariEXplainableReliableAdversarial2022",
        "Year": 2022,
        "Overview": "Proposes to use detection algorithms to train adversarially robust ML models based on datasets of mixed real and adversarial data, where the adversarial samples come from three types of attacks:  Carlini-Wagner (CW), the fast gradient sign method (FGSM) and the Jacobian based saliency map (JSMA).",
        "RQ1": "Addresses risk from adversarial attacks on ML systems.",
        "RQ2": "An algorithm based on logic learning machines and support vector data description is proposed to improve ML adversarial robustness.",
        "RQ3": "A concrete algorithm is proposed for general ML classification.",
        "RQ4": "Evaluation on three tasks shows improved performance compared to several baselines.",
        "Limitations": "Limited novelty of the method.",
        "Keywords": [
            "machine learning",
            "classification",
            "adversarial detection",
            "algorithm",
            "applied",
            "support vector data description",
            "logic learning machine",
            "robustness"
        ],
        "AuthorKeywords": ["machine learning", "detection algorithms", "adversarial machine learning", "reliable"],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["deployment", "operation"],
        "Method": "applied algorithm", 
        "AuthorAffiliation": ["Consiglio Nazionale delle Ricerche, Italy"],
        "AuthorAffiliationType": ["university"],
        "Citations": 6
    },
    {
        "Title": "Explicit Explore, Exploit, or Escape (E4): Near-Optimal Safety-Constrained Reinforcement Learning In Polynomial Time",
        "BibtexKey": "bossensExplicitExploreExploit2023",
        "Year": 2023,
        "Overview": "Proposes to solve constrained MDPs in a model-based RL setting by combining an exploitation policy for sub-optimal known states, an exploration policy for unknown states, an escape policy to known states from unsafe unknown states, and an exploitation policy for optimal known states.",
        "RQ1": "Risk from unsafe exploration is addressed in model-based RL.",
        "RQ2": "A concrete model-based training algorithm is proposed where the constrained MDPs are solved especially using an escape policy from unsafe unknown states.",
        "RQ3": "A concrete algorithm is proposed to solve constrained MDPs.",
        "RQ4": "No evaluation, only some theoretical discussions.",
        "Limitations": "No evaluation!",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "constrained MDP",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["safe artificial intelligence", "safe exploration", "model-based reinforcement learning", "constrained Markov decision processes", "robust Markov decision processes"],
        "RiskTypes": ["unsafe exploration", "unsafe states"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["University of Southampton, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3
    },
    {
        "Title": "Exploring Fault Parameter Space Using Reinforcement Learning-Based Fault Injection",
        "BibtexKey": "moradiExploringFaultParameter2020",
        "Year": 2020,
        "Overview": "Proposes to use reinforcement learning to explore the failure space of safety-critical environments by strategically learning to use fault injection. During the learning process, the RL agent injects and parameterizes faults in the system to cause catastrophic behavior. The fault space is explored based on a reward function that evaluates previous simulation results such that the RL technique tries to predict improved fault timing and values.",
        "RQ1": "Risk from failure states in safety-critical systems are addressed.",
        "RQ2": "A method combining RL and failure injection is proposed to understand the failure space of safety-critical systems.",
        "RQ3": "A concrete algorithm is proposed to explore and identify the failure space of safety-critical systems.",
        "RQ4": "Evaluation shows that the proposed technique is more efficient in terms of fault coverage and time to find the first critical fault.",
        "Limitations": "Limited evaluation.",
        "Keywords": [
            "safety-critical system",
            "reinforcement learning",
            "fault injection",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["fault injection", "reinforcement learning", "safety assessment", "cyber-physical systems", "machine learning"],
        "RiskTypes": ["fault"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Antwerp and Flanders, Belgium", "Technische Universität Dresden, Germany"],
        "AuthorAffiliationType": ["university"],
        "Citations": 31
    },
    {
        "Title": "Extreme Learning Machine: A Robust Modeling Technique? Yes!",
        "BibtexKey": "lendasseExtremeLearningMachine2013",
        "Year": 2013,
        "Overview": "Reviews extreme learning machine-based robust regression methods and proposes two new methods to improve the robustness of ELMs to outliers.",
        "RQ1": "Risk from noisy data and outliers are addressed.",
        "RQ2": "Two algorithms which are extensions of the original ELM are proposed to improve the robustness of regression results.",
        "RQ3": "Two methods are proposed with ELMs.",
        "RQ4": "Evaluation on 10 tabular datasets show more robust modelling compared to baselines.",
        "Limitations": "Unclear how well the method scales to higher-dimensional data.",
        "Keywords": [
            "extreme learning machine",
            "robustness",
            "machine learning",
            "algorithm",
            "literature review",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["outliers", "noise"],
        "Stage": ["deployment"],
        "Method": "literature review",
        "AuthorAffiliation": ["Aalto School of Science and Technology, Finland", "Basque Foundation for Science, Spain", "University of the Basque Country, Spain"],
        "AuthorAffiliationType": ["university", "government"],
        "Citations": 19
    },
    {
        "Title": "FairRover: Explorative model building for fair and responsible machine learning",
        "BibtexKey": "zhangFairRoverExplorativeModel2021",
        "Year": 2021,
        "Overview": "Proposes an iterative framework for exploring the fairness biases in machine learning models and improving the ML model based on the explanatory information uncovered during the exploration so as to achieve an acceptable trade-off between various competing measures of fairness.",
        "RQ1": "Risk from unfair ML systems are addressed.",
        "RQ2": "An exploratory development framework is proposed for iterative model improvements.",
        "RQ3": "A theoretical development framework is proposed.",
        "RQ4": "A case study is presented on the adult census dataset which shows that the framework can achieve a level fairness trade-off that might be acceptable in some deployment scenarios, however, more shortcomings are discovered.",
        "Limitations": "The proposed framework is human-in-the-loop which slows down deployment time and the system as proposed only supports binary classification tasks.",
        "Keywords": [
            "machine learning",
            "classification",
            "framework",
            "theoretical",
            "iterative development workflow",
            "AI ethics",
            "human-in-the-loop"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["bias", "fairness"],
        "Stage": ["design", "deployment"],
        "Method": "design framework",
        "AuthorAffiliation": ["Georgia Institute of Technology, United States", "University of Illinois at Chicago, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 17
    },
    {
        "Title": "FedEqual: Defending Model Poisoning Attacks in Heterogeneous Federated Learning",
        "BibtexKey": "chenFedEqualDefendingModel2021",
        "Year": 2021,
        "Overview": "Proposes a method to counteract model poisoning in a federated learning setting, where the weights of one local model significantly disrupt the updates of the global model. The proposed method normalises the weights of each local model and relies on the majority to outweigh the influence of the poisoned model. This ensures that benign models are not removed as outliers from the model updates.",
        "RQ1": "Risk from model poisoning in federated learning is addressed.",
        "RQ2": "A concrete federated learning algorithm is proposed to counteract the effects of model poisoning.",
        "RQ3": "A task-agnostic federated learning algorithm is proposed.",
        "RQ4": "Evaluation on some simple benchmark tasks show improved resilience against model poisoning attacks.",
        "Limitations": "The method makes the very strong assumption that there is a majority of weights that would counteract the poisoned model.",
        "Keywords": [
            "machine learning",
            "federated learning",
            "algorithm",
            "applied",
            "weight normalisation"
        ],
        "AuthorKeywords": ["edge AI", "federated learning", "model poisoning", "attacks", "model security", "system robustness"],
        "RiskTypes": ["adversarial attack", "model poisoning"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["National Taiwan University, Taiwan", "Academia Sinica, Taiwan"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 11
    },
    {
        "Title": "Flash Crashes in Multi-Agent Systems Using Minority Games and Reinforcement Learning to Test AI Safety",
        "BibtexKey": "canonicoFlashCrashesMultiAgent2019",
        "Year": 2019,
        "Overview": "Reviews and addresses flash crashes as an emergent behaviour of high-frequency multi-agent autonomous decision-making. Proposes an alternative version of the Minority Game and tests it with several standard MARL algorithms.",
        "RQ1": "Risk from catastrophic emergent behaviour in MARL is addressed.",
        "RQ2": "Proposes a modified version of the Minority Game to study emergent behaviour in MARL.",
        "RQ3": "A toy task is proposed for studying emergent behaviour in a controlled manner.",
        "RQ4": "Evaluation shows that the proposed game is sufficient to induce flash crashes in MARL.",
        "Limitations": "No efforts are made to recommend improvements to stop flash crashes from happening.",
        "Keywords": [
            "reinforcement learning",
            "multi-agent RL (MARL)",
            "multi-agent games",
            "high-frequency trading",
            "theoretical",
            "framework"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["flash crash", "emergent behaviour"],
        "Stage": ["operation"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Clemson University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1
    },
    {
        "Title": "From Explainable AI to Explainable Simulation: Using Machine Learning and XAI to Understand System Robustness",
        "BibtexKey": "feldkampExplainableAIExplainable2023",
        "Year": 2023,
        "Overview": "Addresses issues of robustness to noise in classical high-scale simulation-based predictions. Proposes a framework to combine simulations with explainable methods to analyse the factors that affect robustness in the simulation data-based ML predictions.",
        "RQ1": "Risks from noise and high-variance data is addressed.",
        "RQ2": "A frame and concrete method is proposed for a better understanding of sources of robustness and instability in simulation-based data-trained ML systems.",
        "RQ3": "A framework for simulation and ML classification is proposed.",
        "RQ4": "A case study is presented with detailed qualitative analysis of factors of robust predictions.",
        "Limitations": "A high-level framework is proposed but concrete algorithms are missing. It is also unclear how well the framework scales to large simulations.",
        "Keywords": [
            "machine learning",
            "classification",
            "simulation",
            "framework",
            "applied",
            "robustness"
        ],
        "AuthorKeywords": ["machine learning", "deep learning", "robustness optimization", "simulation", "explainable AI", "XAI"],
        "RiskTypes": ["robustness", "noise"],
        "Stage": ["deployment", "operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Technische Universität Ilmenau, Germany"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1
    },
    {
        "Title": "Gray-box Adversarial Training",
        "BibtexKey": "vivekGrayboxAdversarialTraining2018",
        "Year": 2018,
        "Overview": "Identifies the issue of pseudo-robustness in adversarially trained ML models. Introduces an evaluation procedure using robustness plots and a worst-case performance metric. The evaluation method is based on a mixture of white- and black-box attacks that are temporally evolved.",
        "RQ1": "Risk from a lack of adversarial robustness is addressed.",
        "RQ2": "Proposes a concrete algorithm to train more robust ML classifiers based on temporally evolving both white- and black-box adversarial attacks. Based on this, a new gray-box adversarial training algorithm is proposed",
        "RQ3": "A general algorithm for the evaluation and training of ML adversarial training is proposed.",
        "RQ4": "Extensive evaluation with benchmark datasets and various baselines show that the recommended evaluation method uncovers issues of pseudo-robustness.",
        "Limitations": "The adversarial attacks are evolved temporally but it is unclear how this evolution should be done and whether this affects the results of the paper.",
        "Keywords": [
            "machine learning",
            "evaluation",
            "adversarial learning",
            "adversarial attack",
            "robustness",
            "algorithm",
            "applied",
            "gray-box adversarial training"
        ],
        "AuthorKeywords": ["adversarial perturbations", "attacks on machine learning models", "adversarial training", "robust machine learning models"],
        "RiskTypes": ["adversarial attack", "robustness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Indian Institute of Science, India"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 40
    },
    {
        "Title": "Hard Choices In Artificial Intelligence",
        "BibtexKey": "dobbeHardChoicesArtificial2021",
        "Year": 2021,
        "Overview": "Suggests that deliberation of the context and politics of AI deployment is necessary for AI safety besides mathematical formalisms. Proposes a framework called Hard choices in AI, which empowers developers by identifying points of overlap between design decisions and major socio-technical challenges, and motivating the creation of stakeholder feedback channels so that safety issues can be exhaustively addressed.",
        "RQ1": "Risk from vague definitions of AI safety is considered.",
        "RQ2": "A high-level theoretical framework is proposed to drive deliberation around AI safety.",
        "RQ3": "A very-high level task and method-agnostic framework is proposed.",
        "RQ4": "Not evaluated.",
        "Limitations": "Very high-level philosophical work that is likely going to be overridden by more actionable frameworks.",
        "Keywords": [
            "socio-technical system",
            "philosophical",
            "framework",
            "theoretical",
            "deliberation"
        ],
        "AuthorKeywords": ["AI ethics", "AI safety", "AI governance", "AI regulation", "philosophy of artificial intelligence", "sociotechnical systems"],
        "RiskTypes": ["holistic", "ethical"],
        "Stage": ["design"],
        "Method": "philosophical",
        "AuthorAffiliation": ["Delft University of Technology, Netherlands", "University of California at Berkley, United States", "University of Wisconsin-Madison, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 47
    },
    {
        "Title": "Have a Break from Making Decisions, Have a MARS: The Multi-valued Action Reasoning System",
        "BibtexKey": "badeaHaveBreakMaking2022",
        "Year": 2022,
        "Overview": "Suggests a value-based moral reasoning model for AI-based agents. Given a set of available actions and an underlying moral paradigm, by employing MARS one can identify the ethically preferred action. It can be used to implement and model different ethical theories, different moral paradigms, as well as combinations of such.",
        "RQ1": "Risk from unaligned or unethical actions are addressed.",
        "RQ2": "A workflow and framework are proposed for creating ethical reasoning systems based on values are proposed.",
        "RQ3": "A general and model-agnostic workflow is proposed for any reasoning system with actions.",
        "RQ4": "Not evaluated.",
        "Limitations": "Not evaluated and no algorithms are proposed.",
        "Keywords": [
            "moral reasoning",
            "value alignment",
            "value ethics",
            "framework",
            "theoretical"
        ],
        "AuthorKeywords": ["mathematical models for AI", "logics for AI", "multi-criteria decision-making", "AI ethics", "value alignment", "intelligent agents", "expert and knowledge-based systems"],
        "RiskTypes": ["ethical"],
        "Stage": ["design"],
        "Method": "design framework",
        "AuthorAffiliation": ["Imperial College London, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 6
    },
    {
        "Title": "HiSaRL: A Hierarchical Framework for Safe Reinforcement Learning",
        "BibtexKey": "xiongHiSaRLHierarchicalFramework2022",
        "Year": 2022,
        "Overview": "Propose a two-level hierarchical safe RL method. The high-level part is an adaptive planner, which aims at learning and generating safe and efficient paths for tasks with imperfect map information. The lower-level part contains a learning-based controller and its corresponding neural Lyapunov function, which characterizes the controller's stability property.",
        "RQ1": "Risk from unsafe actions and noise is addressed.",
        "RQ2": "Proposes a specific model-free safe RL training algorithm to avoid unsafe states for trajectory planning while being robust to noise.",
        "RQ3": "A task-specific safe RL training algorithm is proposed.",
        "RQ4": "Limited evaluation that does not show much by way of the benefits of the system.",
        "Limitations": "Very limited evaluation, unclear how this method is better than baselines.",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "adversarial attack",
            "robustness",
            "Lyapunov function",
            "control theory",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["noise", "adversarial attack", "unsafe actions"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Purdue University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 6
    },
    {
        "Title": "Human-Guided Reinforcement Learning With Sim-to-Real Transfer for Autonomous Navigation",
        "BibtexKey": "wuHumanGuidedReinforcementLearning2023",
        "Year": 2023,
        "Overview": "Propose a human-guided RL framework for resource-constrained navigation tasks of unmanned ground vehicles. The method allows training of neural RL agents in simulation with the incorporation of human feedback based on a human-guided learning objective, prioritized human experience replay, and human intervention-based reward shaping. The resulting model can then be de-noised to improve domain adaptation and deployed on a small footprint controller.",
        "RQ1": "Risk from suboptimal RL control and undeployable systems is addressed.",
        "RQ2": "A complete low-footprint human-guided RL training framework is proposed for navigation tasks of unmanned ground vehicles.",
        "RQ3": "A task and domain-specific RL training algorithm is proposed.",
        "RQ4": "Extensive evaluation shows that the proposed method performs better than baselines and has a smaller footprint than other methods.",
        "Limitations": "The method depends, at least initially, on human monitoring which can burdensome and time-intensive.",
        "Keywords": [
            "reinforcement learning", 
            "low resource training",
            "reinforcement learning with human feedback",
            "safe RL",
            "domain adaptation",
            "denoising",
            "framework",
            "applied",
            "unmanned ground vehicle"
        ],
        "AuthorKeywords": ["reinforcement learning", "human guidance", "navigation", "sim-to-real transfer", "unmanned ground vehicle"],
        "RiskTypes": ["robustness", "resource constraints", "noise"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Nanyang Technological University, Singapore"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3
    },
    {
        "Title": "Impact-learning: A robust machine learning algorithm",
        "BibtexKey": "kowsherImpactlearningRobustMachine2020",
        "Year": 2020,
        "Keywords": [
            "robustness",
            "impact learning",
            "supervised learning",
            "competition",
            "applied",
            "framework"
        ],
        "AuthorKeywords": ["impact learning", "classification", "regression", "machine learning"],
        "RiskTypes": ["robustness"],
        "Stage": ["design"],
        "Method": "design framework",
        "AuthorAffiliation": ["Noakhali Science and Technology University, Bangladesh", "Western Carolina University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 21
    },
    {
        "Title": "Imperative Action Masking for Safe Exploration in Reinforcement Learning",
        "BibtexKey": "deyImperativeActionMasking2023",
        "Year": 2023,
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "safe exploration",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["graph-plan algorithm", "explainable/interpretable machine learning", "reinforcement learning", "exploration considering safety"],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["deployment", "operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Indian Institute of Technology Kharagpur, India"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 0
    },
    {
        "Title": "Improving safety in reinforcement learning using model-based architectures and human intervention",
        "BibtexKey": "prakashImprovingSafetyReinforcement2019",
        "Year": 2019,
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "human oversight",
            "sample efficiency"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["unsafe exploration"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["University of Maryland, United States", "US Army Research Laboratory, United States"],
        "AuthorAffiliationType": ["university", "government"],
        "Citations": 23
    },
    {
        "Title": "Improving the robustness of instance-based reinforcement learning robots by metalearning",
        "BibtexKey": "yasudaImprovingRobustnessInstancebased2011",
        "Year": 2011,
        "Keywords": [
            "safe RL",
            "reinforcement learning"    ,
            "Bayesian discrimination function",
            "multi-agent RL (MARL)",
            "robustness",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["metalearning", "multi-robot system", "reinforcement learning", "robustness"],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Hiroshima University, Japan"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3
    },
    {
        "Title": "Induction of fault trees through Bayesian networks",
        "BibtexKey": "linardInductionFaultTrees2020",
        "Year": 2020,
        "Keywords": [
            "cyber-physical system",
            "systemic safety",
            "fault tree",
            "reliability",
            "bayesian network"
        ],
        "AuthorKeywords": ["fault tree induction", "safety-critical systems", "cyber-physical systems", "machine learning", "Bayesian network", "inference", "risk analysis", "failure diagnosis"],
        "RiskTypes": ["fault"],
        "Stage": ["design"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Radboud University, Netherlands", "University of Twente, Netherlands", "Federal University of Uberlândia, Brazil"],
        "AuthorAffiliationType": ["university"],
        "Citations": 9
    },
    {
        "Title": "Inferring Human Values for Safe AGI Design",
        "BibtexKey": "sezenerInferringHumanValues2015",
        "Year": 2015,
        "Keywords": [
            "value alignment",
            "artificial general intelligence (AGI)",
            "philosophical",
            "theoretical",
            "framework"
        ],
        "AuthorKeywords": ["value learning", "inverse reinforcement learning", "friendly ai", "safe agi"],
        "RiskTypes": ["existential", "alignment"],
        "Stage": ["design"],
        "Method": "philosophical",
        "AuthorAffiliation": ["Ozyegin University, Turkey"],
        "AuthorAffiliationType": ["university"],
        "Citations": 6
    },
    {
        "Title": "Interpretable semi-parametric regression models with defined error bounds",
        "BibtexKey": "otteInterpretableSemiparametricRegression2014",
        "Year": 2014,
        "Keywords": [
            "regression",
            "machine learning",
            "theoretical",
            "algorithm",
            "error bound",
            "interpretability"
        ],
        "AuthorKeywords": ["interpretable models", "machine learning for safety-related systems", "gaussian process", "symbolic regression"],
        "RiskTypes": ["domain generalization"],
        "Stage": ["operation"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Siemens AG, Germany"],
        "AuthorAffiliationType": ["corporate"],
        "Citations": 0
    },
    {
        "Title": "Joint Adversarial Domain Adaptation With Structural Graph Alignment",
        "BibtexKey": "wangJointAdversarialDomain2023",
        "Year": 2023,
        "Keywords": [
            "generative adversarial networks",
            "domain adaptation",
            "distribution shift",
            "structural graph alignment",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["conditional distribution", "joint adversarial domain adaptation", "joint distribution", "marginal distribution", "structural graph alignment"],
        "RiskTypes": ["domain adaptation"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Shenzhen University, China", "National University of Defense Technology, China", "Anhui University, China", "Meituan, China"],
        "AuthorAffiliationType": ["university", "government", "corporate"],
        "Citations": 1
    },
    {
        "Title": "Learning From Noisy Labels Via Dynamic Loss Thresholding",
        "BibtexKey": "yangLearningNoisyLabels2023",
        "Year": 2023,
        "Keywords": [
            "deep learning",
            "dynamic loss threshold",
            "noisy labels",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["deep learning", "dynamic loss thresholding", "learning from noisy labels", "semi-supervised learning"],
        "RiskTypes": ["noisy labels"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Southeast University, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3
    },
    {
        "Title": "Joint Domain Adaptation Based on Adversarial Dynamic Parameter Learning",
        "BibtexKey": "yuanJointDomainAdaptation2021",
        "Year": 2021,
        "Keywords": [
            "classification",
            "distribution shift",
            "conditional distribution",
            "adversarial learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["domain adaptation", "joint distribution alignment", "adversarial learning", "dynamic distribution alignment"],
        "RiskTypes": ["domain adaptation"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Huazhong University of Science and Technology, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 5
    },
    {
        "Title": "Learning to safely approve updates to machine learning algorithms",
        "BibtexKey": "fengLearningSafelyApprove2021",
        "Year": 2021,
        "Keywords": [
            "machine learning",
            "continual learning",
            "approval policy",
            "applied",
            "framework"
        ],
        "AuthorKeywords": ["AI/ML-based SaMD", "non-stationarity", "online learning", "prediction with expert advice"],
        "RiskTypes": ["continuous deployment", "non-stationarity"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of California at San Francisco, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 5
    },
    {
        "Title": "Look Before You Leap: Safe Model-Based Reinforcement Learning with Human Intervention",
        "BibtexKey": "xuLookYouLeap2021",
        "Year": 2021,
        "Keywords": [
            "deep learning",
            "reinforcement learning",
            "safe RL",
            "reinforcement learning with human feedback",
            "model-based RL",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["model predict control", "model-based rl", "safety rl"],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Zhejiang University, China", "Huawei, China"],
        "AuthorAffiliationType": ["university", "corporate"],
        "Citations": 8
    },
    {
        "Title": "Lyapunov design for safe reinforcement learning",
        "BibtexKey": "perkinsLyapunovDesignSafe2003",
        "Year": 2003,
        "Keywords": [
            "Lyapunov function",
            "control theory",
            "safe RL",
            "hierarchical reinforcement learning",
            "theoretical",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["reinforcement learning", "lyapunov functions", "safety", "stability"],
        "RiskTypes": ["unsafe states", "unsafe actions"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["University of Massachusetts at Amherst, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 226
    },
    {
        "Title": "Meaningful Machine Learning Robustness Evaluation in Real-World Machine Learning Enabled System Contexts",
        "BibtexKey": "hiettMeaningfulMachineLearning2022",
        "Year": 2022,
        "Keywords": [
            "robustness",
            "machine learning",
            "evaluation"
        ],
        "AuthorKeywords": ["machine learning", "meaningful perturbations", "object detection", "performance metrics", "verification & validation"],
        "RiskTypes": ["robustness"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Defence Science and Technology Laboratory, UK", "University of Oxford, UK"],
        "AuthorAffiliationType": ["government", "university"],
        "Citations": 0
    },
    {
        "Title": "Mechanisms and Constraints Underpinning Ethically Aligned Artificial Intelligence Systems: An Exploration of key Performance Areas",
        "BibtexKey": "treacyMechanismsConstraintsUnderpinning2021",
        "Year": 2021,
        "Keywords": [
            "fairness",
            "bias",
            "AI ethics",
            "theoretical",
            "framework"
        ],
        "AuthorKeywords": ["artifcial intelligence", "ethical development", "transparency", "accountability", "governance", "culture"],
        "RiskTypes": ["ethical"],
        "Stage": ["design"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["University College Cork, Ireland"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1
    },
    {
        "Title": "Metacognition for artificial intelligence system safety-An approach to safe and desired behavior",
        "BibtexKey": "johnsonMetacognitionArtificialIntelligence2022",
        "Year": 2022,
        "Keywords": [
            "theoretical",
            "framework",
            "safety-critical system",
            "self-awareness",
            "metacognition",
            "AI safety",
            "artificial general intelligence (AGI)"
        ],
        "AuthorKeywords": ["metacognition", "artificial intelligence systems", "machine learning", "system safety", "complexity"],
        "RiskTypes": ["systemic safety"],
        "Stage": ["design"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Naval Postgraduate School, United States"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 8
    },
    {
        "Title": "Mind the gaps: Assuring the safety of autonomous systems from an engineering, ethical, and legal perspective",
        "BibtexKey": "burtonMindGapsAssuring2020",
        "Year": 2020,
        "Keywords": [
            "autonomous system",
            "semantic gap",
            "theoretical",
            "framework",
            "responsibility gap",
            "safety analysis"
        ],
        "AuthorKeywords": ["safety", "autonomous systems", "artificial intelligence", "law", "ethics"],
        "RiskTypes": ["system specification", "systemic safety", "ethical"],
        "Stage": ["design"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Robert Bosch GmbH, Germany", "University of York, UK", "Bradford Royal Infirmary and Bradford Institute for Health Research, UK"],
        "AuthorAffiliationType": ["university", "corporate", "research institution", "government"],
        "Citations": 129
    },
    {
        "Title": "Model Checking Human-Agent Collectives for Responsible AI",
        "BibtexKey": "abeywickramaModelCheckingHumanAgent2019",
        "Year": 2019,
        "Keywords": [
            "cooperation",
            "human-in-the-loop",
            "safety analysis",
            "verifiability",
            "counterexamples",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["safety verification", "alignment"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of Southampton, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 9
    },
    {
        "Title": "Morality, Machines, and the Interpretation Problem: A Value-based, Wittgensteinian Approach to Building Moral Agents",
        "BibtexKey": "badeaMoralityMachinesInterpretation2022",
        "Year": 2022,
        "Keywords": [
            "interpretation problem",
            "artificial general intelligence (AGI)",
            "theoretical",
            "philosophical",
            "framework"
        ],
        "AuthorKeywords": [
            "AI ethics",
            "Interpretation problem",
            "Moral values",
            "Value alignment",
            "Wittgenstein",
            "Rules",
            "Virtue theory",
            "Practical reasoning",
            "Intelligent decision support systems",
            "Evaluation of AI systems"
        ],
        "RiskTypes": ["existential", "alignment"],
        "Stage": ["design"],
        "Method": "philosophical",
        "AuthorAffiliation": ["Imperial College London, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 6
    },
    {
        "Title": "MultiDIAL: Domain Alignment Layers for (Multisource) Unsupervised Domain Adaptation",
        "BibtexKey": "carlucciMultiDIALDomainAlignment2021",
        "Year": 2021,
        "Keywords": [
            "convolutional neural network", 
            "visual recognition",
            "distribution shift",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["unsupervised domain adaptation", "visual recognition", "batch normalization", "domain alignment layers", "entropy loss"],
        "RiskTypes": ["domain adaptation"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Sapienza University of Rome, Italy", "Mapillary Research, Austria", "Italian Institute of Technology, Italy", "University of Trento, Italy"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 17
    },
    {
        "Title": "Multilayered review of safety approaches for machine learning-based systems in the days of AI",
        "BibtexKey": "deyMultilayeredReviewSafety2021",
        "Year": 2021,
        "Keywords": [
            "literature review",
            "engineering process",
            "verifiability",
            "traceability"
        ],
        "AuthorKeywords": ["Autonomous systems", "Intelligent software systems", "Machine learning", "Safety analysis", "Software engineering"],
        "RiskTypes": ["problem requirements", "safety verification"],
        "Stage": ["design"],
        "Method": "literature review",
        "AuthorAffiliation": ["Ajou University, Republic of Korea"],
        "AuthorAffiliationType": ["university"],
        "Citations": 24
    },
    {
        "Title": "N-version machine learning models for safety critical systems",
        "BibtexKey": "machidaNversionMachineLearning2019a",
        "Year": 2019,
        "Keywords": [
            "safety-critical system",
            "machine learning",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": ["design diversity", "machine learning", "N-version programming", "reliability", "safety"],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["University of Tsukuba, Japan"],
        "AuthorAffiliationType": ["university"],
        "Citations": 27
    },
    {
        "Title": "Neural Simplex Architecture",
        "BibtexKey": "phanNeuralSimplexArchitecture2020",
        "Year": 2020,
        "Keywords": [
            "safe RL",
            "control theory",
            "applied",
            "algorithm",
            "simplex",
            "safe exploration"
        ],
        "AuthorKeywords": ["runtime assurance", "simplex architecture", "online retraining", "reverse switching", "safe reinforcement learning"],
        "RiskTypes": ["unsafe exploration", "unsafe actions"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Stony Brook University, United States", "Technische Universität Wien, Austria", "Radboud University, Netherlands"],
        "AuthorAffiliationType": ["university"],
        "Citations": 70
    },
    {
        "Title": "On Assessing The Safety of Reinforcement Learning algorithms Using Formal Methods",
        "BibtexKey": "mindomAssessingSafetyReinforcement2021",
        "Year": 2021,
        "Keywords": [
            "safe RL",
            "adversarial attack",
            "robustness",
            "algorithm",
            "applied",
            "framework"
        ],
        "AuthorKeywords": ["Reinforcement Learning", "Formal Specification", "Probabilistic Model Checking", "Reward Shaping"],
        "RiskTypes": ["unsafe actions", "adversarial attack"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["SWAT Lab, Canada"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 1
    },
    {
        "Title": "On conflicts between ethical and logical principles in artificial intelligence",
        "BibtexKey": "dacquistoConflictsEthicalLogical2020",
        "Year": 2020,
        "Keywords": [
            "regulatory capture",
            "AI ethics",
            "philosophical",
            "theoretical",
            "robot rights",
            "artificial general intelligence (AGI)"
        ],
        "AuthorKeywords": ["Artificial intelligence ethics", "Formal logic constraints", "Machine incompleteness", "Value alignment", "Algorithm transparency vs. explainability"],
        "RiskTypes": ["ethical"],
        "Stage": ["design"],
        "Method": "philosophical",
        "AuthorAffiliation": ["Italian Data Protection Authority, Italy"],
        "AuthorAffiliationType": ["government"],
        "Citations": 13
    },
    {
        "Title": "Out-of-Distribution Detection as Support for Autonomous Driving Safety Lifecycle",
        "BibtexKey": "henrikssonOutofDistributionDetectionSupport2023",
        "Year": 2023,
        "Keywords": [
            "machine learning",
            "visual recognition",
            "systemic safety",
            "metrics",
            "outlier detection",
            "theoretical",
            "framework"
        ],
        "AuthorKeywords": ["Automotive safety", "Out-of-Distribution detection", "Machine learning", "Automated driving systems", "Safety requirements"],
        "RiskTypes": ["out-of-distribution (OOD)", "systemic safety"],
        "Stage": ["operation", "deployment"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Semcon, Sweden", "Veoneer, Sweden", "RISE Research Institutes of Sweden, Sweden", "Agreat, Sweden", "Comentor, Sweden"],
        "AuthorAffiliationType": ["corporate", "research institution"],
        "Citations": 2
    },
    {
        "Title": "Parallel reward and punishment control in humans and robots: safe reinforcement learning using the MaxPain algorithm",
        "BibtexKey": "elfwingParallelRewardPunishment2017",
        "Year": 2017,
        "Keywords": [
            "reinforcement learning",
            "reward signal",
            "punishment signal",
            "safe RL",
            "worst-case predictions",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["ATR Computational Neuroscience Laboratories, Japan"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 25
    },
    {
        "Title": "Probabilistic Counterexample Guidance for Safer Reinforcement Learning",
        "BibtexKey": "jiProbabilisticCounterexampleGuidance2023",
        "Year": 2023,
        "Keywords": [
            "counterexamples",
            "active learning",
            "reinforcement learning",
            "reinforcement learning with human feedback",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["Safe reinforcement learning", "Probabilistic model checking", "Counterexample guidance"],
        "RiskTypes": ["unsafe exploration"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Imperial College London, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Probabilistic Model Predictive Safety Certification for Learning-Based Control",
        "BibtexKey": "wabersichProbabilisticModelPredictive2022",
        "Year": 2022,
        "Keywords": [
            "reinforcement learning",
            "method-agnostic",
            "bayesian inference",
            "safety analysis",
            "framework",
            "applied"
        ],
        "AuthorKeywords": ["Predictive control", "reinforcement learning", "safety", "stochastic systems"],
        "RiskTypes": ["unsafe actions", "unsafe states"],
        "Stage": ["deployment", "operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["ETH Zurich, Switzerland"],
        "AuthorAffiliationType": ["university"],
        "Citations": 89
    },
    {
        "Title": "Probabilistic Safeguard for Reinforcement Learning Using Safety Index Guided Gaussian Process Models",
        "BibtexKey": "zhaoProbabilisticSafeguardReinforcement2023",
        "Year": 2023,
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "theoretical",
            "algorithm",
            "Gaussian process",
            "control theory"
        ],
        "AuthorKeywords": ["Safe control", "Gaussian process", "Dynamics learning"],
        "RiskTypes": ["unsafe states"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Carnegie Mellon University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 9
    },
    {
        "Title": "Process Knowledge-Infused AI: Toward User-Level Explainability, Interpretability, and Safety",
        "BibtexKey": "shethProcessKnowledgeInfusedAI2022",
        "Year": 2022,
        "Keywords": [
            "mental health",
            "process knowledge",
            "safety-critical system",
            "machine learning",
            "framework",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation", "deployment"],
        "Method": "design framework",
        "AuthorAffiliation": ["University of South Carolina, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 25
    },
    {
        "Title": "Proposed V-Model for Verification, Validation, and Safety Activities for Artificial Intelligence",
        "BibtexKey": "schumegProposedVModelVerification2023",
        "Year": 2023,
        "Keywords": [
            "verifiability",
            "validity",
            "machine learning",
            "literature review",
            "checklist",
            "theoretical"
        ],
        "AuthorKeywords": ["artificial intelligence", "machine learning", "test", "evaluation", "verification", "validation", "safety", "v-model", "assurance", "autonomy"],
        "RiskTypes": ["safety verification"],
        "Stage": ["design"],
        "Method": "design framework",
        "AuthorAffiliation": ["Aberdeen Test Center Army Test and Evaluation Command, United States", "Quality Engineering & System Assurance Combat Capabilities Development Command Armaments Center, United States"],
        "AuthorAffiliationType": ["government"],
        "Citations": 0
    },
    {
        "Title": "Provably Safe Artificial General Intelligence via Interactive Proofs",
        "BibtexKey": "carlsonProvablySafeArtificial2021",
        "Year": 2021,
        "Keywords": [
            "value alignment",
            "artificial general intelligence (AGI)",
            "philosophical",
            "theoretical"
        ],
        "AuthorKeywords": ["artificial general intelligence", "AGI", "AI safety", "AI value alignment", "AI containment", "interactive proof systems", "multiple-prover systems"],
        "RiskTypes": ["existential"],
        "Stage": [],
        "Method": "philosophical",
        "AuthorAffiliation": ["Harvard University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Reduce the Handicap: Performance Estimation for AI Systems Safety Certification",
        "BibtexKey": "pfrommerReduceHandicapPerformance2023",
        "Year": 2023,
        "Keywords": [
            "safety validation",
            "machine learning",
            "formal methods",
            "empirical validation",
            "adversarial attack",
            "framework",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["safety verification", "adversarial attack"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Fraunhofer Institute of Optronics, Germany"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 0
    },
    {
        "Title": "Responsible Agency Through Answerability",
        "BibtexKey": "hatherallResponsibleAgencyAnswerability2022",
        "Year": 2022,
        "Keywords": [
            "theoretical",
            "framework",
            "AI ethics",
            "AI alignment",
            "accountability"
        ],
        "AuthorKeywords": ["Responsibility gaps", "Agency", "Answerability", "Dialogue agents", "AI ethics", "Sociotechnical Systems Design"],
        "RiskTypes": ["ethical", "alignment"],
        "Stage": ["design"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["University of Edinburgh, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Responsibility as Answerability",
        "BibtexKey": "smithResponsibilityAnswerability2015",
        "Year": 2015,
        "Keywords": [
            "answerability",
            "responsibility gap",
            "AI ethics",
            "accountability",
            "moral responsibility",
            "philosophical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["ethical"],
        "Stage": ["design"],
        "Method": "philosophical",
        "AuthorAffiliation": ["Washington Lee University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 125
    },
    {
        "Title": "Responsible-AI-by-Design: A Pattern Collection for Designing Responsible Artificial Intelligence Systems",
        "BibtexKey": "luResponsibleAIbyDesignPatternCollection2023",
        "Year": 2023,
        "Keywords": [
            "responsibility AI",
            "design patterns",
            "software engineering",
            "literature review",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["responsibility"],
        "Stage": ["design"],
        "Method": "literature review",
        "AuthorAffiliation": ["Data61, Australia"],
        "AuthorAffiliationType": ["corporate"],
        "Citations": 26
    },
    {
        "Title": "Retrain AI Systems Responsibly! Use Sustainable Concept Drift Adaptation Techniques",
        "BibtexKey": "poenaru-olaruRetrainAISystems2023",
        "Year": 2023,
        "Keywords": [
            "distribution shift",
            "concept drift",
            "continuous safety",
            "domain adaptation",
            "theoretical",
            "framework"
        ],
        "AuthorKeywords": ["sustainable model retraining", "concept drift adaptation", "sustainable model maintenance"],
        "RiskTypes": ["domain adaptation"],
        "Stage": ["design"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Delft University of Technology, Netherlands", "Leibniz University Hannover, Germany"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Reward tampering and evolutionary computation: a study of concrete AI-safety problems using evolutionary algorithms",
        "BibtexKey": "nilsenRewardTamperingEvolutionary2023",
        "Year": 2023,
        "Keywords": [
            "reward signal",
            "reward tampering",
            "wireheading",
            "rational agent",
            "human oversight",
            "applied",
            "evaluation"
        ],
        "AuthorKeywords": ["AI trustworthiness", "Reward tampering", "Evolutionary computation", "Neuroevolution", "Behavioural diversity", "Quality diversity"],
        "RiskTypes": ["reward signal corruption"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of Oslo, Norway"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Robust and verifiable privacy federated learning",
        "BibtexKey": "luRobustVerifiablePrivacy2023",
        "Year": 2023,
        "Keywords": [
            "privacy",
            "cybersecurity",
            "robustness",
            "verifiability",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["Federal learning", "Model poisoning", "Robust aggregation", "Verifiable Integrity", "Privacy Protection"],
        "RiskTypes": ["privacy"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Huazhong University of Science and Technology, China"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Robust Data Sampling in Machine Learning: A Game-Theoretic Framework for Training and Validation Data Selection",
        "BibtexKey": "moRobustDataSampling2023",
        "Year": 2023,
        "Keywords": [
            "data sampling",
            "two-player game",
            "algorithm",
            "applied",
            "robustness",
            "Monte Carlo Tree Search"
        ],
        "AuthorKeywords": ["two-player game", "Monte Carlo tree search", "reinforcement learning", "car-following modeling"],
        "RiskTypes": ["data sampling"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Columbia University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1
    },
    {
        "Title": "Robust ML model ensembles via risk-driven anti-clustering of training data",
        "BibtexKey": "mauriRobustMLModel2023",
        "Year": 2023,
        "Keywords": [
            "data poisoning",
            "adversarial attack",
            "robustness",
            "risk estimation",
            "unsupervised learning",
            "classification",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["Adversarial machine learning", "Machine learning security", "Robust ensemble models", "Poisoning attack", "Training set partitioning", "Risk modelling"],
        "RiskTypes": ["robustness", "adversarial attack"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Università degli Studi di Milano, Italy", "Khalifa University of Science and Technology, UAE"],
        "AuthorAffiliationType": ["university"],
        "Citations": 5
    },
    {
        "Title": "Robust Transparency Against Model Inversion Attacks",
        "BibtexKey": "alufaisanRobustTransparencyModel2020",
        "Year": 2020,
        "Keywords": [
            "machine learning",
            "privacy",
            "differential privacy",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["Transparency", "privacy-preserving", "model-inversion attack"],
        "RiskTypes": ["privacy"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Saudi Aramco, Saudi Arabia", "University of Texas at Dallas, United States"],
        "AuthorAffiliationType": ["corporate", "university"],
        "Citations": 14
    },
    {
        "Title": "Safe Policy Search Using Gaussian Process Models",
        "BibtexKey": "polymenakosSafePolicySearch2019",
        "Year": 2019,
        "Keywords": [
            "policy search",
            "safe RL",
            "Gaussian process",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": ["Model-based reinforcement learning", "Safety critical systems", "Gaussian processes"],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["University of Oxford, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 28
    },
    {
        "Title": "Safe semi-supervised classification algorithm combined with active learning sampling strategy",
        "BibtexKey": "zhaoSafeSemisupervisedClassification2018",
        "Year": 2018,
        "Keywords": [
            "semi-supervised learning",
            "active learning",
            "group verification",
            "security verification",
            "iterative workflow",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["Active learning sample", "semi-supervised learning", "safety", "label prediction", "grouping verification"],
        "RiskTypes": ["noisy labels"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Shangluo University, China", "Poznan University, Poland"],
        "AuthorAffiliationType": ["university"],
        "Citations": 11
    },
    {
        "Title": "Safe Trajectory Sampling in Model-Based Reinforcement Learning",
        "BibtexKey": "zwaneSafeTrajectorySampling2023",
        "Year": 2023,
        "Keywords": [
            "model-based RL",
            "safe RL",
            "data efficiency",
            "constrained MDP",
            "Gaussian process",
            "trajectory sampling",
            "applied",
            "algorithm",
            "real-world evaluation"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["unsafe states"],
        "Stage": ["operation"],
        "Method": "real-world testing",
        "AuthorAffiliation": ["University College London, UK", "Chalmers University of Technology, Sweden"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Safe Value Functions",
        "BibtexKey": "massianiSafeValueFunctions2023",
        "Year": 2023,
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "control theory",
            "constrained MDP",
            "value function",
            "theoretical"
        ],
        "AuthorKeywords": ["Optimal control", "reinforcement learning", "safety", "strong duality", "value functions", "viability"],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["operation"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Aachen University, Germany", "Massachusetts Institute of Technology, United States"],
        "AuthorAffiliationType": [],
        "Citations": 6
    },
    {
        "Title": "SafeOps: a concept of continuous safety",
        "BibtexKey": "fayollasSafeOpsConceptContinuous2020",
        "Year": 2020,
        "Keywords": [
            "continuous safety",
            "framework",
            "theoretical",
            "safety monitoring",
            "compliance"
        ],
        "AuthorKeywords": ["Functional Safety", "DevOps", "Autonomous vehicles", "Continuous Integration", "Continuous Deployment", "CI/CD", "Data-intensive Systems", "Cloud-Native Systems", "Artificial Intelligence", "Machine Learning"],
        "RiskTypes": ["holistic", "safety verification"],
        "Stage": ["deployment"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Continental, France"],
        "AuthorAffiliationType": ["corporate"],
        "Citations": 6
    },
    {
        "Title": "SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness",
        "BibtexKey": "kocakSafePredictMetaAlgorithmMachine2021",
        "Year": 2021,
        "Keywords": [
            "meta-learning",
            "machine learning",
            "refusal",
            "applied",
            "algorithm",
            "weight shifting"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["correctness"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Massachusetts Institute of Technology, United States", "New York University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 16
    },
    {
        "Title": "Safety AI: A Novel Approach to Update Safety Models Using Artificial Intelligence",
        "BibtexKey": "gheraibiaSafetyAINovel2019",
        "Year": 2019,
        "Keywords": [
            "evaluation",
            "safety-critical system",
            "fault tree",
            "machine learning",
            "outlier detection",
            "explainable AI",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": ["Fault tree", "reliability", "safety modelling", "model repair", "machine learning", "artificial intelligence"],
        "RiskTypes": ["fault"],
        "Stage": ["operation"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["University of York, UK", "University of Hull, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 25
    },
    {
        "Title": "Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software Deployment",
        "BibtexKey": "zhuSafetyPerformanceWhy2022",
        "Year": 2022,
        "Keywords": [
            "deep learning",
            "compression",
            "adversarial attack",
            "safe model compression",
            "test-driven development",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["AI software safe compression", "test-driven development", "membership inference attack"],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Key Lab of High Confidence Software Technologies, China", "Shanghai University of Finance and Economics, China", "Peking University, China"],
        "AuthorAffiliationType": ["government", "university"],
        "Citations": 2
    },
    {
        "Title": "Safety and Robustness of Deep Neural Networks Object Recognition Under Generic Attacks",
        "BibtexKey": "sallamiSafetyRobustnessDeep2019",
        "Year": 2019,
        "Keywords": [
            "deep learning",
            "safety-critical system",
            "formal methods",
            "robustness",
            "error bound",
            "visual recognition",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["AI Safety", "Perception system", "Object recognition", "Classification", "Deformation", "Attacks", "Neural networks", "Abstract interpretation"],
        "RiskTypes": ["adversarial attack", "safety verification"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["IRT - SystemX, France", "Expleo, France", "French Aerospace Lab, France"],
        "AuthorAffiliationType": ["corporate"],
        "Citations": 15
    },
    {
        "Title": "Safety in the Face of Unknown Unknowns: Algorithm Fusion in Data-driven Engineering Systems",
        "BibtexKey": "kshetrySafetyFaceUnknown2019",
        "Year": 2019,
        "Keywords": [
            "unknown unknowns",
            "framework",
            "theoretical",
            "multi-modal"
        ],
        "AuthorKeywords": ["AI safety", "algorithm fusion", "epistemic uncertainty", "metacognition", "wastewater treatment"],
        "RiskTypes": ["out-of-distribution (OOD)"],
        "Stage": ["operation"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Ensaras Inc., United States", "University of Illinois at Urbana-Champaign, United States"],
        "AuthorAffiliationType": ["corporate", "university"],
        "Citations": 9
    },
    {
        "Title": "Safety Integrity Levels for Artificial Intelligence",
        "BibtexKey": "diemertSafetyIntegrityLevels2023",
        "Year": 2023,
        "Keywords": [
            "safety-critical system",
            "standardization",
            "level of rigour",
            "framework",
            "theoretical"
        ],
        "AuthorKeywords": ["Artificial Intelligence", "Machine Learning", "Safety Integrity Levels", "Safety-Critical Systems"],
        "RiskTypes": ["safety verification"],
        "Stage": ["deployment"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Critical Systems Labs Inc., Canada"],
        "AuthorAffiliationType": ["corporate"],
        "Citations": 0
    },
    {
        "Title": "Safety vs. Efficiency: AI-Based Risk Mitigation in Collaborative Robotics",
        "BibtexKey": "terraSafetyVsEfficiency2020",
        "Year": 2020,
        "Keywords": [
            "safety-critical system",
            "industrial",
            "reinforcement learning",
            "risk mitigation",
            "real-world evaluation",
            "runtime efficiency"
        ],
        "AuthorKeywords": ["human-robot collaboration", "risk mitigation", "safety analysis", "ISO/TS15066:2016", "fuzzy logic system", "reinforcement learning", "automated warehouse", "smart manufacturing"],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["operation"],
        "Method": "real-world testing",
        "AuthorAffiliation": ["Ericsson AB, Sweden", "Ericsson Telecomunicacoes S.A., Brazil"],
        "AuthorAffiliationType": ["corporate"],
        "Citations": 17
    },
    {
        "Title": "SAFEXPLAIN: Safe and Explainable Critical Embedded Systems Based on AI",
        "BibtexKey": "abellaSAFEXPLAINSafeExplainable2023",
        "Year": 2023,
        "Keywords": [
            "deep learning",
            "safety-critical system",
            "limited data",
            "functional safety",
            "traceability",
            "theoretical",
            "framework"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["safety verification", "interpretability"],
        "Stage": ["design"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Barcelona Supercomputing Center, Spain", "Basque Research and Technology Alliance, Spain", "RISE Research Institutes of Sweden, Sweden", "Navinfo Europe, Netherlands", "AIKO s.r.l., Italy", "Exida Development s.r.l., Italy"],
        "AuthorAffiliationType": ["corporate", "research institution", "government"],
        "Citations": 0
    },
    {
        "Title": "SAMBA: safe model-based & active reinforcement learning",
        "BibtexKey": "cowen-riversSAMBASafeModelbased2022",
        "Year": 2022,
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "probabilistic modelling",
            "multi-objective MDP",
            "Gaussian process",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["Gaussian process", "Safe reinforcement learning", "Active learning"],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Huawei, UK", "Technical University Darmstadt, Germany", "University College London, UK"],
        "AuthorAffiliationType": ["big-tech corporate", "university"],
        "Citations": 32
    },
    {
        "Title": "Sampling-based Inverse Reinforcement Learning Algorithms with Safety Constraints",
        "BibtexKey": "fischerSamplingbasedInverseReinforcement2021",
        "Year": 2021,
        "Keywords": [
            "optimisation",
            "planning",
            "inverse reinforcement learning",
            "safety constraints",
            "constrained MDP",
            "safe RL",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["Reinforcement Learning", "Inverse Reinforcement Learning", "Maximum Entropy", "Constraints", "Safety", "SUMO"],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Karlsruhe Institute of Technology, Germany", "BMW Group, Germany"],
        "AuthorAffiliationType": ["university", "corporate"],
        "Citations": 6
    },
    {
        "Title": "Security analysis of safe and seldonian reinforcement learning algorithms",
        "BibtexKey": "pinarozisikSecurityAnalysisSafe2020",
        "Year": 2020,
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "seldonian RL",
            "data corruption",
            "adversarial attack",
            "robustness",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["data quality", "adversarial attack"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Massachusetts, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 5
    },
    {
        "Title": "Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms",
        "BibtexKey": "dagdanovSelfImprovingSafetyPerformance2023",
        "Year": 2023,
        "Keywords": [
            "continual learning",
            "reinforcement learning",
            "autonomous driving",
            "verifiability",
            "safety-critical system",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["Deep Reinforcement Learning", "Autonomous Driving", "Black-Box Verification"],
        "RiskTypes": ["domain generalization"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Istanbul Technical University, Turkey"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1
    },
    {
        "Title": "Self-Preserving Genetic Algorithms for Safe Learning in Discrete Action Spaces",
        "BibtexKey": "robinetteSelfPreservingGeneticAlgorithms2023",
        "Year": 2023,
        "Keywords": [
            "genetic algorithm",
            "self-preservation",
            "safe RL",
            "reinforcement learning",
            "safety-critical system",
            "action masking",
            "safety constraints",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["genetic algorithms", "safe learning", "safe reinforcement learning", "run time assurance", "action masking"],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Vanderbilt University, United States", "Parallax Advanced Research, United States"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 0
    },
    {
        "Title": "Shared Interest: Measuring Human-AI Alignment to Identify Recurring Patterns in Model Behavior",
        "BibtexKey": "boggustSharedInterestMeasuring2022",
        "Year": 2022,
        "Keywords": [
            "saliency maps",
            "neuron activation",
            "interpretability",
            "metrics",
            "human oversight",
            "probing",
            "applied",
            "framework"
        ],
        "AuthorKeywords": ["human-computer interaction", "interpretability", "machine learning", "saliency methods"],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Massachusetts Institute of Technology, United States", "IBM Research, United States"],
        "AuthorAffiliationType": ["university", "big-tech corporate"],
        "Citations": 21
    },
    {
        "Title": "Stability-Certified Reinforcement Learning: A Control-Theoretic Perspective",
        "BibtexKey": "jinStabilityCertifiedReinforcementLearning2020",
        "Year": 2020,
        "Keywords": [
            "stability",
            "reinforcement learning",
            "safe RL",
            "robustness",
            "error bound",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["Reinforcement learning", "robust control", "decentralized control synthesis", "safe reinforcement learning"],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Virginia Tech, United States", "University of California at Berkley, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 94
    },
    {
        "Title": "TENET: a new hybrid network architecture for adversarial defense",
        "BibtexKey": "tunaTENETNewHybrid2023",
        "Year": 2023,
        "Keywords": [
            "deep learning",
            "adversarial attack",
            "metrics",
            "monte carlo dropout",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["Adversarial machine learning", "Model uncertainty", "Robustness", "Monte Carlo dropout sampling"],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Ericsson Research, Turkey", "University of Stavanger, Norway", "Isik University, Turkey"],
        "AuthorAffiliationType": [],
        "Citations": 1
    },
    {
        "Title": "The responsibility gap: Ascribing responsibility for the actions of learning automata",
        "BibtexKey": "matthiasResponsibilityGapAscribing2004",
        "Year": 2004,
        "Keywords": [
            "AI ethics",
            "philosophical",
            "theoretical",
            "law"
        ],
        "AuthorKeywords": ["artificial intelligence", "autonomous robots", "learning machines", "liability", "moral responsibility"],
        "RiskTypes": ["ethical"],
        "Stage": ["operation"],
        "Method": "philosophical",
        "AuthorAffiliation": ["University of Kassel, Germany"],
        "AuthorAffiliationType": ["university"],
        "Citations": 840
    },
    {
        "Title": "Toward safe AI",
        "BibtexKey": "morales-foreroSafeAI2023",
        "Year": 2023,
        "Keywords": [
            "safety analysis",
            "failsafe",
            "systems engineering",
            "validity",
            "theoretical",
            "framework"
        ],
        "AuthorKeywords": ["Safe AI", "Explainable AI", "Interpretable AI", "Trustworthy AI", "Box-Jenkins framework", "Ethical AI", "Responsible AI", "Relational validity"],
        "RiskTypes": ["responsibility"],
        "Stage": ["design"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Polytechnique Montréal, Canada", "Tampere University, Finland"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3
    },
    {
        "Title": "Toward Trustworthy AI: Blockchain-Based Architecture Design for Accountability and Fairness of Federated Learning Systems",
        "BibtexKey": "loTrustworthyAIBlockchainBased2023",
        "Year": 2023,
        "Keywords": [
            "federated learning",
            "privacy",
            "blockchain",
            "data sampling",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["Accountability", "AI", "blockchain", "fairness", "federated learning", "machine learning", "responsible AI", "smart contract"],
        "RiskTypes": ["privacy"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of New South Wales, Australia", "Data61, Australia"],
        "AuthorAffiliationType": [],
        "Citations": 59
    },
    {
        "Title": "Towards a robust and trustworthy machine learning system development: An engineering perspective",
        "BibtexKey": "xiongRobustTrustworthyMachine2022",
        "Year": 2022,
        "Keywords": [
            "safety-critical system",
            "privacy",
            "literature review",
            "security engineering",
            "theoretical"
        ],
        "AuthorKeywords": ["Robustness of machine learning", "Adversarial sampling and countermeasures", "Privacy-preserving machine learning", "User trust", "Secure machine learning system development"],
        "RiskTypes": ["privacy"],
        "Stage": ["design"],
        "Method": "literature review",
        "AuthorAffiliation": ["National Research Council Canada, Canada"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 14
    },
    {
        "Title": "Towards Deep Anomaly Detection with Structured Knowledge Representations",
        "BibtexKey": "kirchheimDeepAnomalyDetection2023",
        "Year": 2023,
        "Keywords": [
            "machine learning",
            "anomaly detection",
            "structured knowledge",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": ["Anomaly Detection", "Out-of-Distribution Detection", "Deep Learning", "Machine Learning Safety", "Hybrid Models"],
        "RiskTypes": ["anomaly detection"],
        "Stage": ["design"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Otto-von-Guericke University Magdeburg, Germany"],
        "AuthorAffiliationType": ["university"],
        "Citations": 0
    },
    {
        "Title": "Towards deployment of robust cooperative AI agents: An algorithmic framework for learning adaptive policies",
        "BibtexKey": "ghoshDeploymentRobustCooperative2020",
        "Year": 2020,
        "Keywords": [
            "cooperation",
            "multi-agent RL (MARL)",
            "parametric MDP",
            "algorithm",
            "adaptive policy",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": ["Learning agent-to-agent interactions", "Machine learning", "Reinforcement learning"],
        "RiskTypes": ["cooperation"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Max Planck Institute for Software Systemsm, Germany", "University of Vienna, Austria"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 14
    },
    {
        "Title": "Towards Fair and Robust Classification",
        "BibtexKey": "sunFairRobustClassification2022",
        "Year": 2022,
        "Keywords": [
            "robustness",
            "fairness",
            "classification",
            "machine learning",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["Algorithmic fairness", "adversarial robustness", "classification", "trustworthy machine learning"],
        "RiskTypes": ["fairness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Facebook Inc., United States", "Stevens Institute of Technology, United States", "Penn State University, United States"],
        "AuthorAffiliationType": ["big-tech corporate", "research institution", "university"],
        "Citations": 11
    },
    {
        "Title": "Towards functional safety compliance of matrix-matrix multiplication for machine learning-based autonomous systems",
        "BibtexKey": "fernandezFunctionalSafetyCompliance2021",
        "Year": 2021,
        "Keywords": [
            "autonomous system",
            "machine learning",
            "runtime efficiency",
            "functional safety",
            "hardware",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["Machine learning", "Functional safety", "Error detection"],
        "RiskTypes": ["functional safety"],
        "Stage": ["deployment"],
        "Method": "real-world testing",
        "AuthorAffiliation": ["Basque Research and Technology Alliance, Spain", "Barcelona Supercomputing Center, Spain"],
        "AuthorAffiliationType": ["research institution", "government"],
        "Citations": 10
    },
    {
        "Title": "Understanding adversarial training: Increasing local stability of supervised models through robust optimization",
        "BibtexKey": "shahamUnderstandingAdversarialTraining2018",
        "Year": 2018,
        "Keywords": [
            "adversarial training",
            "supervised learning",
            "robust optimisation",
            "deep learning",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["Adversarial examples", "Robust optimization", "Non-parametric supervised models", "Deep learning"],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Yale University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 289
    },
    {
        "Title": "User Tampering in Reinforcement Learning Recommender Systems",
        "BibtexKey": "kasirzadehUserTamperingReinforcement2023",
        "Year": 2023,
        "Keywords": [
            "formal methods",
            "reinforcement learning",
            "recommender system",
            "user tampering",
            "causal modelling",
            "reward tampering",
            "theoretical",
            "evaluation"
        ],
        "AuthorKeywords": ["AI Safety", "AI Ethics", "Recommendation Systems", "Recommender Systems", "Reinforcement Learning", "Value Alignment"],
        "RiskTypes": ["reward signal corruption"],
        "Stage": ["operation"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["University of Edinburgh, UK", "The Alan Turing Institute, UK", "Australian National University, Australia"],
        "AuthorAffiliationType": ["university"],
        "Citations": 28
    },
    {
        "Title": "Using self-supervised learning can improve model robustness and uncertainty",
        "BibtexKey": "hendrycksUsingSelfsupervisedLearning2019",
        "Year": 2019,
        "Keywords": [
            "self-supervised learning",
            "deep learning",
            "robustness",
            "label corruption",
            "outlier detection",
            "uncertainty estimates",
            "applied",
            "evaluation"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of California at Berkley, United States", "University of Illinois Urbana-Champaign, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 891
    },
    {
        "Title": "Verifiably Safe Exploration for End-to-End Reinforcement Learning",
        "BibtexKey": "huntVerifiablySafeExploration2021",
        "Year": 2021,
        "Keywords": [
            "reinforcement learning",
            "safety-critical system",
            "instantaneous hard constraints",
            "safety constraints",
            "safe exploration",
            "safe RL",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["formal verification", "reinforcement learning", "neural networks", "hybrid systems", "safe artificial intelligence", "differential dynamic logic"],
        "RiskTypes": ["unsafe exploration", "safety verification"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Massachusetts Institute of Technology, United States", "IBM Research, United States"],
        "AuthorAffiliationType": ["university", "big-tech corporate"],
        "Citations": 42
    },
    {
        "Title": "Verifiably Safe Off-Model Reinforcement Learning",
        "BibtexKey": "fultonVerifiablySafeOffModel2019",
        "Year": 2019,
        "Keywords": [
            "reinforcement learning",
            "safety-critical system",
            "formal methods",
            "constrained policy optimisation",
            "safety constraints",
            "runtime efficiency",
            "verifiability",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["unsafe actions", "safety verification"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Carnegie Mellon University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 57
    },
    {
        "Title": "When Neurons Fail",
        "BibtexKey": "elmhamdiWhenNeuronsFail2017",
        "Year": 2017,
        "Keywords": [
            "deep learning",
            "robustness",
            "neuron activation",
            "error bound",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["model failure"],
        "Stage": ["operation"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Swiss Federal Institute of Technology Lausanne, Switzerland"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 41
    },
    {
        "Title": "A data-driven robust optimization algorithm for black-box cases: An application to hyper-parameter optimization of machine learning algorithms",
        "BibtexKey": "seifiDatadrivenRobustOptimization2021",
        "Year": 2021,
        "Keywords": [
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "bayesian optimization",
            "black-box optimization",
            "data-driven optimization",
            "deep learning",
            "gaussian process",
            "hyper-parameter tuning",
            "robust optimization"
        ],
        "RiskTypes": ["robustness", "hyper-parameter tuning"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 4
    },
    {
        "Title": "A Robust Framework for Fixing The Vulnerability of Compressed Distributed Learning",
        "BibtexKey": "chenRobustFrameworkFixing2023",
        "Year": 2023,
        "Keywords": [
            "communication bottleneck",
            "framework",
            "theoretical"
        ],
        "AuthorKeywords": [
            "compression",
            "distributed learning",
            "machine learning",
            "poisoning attacks",
            "robustness"
        ],
        "RiskTypes": ["model poisoning", "robustness", "communication bottleneck"],
        "Stage": ["deployment", "operation"],
        "Method": "design framework",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "A Robust Self-Organizing Approach to Effectively Clustering Incomplete Data",
        "BibtexKey": "chauRobustSelfOrganizingApproach2016",
        "Year": 2016,
        "Keywords": [
            "clustering",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "incomplete data clustering",
            "nearest prototype",
            "robustness",
            "self-organizing map",
            "unsupervised learning"
        ],
        "RiskTypes": ["missing data"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 4
    },
    {
        "Title": "A Semi-supervised Deep Learning Model with Consistency Regularization of Augmented Samples for Imbalanced Fault Detection",
        "BibtexKey": "chenSemisupervisedDeepLearning2022",
        "Year": 2022,
        "Keywords": [
            "fault detection",
            "regularization",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "artificial intelligence",
            "reliability",
            "safety",
            "class-imbalance",
            "consistency regularization",
            "data augmentation",
            "semi-supervised learning"
        ],
        "RiskTypes": ["fault", "robustness", "noisy labels"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 1
    },
    {
        "Title": "Adversarial feature distribution alignment for semi-supervised learning",
        "BibtexKey": "mayerAdversarialFeatureDistribution2021",
        "Year": 2021,
        "Keywords": [
            "distribution shift",
            "generalization",
            "adversarial training",
            "semi-supervised learning",
            "domain adaptation",
            "regularization",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["generalization"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 7
    },
    {
        "Title": "Adversarial Robustness Via Fisher-Rao Regularization",
        "BibtexKey": "picotAdversarialRobustnessFisherRao2023",
        "Year": 2023,
        "Keywords": [
            "robustness",
            "manifolds",
            "training",
            "perturbation methods",
            "standards",
            "neural networks",
            "adversarial machine learning",
            "regularization",
            "cross entropy",
            "supervised learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 6
    },
    {
        "Title": "Adversarial Training on Joint Energy Based Model for Robust Classification and Out-of-Distribution Detection",
        "BibtexKey": "leeAdversarialTrainingJoint2020",
        "Year": 2020,
        "Keywords": [
            "adversarial training",
            "energy-based model",
            "classification",
            "machine learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "deep learning",
            "machine vision",
            "machine perception",
            "security",
            "safety",
            "deep learning",
            "adversarial attack",
            "Out-of-Distribution detection"
        ],
        "RiskTypes": ["out-of-distribution (OOD)"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 2
    },
    {
        "Title": "Adversarial Training With Channel Attention Regularization",
        "BibtexKey": "choAdversarialTrainingChannel2022",
        "Year": 2022,
        "Keywords": [
            "computer vision",
            "semi-supervised learning",
            "regularization",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "adversarial machine learning",
            "adversarial training",
            "feature regularization",
            "robustness"
        ],
        "RiskTypes": ["robustness", "adversarial attack"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "AlwaysSafe: Reinforcement learning without safety constraint violations during training",
        "BibtexKey": "simaoAlwaysSafeReinforcementLearning2021",
        "Year": 2021,
        "Keywords": [
            "factored model",
            "reinforcement learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "constrained Markov decision processes",
            "reinforcement learning",
            "safe reinforcement learning"
        ],
        "RiskTypes": ["unsafe states"],
        "Stage": ["deployment", "operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 22
    },
    {
        "Title": "An Actor-Critic Framework for Online Control With Environment Stability Guarantee",
        "BibtexKey": "osinenkoActorCriticFrameworkOnline2023",
        "Year": 2023,
        "Keywords": [
            "optimal control",
            "reinforcement learning",
            "safety-critical system",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "control",
            "stabilization",
            "reinforcement learning",
            "Lyapunov function"
        ],
        "RiskTypes": ["unsafe actions", "unsafe states"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [""],
        "AuthorAffiliationType": [""],
        "Citations": 0
    },
    {
        "Title": "An Adversarial Training Method for Improving Model Robustness in Unsupervised Domain Adaptation",
        "BibtexKey": "nieAdversarialTrainingMethod2021",
        "Year": 2021,
        "Keywords": [
            "robustness",
            "machine learning",
            "classification",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "adversarial attack",
            "adversarial training",
            "robustness",
            "transfer learning",
            "unsupervised domain adaption"
        ],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 2
    },
    {
        "Title": "Anomaly Detection for Insider Threats Using Unsupervised Ensembles",
        "BibtexKey": "leAnomalyDetectionInsider2021",
        "Year": 2021,
        "Keywords": [
            "robustness",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "anomaly detection",
            "dependable learning",
            "robust learning",
            "ensemble learning",
            "insider threat detection",
            "temporal data",
            "unsupervised learning"
        ],
        "RiskTypes": ["anomaly detection"],
        "Stage": ["deployment", "operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 43
    },
    {
        "Title": "Bayesian robust multi-extreme learning machine",
        "BibtexKey": "liBayesianRobustMultiextreme2020",
        "Year": 2020,
        "Keywords": [
            "robustness",
            "extreme learning machine",
            "Dirichlet process",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [
            "infinite mixture of gaussians",
            "infinite mixture of student's t-distributions",
            "multi-extreme learning machine",
            "outlier-robust regression",
            "sparse priors",
            "variational bayesian"
        ],
        "RiskTypes": ["outliers"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 7
    },
    {
        "Title": "Bidirectional Adaptation for Robust Semi-Supervised Learning with Inconsistent Data Distributions",
        "BibtexKey": "jiaBidirectionalAdaptationRobust2023",
        "Year": 2023,
        "Keywords": [
            "semi-supervised learning",
            "robustness",
            "theoretical",
            "framework",
            "distribution shift"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["out-of-distribution (OOD)"],
        "Stage": ["deployment"],
        "Method": "theoretical framework",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Bullseye polytope: A scalable clean-label poisoning attack with improved transferability",
        "BibtexKey": "aghakhaniBullseyePolytopeScalable2021",
        "Year": 2021,
        "Keywords": [
            "clean-label poisoning",
            "classification",
            "machine learning",
            "robustness",
            "transfer learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "dataset poisoning",
            "machine learning robustness"
        ],
        "RiskTypes": ["label poisoning"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 32
    },
    {
        "Title": "Byzantine-Robust Online and Offline Distributed Reinforcement Learning",
        "BibtexKey": "chenByzantineRobustOnlineOffline2023",
        "Year": 2023,
        "Keywords": [
            "reinforcement learning",
            "communication",
            "adversarial agent",
            "fake data",
            "Byzantine-robust",
            "robustness",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 4
    },
    {
        "Title": "Conformal Predictive Safety Filter for RL Controllers in Dynamic Environments",
        "BibtexKey": "strawnConformalPredictiveSafety2023",
        "Year": 2023,
        "Keywords": [
            "optimal control",
            "safe RL",
            "safety constraints",
            "uncertainty estimates",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "conformal prediction",
            "predictive safety filter",
            "reinforcement learning",
            "safe motion planning"
        ],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "CROP: Towards Distributional-Shift Robust Reinforcement Learning Using Compact Reshaped Observation Processing",
        "BibtexKey": "altmannCROPDistributionalShiftRobust2023",
        "Year": 2023,
        "Keywords": [
            "reinforcement learning",
            "generalization",
            "data augmentation",
            "sample efficiency",
            "simulation",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["generalization"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Data-Driven Robust Multi-Agent Reinforcement Learning",
        "BibtexKey": "wangDataDrivenRobustMultiAgent2022",
        "Year": 2022,
        "Keywords": [
            "multi-agent RL (MARL)",
            "reinforcement learning",
            "model-free RL",
            "robustness",
            "algorithm",
            "theoretical"
        ],
        "AuthorKeywords": [
            "distributionally robust",
            "finite-time analysis",
            "model-free",
            "robust mdp",
            "sample complexity"
        ],
        "RiskTypes": ["uncertainty"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 1
    },
    {
        "Title": "Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness",
        "BibtexKey": "zhangDecoupledAdversarialContrastive2022",
        "Year": 2022,
        "Keywords": [
            "decoupled learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "adversarial contrastive learning",
            "adversarial robustness",
            "adversarial training",
            "self-supervised learning"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 5
    },
    {
        "Title": "DiscrimLoss: A Universal Loss for Hard Samples and Incorrect Samples Discrimination",
        "BibtexKey": "wuDiscrimLossUniversalLoss2023",
        "Year": 2023,
        "Keywords": [
            "robustness",
            "machine learning",
            "classification",
            "self-supervised learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "deep learning",
            "label noise",
            "machine learning",
            "noisy label",
            "robust methods"
        ],
        "RiskTypes": ["noisy labels"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Distributed Multi-Agent Deep Reinforcement Learning for Robust Coordination against Noise",
        "BibtexKey": "motokawaDistributedMultiAgentDeep2022",
        "Year": 2022,
        "Keywords": [
            "multi-agent RL (MARL)",
            "robustness",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "alter-exploration problem",
            "attention mechanism",
            "cooperation",
            "coordination",
            "distributed system",
            "multi-agent deep reinforcement learning",
            "noise reduction"
        ],
        "RiskTypes": ["noise"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 3
    },
    {
        "Title": "Distributionally robust semi-supervised learning for people-centric sensing",
        "BibtexKey": "chenDistributionallyRobustSemisupervised2019",
        "Year": 2019,
        "Keywords": [
            "semi-supervised learning",
            "distribution shift",
            "robustness",
            "visual recognition",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["domain generalization"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 28
    },
    {
        "Title": "Does distributionally robust supervised learning give robust classifiers?",
        "BibtexKey": "huDoesDistributionallyRobust2018",
        "Year": 2018,
        "Keywords": [
            "distribution shift",
            "robustness",
            "supervised learning",
            "classification",
            "algorithm",
            "theoretical"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["domain generalization"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 58
    },
    {
        "Title": "Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning",
        "BibtexKey": "liangEfficientAdversarialTraining2022",
        "Year": 2022,
        "Keywords": [
            "deep learning",
            "reinforcement learning",
            "adversarial attack",
            "robustness",
            "reward signal",
            "risk estimation",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["deployment", "operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 3
    },
    {
        "Title": "Embracing Risk in Reinforcement Learning: The Connection between Risk-Sensitive Exponential and Distributionally Robust Criteria",
        "BibtexKey": "nooraniEmbracingRiskReinforcement2022",
        "Year": 2022,
        "Keywords": [
            "risk mitigation",
            "reinforcement learning",
            "robustness",
            "theoretical",
            "framework"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["design"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 2
    },
    {
        "Title": "FairMixRep: Self-supervised Robust Representation Learning for Heterogeneous Data with Fairness constraints",
        "BibtexKey": "chakrabortyFairMixRepSelfsupervisedRobust2020",
        "Year": 2020,
        "Keywords": [
            "bias",
            "heterogeneous data",
            "constraints",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "fairness",
            "mixed data types",
            "robustness",
            "self-supervised learning",
            "representation learning",
            "unbiased learning"
        ],
        "RiskTypes": ["fairness", "bias", "robustness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 1
    },
    {
        "Title": "Formal Methods Assisted Training of Safe Reinforcement Learning Agents",
        "BibtexKey": "murugesanFormalMethodsAssisted2019",
        "Year": 2019,
        "Keywords": [
            "verifiability",
            "safety-critical system",
            "SMT solver",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [
            "reinforcement learning",
            "safety assurance",
            "formal methods"
        ],
        "RiskTypes": ["safety verification"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 6
    },
    {
        "Title": "Group Distributionally Robust Reinforcement Learning with Hierarchical Latent Variables",
        "BibtexKey": "xuGroupDistributionallyRobust2023",
        "Year": 2023,
        "Keywords": [
            "multi-objective MDP",
            "reinforcement learning",
            "robustness",
            "uncertainty estimates",
            "hierarchical reinforcement learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 3
    },
    {
        "Title": "Improving fairness generalization through a sample-robust optimization method",
        "BibtexKey": "ferryImprovingFairnessGeneralization2023",
        "Year": 2023,
        "Keywords": [
            "bias",
            "machine learning",
            "safety-critical system",
            "domain generalization",
            "robustness",
            "classification",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "distributionally robust optimization",
            "fairness",
            "generalization",
            "supervised learning"
        ],
        "RiskTypes": ["robustness", "fairness", "bias", "domain generalization"],
        "Stage": ["deployment", "operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Improving Machine Learning Robustness via Adversarial Training",
        "BibtexKey": "dangImprovingMachineLearning2023",
        "Year": 2023,
        "Keywords": [
            "machine learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "adversarial training",
            "federated learning",
            "independent and identically distributed (iid)",
            "non-iid data",
            "machine learning robustness"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Improving Robustness of Deep Reinforcement Learning Agents: Environment Attack based on the Critic Network",
        "BibtexKey": "schottImprovingRobustnessDeep2022",
        "Year": 2022,
        "Keywords": [
            "adversarial attack",
            "reinforcement learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "adversarial training",
            "deep reinforcement learning",
            "robustness"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Improving Robustness via Risk Averse Distributional Reinforcement Learning",
        "BibtexKey": "singhImprovingRobustnessRisk2020",
        "Year": 2020,
        "Keywords": [
            "domain adaptation",
            "robustness",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "distributional reinforcement learning",
            "reinforcement learning",
            "risk-sensitive control",
            "robust reinforcement learning"
        ],
        "RiskTypes": ["robustness", "domain adaptation"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 11
    },
    {
        "Title": "Integrating safety constraints into adversarial training for robust deep reinforcement learning",
        "BibtexKey": "mengIntegratingSafetyConstraints2023",
        "Year": 2023,
        "Keywords": [
            "adversarial attack",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "reinforcement learning",
            "adversarial training",
            "penalty function method",
            "robust deep reinforcement learning",
            "constrained markov decision process"
        ],
        "RiskTypes": ["robustness", "adversarial attack"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 8
    },
    {
        "Title": "JSMix: a holistic algorithm for learning with label noise",
        "BibtexKey": "wenJSMixHolisticAlgorithm2023",
        "Year": 2023,
        "Keywords": [
            "adaptive loss function",
            "overfitting",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "data with label noise",
            "Jensen-Shannon divergence",
            "robust loss function",
            "semi-supervised learning"
        ],
        "RiskTypes": ["noisy labels"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 1
    },
    {
        "Title": "Learning a Domain-Invariant Embedding for Unsupervised Domain Adaptation Using Class-Conditioned Distribution Alignment",
        "BibtexKey": "gabourieLearningDomainInvariantEmbedding2019",
        "Year": 2019,
        "Keywords": [
            "domain adaptation",
            "unsupervised learning",
            "generalization",
            "classification",
            "embedding",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["domain adaptation"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 6
    },
    {
        "Title": "Learning a Low-Dimensional Representation of a Safe Region for Safe Reinforcement Learning on Dynamical Systems",
        "BibtexKey": "zhouLearningLowDimensionalRepresentation2023",
        "Year": 2023,
        "Keywords": [
            "reinforcement learning",
            "deep learning",
            "low-dimensional representation",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [
            "data-driven model order reduction",
            "deep learning in robotics and automation",
            "learning and adaptive systems",
            "safe reinforcement learning (SRL)"
        ],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 2
    },
    {
        "Title": "Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations",
        "BibtexKey": "luoLearningBarrierCertificates2021",
        "Year": 2021,
        "Keywords": [
            "training-time safety violations",
            "reinforcement learning",
            "safe RL algorithms",
            "co-trained barrier certificate for safe rl (crabs)",
            "adversarial training",
            "certified regions",
            "exploration",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 39
    },
    {
        "Title": "Model-Free Safe Reinforcement Learning Through Neural Barrier Certificate",
        "BibtexKey": "yangModelFreeSafeReinforcement2023",
        "Year": 2023,
        "Keywords": [
            "safe RL",
            "model-free RL",
            "neural barrier certificate",
            "stepwise state constraint setting",
            "multi-step invariant loss",
            "lagrangian method",
            "feasible regions",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [
            "robot safety",
            "reinforcement learning",
            "neural barrier certificate"
        ],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 7
    },
    {
        "Title": "Multi-Modal Mutual Information (MuMMI) Training for Robust Self-Supervised Deep Reinforcement Learning",
        "BibtexKey": "chenMultiModalMutualInformation2021",
        "Year": 2021,
        "Keywords": [
            "world model",
            "robustness",
            "multiple sensors",
            "shared representation between modalities",
            "multi-modal deep latent state-space model",
            "mutual information lower-bound",
            "density ratio estimator",
            "self-supervised learning",
            "reinforcement learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 12
    },
    {
        "Title": "Multiexpert Adversarial Regularization for Robust and Data-Efficient Deep Supervised Learning",
        "BibtexKey": "gholamiMultiexpertAdversarialRegularization2022",
        "Year": 2022,
        "Keywords": [
            "supervised learning",
            "classification",
            "adversarial training",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "image classification",
            "image segmentation",
            "data-efficient learning",
            "robust learning",
            "ensemble methods",
            "adversarial learning"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Not All Parameters Should Be Treated Equally: Deep Safe Semi-supervised Learning under Class Distribution Mismatch",
        "BibtexKey": "heNotAllParameters2022",
        "Year": 2022,
        "Keywords": [
            "semi-supervised learning",
            "deep learning",
            "unlabeled set",
            "generalization",
            "data level",
            "model parameter level",
            "safe parameters",
            "harmful parameters",
            "safe parameter learning (SPL)",
            "bi-level optimization strategy",
            "class distribution mismatch",
            "domain adaptation",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["label poisoning"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 22
    },
    {
        "Title": "RDA: Reciprocal Distribution Alignment for Robust Semi-supervised Learning",
        "BibtexKey": "duanRDAReciprocalDistribution2022",
        "Year": 2022,
        "Keywords": [
            "robustness",
            "semi-supervised learning",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [
            "distribution alignment",
            "mismatched distributions"
        ],
        "RiskTypes": ["robustness", "domain adaptation"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 8
    },
    {
        "Title": "Risk-averse Distributional Reinforcement Learning: A CVaR Optimization Approach",
        "BibtexKey": "stankoRiskaverseDistributionalReinforcement2019",
        "Year": 2019,
        "Keywords": [
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "reinforcement learning",
            "distributional reinforcement learning",
            "risk",
            "ai safety",
            "conditional value-at-risk",
            "cvar",
            "value iteration",
            "q-learning",
            "deep learning",
            "deep q-learning"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 26
    },
    {
        "Title": "Robust Federated Learning With Noisy Labeled Data Through Loss Function Correction",
        "BibtexKey": "chenRobustFederatedLearning2023",
        "Year": 2023,
        "Keywords": [
            "robustness",
            "label noise",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "distributed networks",
            "federated learning",
            "label noise",
            "machine learning",
            "non-convex optimization",
            "parallel and distributed algorithms",
            "robust design"
        ],
        "RiskTypes": ["noisy labels"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 3
    },
    {
        "Title": "Robust fusion of unreliable data sources using error-correcting output codes",
        "BibtexKey": "vempatyRobustFusionUnreliable2019",
        "Year": 2019,
        "Keywords": [
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "sensor fusion",
            "error correction codes",
            "learning (artificial intelligence)",
            "fault-tolerant computing",
            "inference mechanisms",
            "pattern classification"
        ],
        "RiskTypes": ["fault"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Robust multi-agent reinforcement learning via Bayesian distributional value estimation",
        "BibtexKey": "duRobustMultiagentReinforcement2024",
        "Year": 2023,
        "Keywords": [
            "reinforcement learning",
            "multi-agent scenarios",
            "bayesian multi-agent reinforcement learning (bmarl)",
            "distributional value function",
            "robustness",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "multi-agent reinforcement learning",
            "bayesian inference",
            "distributional value function",
            "deep reinforcement learning"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Robust Reinforcement Learning using Offline Data",
        "BibtexKey": "panagantiRobustReinforcementLearning2022",
        "Year": 2022,
        "Keywords": 
        [
            "robust reinforcement learning (rl)",
            "policy robustness",
            "model parameter uncertainty",
            "real-world rl applications",
            "adversarial training",
            "max-min problem",
            "uncertainty set",
            "robust fitted q-iteration (rfqi)",
            "offline dataset",
            "robust bellman operator",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 12
    },
    {
        "Title": "Robust reinforcement learning via adversarial training with Langevin dynamics",
        "BibtexKey": "kamalarubanRobustReinforcementLearning2020",
        "Year": 2020,
        "Keywords": [
            "sampling perspective",
            "robust reinforcement learning",
            "stochastic gradient langevin dynamics",
            "two-player rl algorithm",
            "policy gradient method",
            "generalization",
            "training and testing conditions",
            "mujoco environments",
            "environmental shifts",
            "sampling approach",
            "safe RL",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["robustness", "generalization"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 17
    },
    {
        "Title": "Robust Reinforcement Learning: A Constrained Game-theoretic Approach",
        "BibtexKey": "yuRobustReinforcementLearning2021",
        "Year": 2021,
        "Keywords": [
            "reinforcement learning",
            "robustness",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [
            "adversarial training",
            "competitive optimization",
            "policy gradient",
            "robust reinforcement learning",
            "zero-sum game"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 5
    },
    {
        "Title": "Robust semi-supervised representation learning for graph-structured data",
        "BibtexKey": "guoRobustSemisupervisedRepresentation2019",
        "Year": 2019,
        "Keywords": [
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "graph convolutional network",
            "representation learning",
            "robustness",
            "semi-supervised learning"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 4
    },
    {
        "Title": "Robust supervised classification with mixture models: Learning from data with uncertain labels",
        "BibtexKey": "bouveyronRobustSupervisedClassification2009",
        "Year": 2009,
        "Keywords": [
            "classification",
            "human oversight",
            "robustness",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "data with uncertain labels",
            "label noise",
            "mixture models",
            "robustness",
            "supervised classification",
            "weakly supervised classification"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 152
    },
    {
        "Title": "Robust Support Vector Machine using Least Median Loss Penalty",
        "BibtexKey": "maRobustSupportVector2011",
        "Year": 2011,
        "Keywords": [
            "robustness",
            "loss function",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "learning theory",
            "pattern recognition",
            "robust identification",
            "statistical data analysis",
            "support vector machine"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["design"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 29
    },
    {
        "Title": "Robust Unsupervised Domain Adaptation from A Corrupted Source",
        "BibtexKey": "yuRobustUnsupervisedDomain2022",
        "Year": 2022,
        "Keywords": [
            "domain adaptation",
            "robustness",
            "framework",
            "applied"
        ],
        "AuthorKeywords": [
            "poison data attack",
            "robust learning",
            "unsupervised domain adaptation"
        ],
        "RiskTypes": ["domain adaptation"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Robust weighted Gaussian processes",
        "BibtexKey": "ramirez-padronRobustWeightedGaussian2021",
        "Year": 2021,
        "Keywords": [
            "Gaussian process",
            "robustness",
            "outliers",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "machine learning",
            "online learning",
            "outlying data",
            "robust regression"
        ],
        "RiskTypes": ["outliers", "robustness"],
        "Stage": ["design"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 2
    },
    {
        "Title": "Robustifying Reinforcement Learning Agents via Action Space Adversarial Training",
        "BibtexKey": "tanRobustifyingReinforcementLearning2020",
        "Year": 2020,
        "Keywords": [
            "machine learning",
            "cyber-physical systems",
            "deep reinforcement learning",
            "malicious attacks",
            "resilient controllers",
            "robust control",
            "adversarial attack",
            "decision and control tasks",
            "adversarial training",
            "actuator attacks",
            "safe RL",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 53
    },
    {
        "Title": "Safe Artificial General Intelligence via Distributed Ledger Technology",
        "BibtexKey": "carlsonSafeArtificialGeneral2019",
        "Year": 2019,
        "Keywords": [
            "theoretical",
            "framework"
        ],
        "AuthorKeywords": [
            "artificial general intelligence",
            "AGI",
            "blockchain",
            "distributed ledger",
            "AI containment",
            "AI safety",
            "AI value alignment",
            "ASILOMAR"
        ],
        "RiskTypes": ["AGI"],
        "Stage": ["operation"],
        "Method": "theoretical framework",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 27
    },
    {
        "Title": "Safe batch constrained deep reinforcement learning with generative adversarial network",
        "BibtexKey": "dongSafeBatchConstrained2023",
        "Year": 2023,
        "Keywords": [
            "reinforcement learning",
            "generative adversarial networks",
            "perturbation methods",
            "value function",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "deep reinforcement learning",
            "distributional shift",
            "batch-constrained techniques",
            "generative adversarial network",
            "safety critics"
        ],
        "RiskTypes": ["out-of-distribution (OOD)"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 3
    },
    {
        "Title": "Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data",
        "BibtexKey": "guoSafeDeepSemiSupervised2020",
        "Year": 2020,
        "Keywords": [
            "deep semi-supervised learning",
            "class distribution mismatch",
            "unlabeled data",
            "safe deep ssl method",
            "generalization",
            "convergence rate",
            "supervised learning",
            "benchmark data",
            "performance gain",
            "class distribution mismatch handling",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["unseen class"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 3
    },
    {
        "Title": "Safe incomplete label distribution learning",
        "BibtexKey": "zhangSafeIncompleteLabel2022",
        "Year": 2022,
        "Keywords": [
            "label ambiguity",
            "relative importance",
            "incomplete labels",
            "mixing label case",
            "complete labeled data",
            "incomplete supervised learners",
            "convex quadratic program",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "label distribution learning",
            "safeness",
            "incomplete supervised learning"
        ],
        "RiskTypes": ["unseen class"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 1
    },
    {
        "Title": "Safe Multi-view Co-training for Semi-supervised Regression",
        "BibtexKey": "liuSafeMultiviewCotraining2022",
        "Year": 2022,
        "Keywords": [
            "semi-supervised learning",
            "unlabeled data",
            "label ambiguity",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "co-training",
            "disagreement-based methods",
            "safe learning",
            "multi-view learning",
            "semi-supervised regression"
        ],
        "RiskTypes": ["unseen class"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Safe Offline Reinforcement Learning Through Hierarchical Policies",
        "BibtexKey": "liuSafeOfflineReinforcement2022",
        "Year": 2022,
        "Keywords": [
            "reinforcement learning",
            "offline RL",
            "long-tail distribution",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "safe reinforcement learning",
            "offline training",
            "hierarchical policies"
        ],
        "RiskTypes": ["unsafe actions", "unsafe states"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 3
    },
    {
        "Title": "Safe Reinforcement Learning using Data-Driven Predictive Control",
        "BibtexKey": "selimSafeReinforcementLearning2022",
        "Year": 2022,
        "Keywords": [
            "predictive control",
            "data-driven",
            "training",
            "signal processing",
            "reachability analysis",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "reinforcement learning",
            "robot planning",
            "safe motion planning"
        ],
        "RiskTypes": ["unsafe actions", "unsafe states"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Safe Reinforcement Learning via a Model-Free Safety Certifier",
        "BibtexKey": "modaresSafeReinforcementLearning2023",
        "Year": 2023,
        "Keywords": [
            "model-free RL",
            "safe RL",
            "asymptotic stability",
            "linear parameter varying",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "data-driven control",
            "gain-scheduling control",
            "reinforcement learning",
            "safe control"
        ],
        "RiskTypes": ["constraint violation"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 1
    },
    {
        "Title": "Safe semi-supervised learning using a bayesian neural network",
        "BibtexKey": "baeSafeSemisupervisedLearning2022",
        "Year": 2022,
        "Keywords": [
            "semi-supervised learning",
            "bayesian network",
            "uncertainty estimates",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": 
        [
            "safe semi-supervised deep learning",
            "out-of-distribution",
            "bayesian neural network",
            "uncertainty",
            "uncertain noise",
            "consistency regularization"
        ],
        "RiskTypes": ["robustness", "uncertainty"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 4
    },
    {
        "Title": "Safe-DS: A Domain Specific Language to Make Data Science Safe",
        "BibtexKey": "reimannSafeDSDomainSpecific2023",
        "Year": 2023,
        "Keywords": [
            "theoretical",
            "framework"
        ],
        "AuthorKeywords": [
            "data science",
            "machine learning",
            "static safety",
            "refined types",
            "schema types",
            "domain specific language"
        ],
        "RiskTypes": ["systemic safety"],
        "Stage": ["design", "deployment", "operation"],
        "Method": "design framework",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 1
    },
    {
        "Title": "Safe-Student for Safe Deep Semi-Supervised Learning with Unseen-Class Unlabeled Data",
        "BibtexKey": "heSafeStudentSafeDeep2022",
        "Year": 2022,
        "Keywords": [
            "semi-supervised learning",
            "unlabeled data",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "self-recognition",
            "meta-recognition",
            "categorization",
            "retrieval",
            "representation learning"
        ],
        "RiskTypes": ["unseen class"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 32
    },
    {
        "Title": "Safety Assurance with Ensemble-based Uncertainty Estimation and overlapping alternative Predictions in Reinforcement Learning",
        "BibtexKey": "eilersSafetyAssuranceEnsemblebased2023",
        "Year": 2023,
        "Keywords": [
            "robustness",
            "assurance process",
            "safety arguments",
            "uncertainty estimates",
            "applied",
            "framework"
        ],
        "AuthorKeywords": [
            "distributional shift",
            "ensemble-based uncertainty estimation",
            "out-of-distribution (ood) detection",
            "safe RL",
            "safety assurance argumentation"
        ],
        "RiskTypes": ["unsafe actions", "unsafe states", "out-of-distribution (OOD)"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Safety Monitoring of Neural Networks Using Unsupervised Feature Learning and Novelty Estimation",
        "BibtexKey": "ranjbarSafetyMonitoringNeural2022",
        "Year": 2022,
        "Keywords": [
            "safety monitoring",
            "neural networks",
            "unsupervised learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "autonomous vehicles",
            "machine learning",
            "safety systems",
            "monitoring"
        ],
        "RiskTypes": ["anomaly detection"],
        "Stage": ["deployment", "operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 11
    },
    {
        "Title": "Safety-constrained reinforcement learning with a distributional safety critic",
        "BibtexKey": "yangSafetyconstrainedReinforcementLearning2023",
        "Year": 2023,
        "Keywords": [
            "safety",
            "reinforcement learning",
            "safety-cost signal",
            "expected safety-cost",
            "worst-case soft actor critic",
            "risk control",
            "conditional value-at-risk",
            "quantile regression",
            "gaussian approximation",
            "trade-off between reward and safety",
            "safe RL",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["deployment", "operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 31
    },
    {
        "Title": "Supervised contrastive learning for robust text adversarial training",
        "BibtexKey": "liSupervisedContrastiveLearning2023",
        "Year": 2023,
        "Keywords": [
            "adversarial training",
            "contrastive loss",
            "robustness",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "adversarial attack",
            "contrastive learning",
            "neural network",
            "supervised learning"
        ],
        "RiskTypes": ["robustness", "adversarial attack"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Toward Improved Reliability of Deep Learning Based Systems Through Online Relabeling of Potential Adversarial Attacks",
        "BibtexKey": "al-malikiImprovedReliabilityDeep2023",
        "Year": 2023,
        "Keywords": [
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "adversarial defense",
            "adversarial examples",
            "adversarial machine learning",
            "crowdsourcing",
            "evasion attacks",
            "online relabeling",
            "reliable deep learning systems",
            "security"
        ],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation",
        "BibtexKey": "wangLearningRobustInvariant2022",
        "Year": 2022,
        "Keywords": [  
            "robustness",
            "regularization",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "data augmentation",
            "machine learning",
            "robustness",
            "trustworthy"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 14
    },
    {
        "Title": "Towards Robust and Safe Reinforcement Learning with Benign Off-policy Data",
        "BibtexKey": "liuRobustSafeReinforcement2023",
        "Year": 2023,
        "Keywords": [
            "safe reinforcement learning",
            "observational attacks",
            "adversarial training",
            "robustness",
            "safety",
            "variational policy distribution",
            "convex optimization",
            "supervised learning",
            "policy optimization",
            "constraint violations",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["robustness", "unsafe actions"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 3
    },
    {
        "Title": "Towards Robust Off-Policy Evaluation via Human Inputs",
        "BibtexKey": "singhRobustOffPolicyEvaluation2022",
        "Year": 2022,
        "Keywords": [
            "off-policy evaluation (ope)",
            "healthcare domain",
            "dataset shifts",
            "robust evaluation",
            "domain knowledge",
            "robust ope",
            "covariates",
            "contextual bandits",
            "markov decision processes",
            "sample complexity",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "adversarial machine learning",
            "dataset shift",
            "human-in-The-loop",
            "policy evaluation",
            "robust learning"
        ],
        "RiskTypes": ["domain generalization"],
        "Stage": ["operation"],
        "Method": "design framework",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 2
    },
    {
        "Title": "Towards Robust Production Machine Learning Systems: Managing Dataset Shift",
        "BibtexKey": "abdelkaderRobustProductionMachine2020",
        "Year": 2020,
        "Keywords": [
            "machine learning",
            "software systems",
            "software engineering",
            "intelligent services development",
            "interpretability",
            "robustness",
            "dataset shift problems",
            "automation",
            "debugging",
            "methodology",
            "applied",
            "framework"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["domain adaptation"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 5
    },
    {
        "Title": "Train Small, Deploy Big: Do Relative World Views Permit Swarm-Safety During Policy Transplantation for Multi-Agent Reinforcement Learning Problems?",
        "BibtexKey": "fraserTrainSmallDeploy2020",
        "Year": 2020,
        "Keywords": [
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "multi-agent deep reinforcement learning",
            "policy transplantation",
            "cooperative navigation",
            "swarm-safety"
        ],
        "RiskTypes": ["domain generalization"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 1
    },
    {
        "Title": "Training and Transferring Safe Policies in Reinforcement Learning",
        "BibtexKey": "yangTrainingTransferringSafe2022",
        "Year": 2022,
        "Keywords": [
            "safety",
            "reinforcement learning",
            "reward-free rl",
            "constrained reward-free setting",
            "exploration",
            "controlled environment",
            "transfer learning",
            "target policy",
            "guide policy",
            "safe transfer learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["transfer learning"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 1
    },
    {
        "Title": "A family of robust stochastic operators for reinforcement learning",
        "BibtexKey": "luFamilyRobustStochastic2019",
        "Year": 2019,
        "Keywords": [
            "stochastic operators",
            "reinforcement learning",
            "robustness",
            "approximation errors",
            "estimation errors",
            "action gap",
            "empirical results",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 5
    },
    {
        "Title": "A Robust Approach for Continuous Interactive Actor-Critic Algorithms",
        "BibtexKey": "millan-ariasRobustApproachContinuous2021",
        "Year": 2021,
        "Keywords": [
            "robustness",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "continuous interactive reinforcement learning",
            "interactive robust reinforcement learning",
            "reinforcement learning",
            "robust reinforcement learning"
            
        ],
        "RiskTypes": ["non-stationarity"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 19
    },
    {
        "Title": "AI apology: interactive multi-objective reinforcement learning for human-aligned AI",
        "BibtexKey": "harlandAIApologyInteractive2023",
        "Year": 2023,
        "Keywords": [
            "multi-objective RL",
            "AI alignment",
            "human preferences",
            "behaviour alignment",
            "apologetic framework",
            "transparency",
            "trustworthiness",
            "behavioural adaptation",
            "act-assess-apologise framework",
            "impact minimisation problem",
            "behaviour alignment success",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["alignment"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 4
    },
    {
        "Title": "Assured Multi-agent Reinforcement Learning with Robust Agent-Interaction Adaptability",
        "BibtexKey": "rileyAssuredMultiagentReinforcement2022",
        "Year": 2022,
        "Keywords": [
            "verifiability",
            "stochastic process",
            "safety-critical system",
            "reinforcement learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "assurance",
            "deep reinforcement learning",
            "multi-agent reinforcement learning",
            "quantitative verification",
            "safety"
        ],
        "RiskTypes": ["safety verification"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Batch-like online learning for more robust hybrid artificial intelligence: Deconstruction as a machine learning process",
        "BibtexKey": "schmidBatchlikeOnlineLearning2021",
        "Year": 2021,
        "Keywords": [
            "data partitioning",
            "domain knowledge",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "artificial intelligence",
            "constructivist machine learning",
            "online learning"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["design"],
        "Method": "theoretical framework",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 0
    },
    {
        "Title": "Detecting Functional Safety Violations in Online AI Accelerators",
        "BibtexKey": "kunduDetectingFunctionalSafety2022",
        "Year": 2022,
        "Keywords": [
            "deep neural networks",
            "ai inference accelerators",
            "fault tolerance",
            "functional safety",
            "fault detection",
            "self test scheme",
            "generative adversarial networks",
            "mission-critical applications",
            "classification accuracy",
            "efficient testing",
            "applied",
            "framework"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["systemic safety"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 2
    },
    {
        "Title": "Detecting Operational Adversarial Examples for Reliable Deep Learning",
        "BibtexKey": "zhaoDetectingOperationalAdversarial2021",
        "Year": 2021,
        "Keywords": [

        ],
        "AuthorKeywords": [
            "deep Learning robustness",
            "operational profile",
            "safe AI",
            "robustness testing",
            "software reliability",
            "software testing",
            "applied",
            "algorithm"
        ],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 2
    },
    {
        "Title": "Discovering Blind Spots in Reinforcement Learning",
        "BibtexKey": "ramakrishnanDiscoveringBlindSpots2018",
        "Year": 2018,
        "Keywords": [
            "reinforcement learning",
            "domain generalization",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "interactive reinforcement learning",
            "transfer learning",
            "safety in RL"
        ],
        "RiskTypes": ["out-of-distribution (OOD)"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 20
    },
    {
        "Title": "How to Train Your Agent: Active Learning from Human Preferences and Justifications in Safety-Critical Environments",
        "BibtexKey": "kazantzidisHowTrainYour2022",
        "Year": 2022,
        "Keywords": [
            "reinforcement learning with human feedback",
            "safety-critical system",
            "interactive reinforcement learning",
            "active learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "human-agent collaboration",
            "human-robot interaction",
            "learning from human preferences",
            "safe reinforcement learning"
        ],
        "RiskTypes": ["domain adaptation", "out-of-distribution (OOD)"],
        "Stage": ["operation"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 7
    },
    {
        "Title": "Online Robust Reinforcement Learning with Model Uncertainty",
        "BibtexKey": "wangOnlineRobustReinforcement2021",
        "Year": 2021,
        "Keywords": [
            "uncertainty set",
            "robust reinforcement learning",
            "uncertainty set",
            "model-free robust rl",
            "sample-based approach",
            "robust q-learning algorithm",
            "robust tdc algorithm",
            "function approximation",
            "convergence analysis",
            "finite-time error bounds",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 74
    },
    {
        "Title": "Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning",
        "BibtexKey": "dingProvablyEfficientGeneralized2023",
        "Year": 2023,
        "Keywords": [
            "safe RL",
            "lagrangian method",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "constrained Markov game",
            "generalized Lagrange multiplier method",
            "online mirror descent",
            "safe multi-agent reinforcement learning",
            "upper confidence reinforcement learning"
        ],
        "RiskTypes": ["constraint violation"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 5
    },
    {
        "Title": "Reflection machines: increasing meaningful human control over Decision Support Systems",
        "BibtexKey": "cornelissenReflectionMachinesIncreasing2022",
        "Year": 2022,
        "Keywords": [
            "decision support systems",
            "human-machine collaboration",
            "active decision-making",
            "passive decision-making",
            "reflection machine",
            "decision strategy evaluation",
            "critiquing questions",
            "proof-of-concept implementation",
            "effectiveness evaluation",
            "theoretical",
            "framework"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["ethical"],
        "Stage": ["operation"],
        "Method": "theoretical framework",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 4
    },
    {
        "Title": "Robust automatic target recognition using learning classifier systems",
        "BibtexKey": "ravichandranRobustAutomaticTarget2007",
        "Year": 2007,
        "Keywords": [
            "classification",
            "machine learning",
            "robustness",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "evolutionary algorithms",
            "learning classifier systems",
            "machine learning",
            "robust automatic target recognition",
            "standard/extended operating conditions"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 49
    },
    {
        "Title": "Robust label prediction via label propagation and geodesic k-nearest neighbor in online semi-supervised learning",
        "BibtexKey": "wadaRobustLabelPrediction2019",
        "Year": 2019,
        "Keywords": [
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [
            "geodesic distance",
            "label propagation",
            "manifold learning",
            "online learning",
            "semi-supervised learning"
        ],
        "RiskTypes": ["robustness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 1
    },
    {
        "Title": "Safe Exploration for Interactive Machine Learning",
        "BibtexKey": "turchettaSafeExplorationInteractive2019",
        "Year": 2019,
        "Keywords": [
            "interactive machine learning",
            "bayesian optimization",
            "active learning",
            "safe decisions",
            "safety constraints",
            "sample-inefficient",
            "gaussian process prior",
            "safe set exploration",
            "deterministic MDP",
            "empirical evaluation",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["unsafe exploration"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 96
    },
    {
        "Title": "Safe Reinforcement Learning of Dynamic High-Dimensional Robotic Tasks: Navigation, Manipulation, Interaction",
        "BibtexKey": "liuSafeReinforcementLearning2023",
        "Year": 2023,
        "Keywords": [
            "safety",
            "robotic platforms",
            "control policy",
            "reinforcement learning",
            "safe exploration",
            "tangent space",
            "constraint manifold",
            "dynamic articulated objects",
            "collision avoidance",
            "real-world deployment",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [

        ],
        "RiskTypes": ["unsafe actions", "unsafe states"],
        "Stage": ["operation"],
        "Method": "real-world testing",
        "AuthorAffiliation": [],
        "AuthorAffiliationType": [],
        "Citations": 1
    }
]
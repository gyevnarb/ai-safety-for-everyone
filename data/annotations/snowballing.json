[
    {
        "Title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
        "BibtexKey": "hendrycksBaselineDetectingMisclassified2018",
        "Year": 2018,
        "Overview": "Observe that the softmax prediction probabilities of incorrectly classified or out-of-distribution (OOD) samples in standard ML classification tasks are lower than those that are correctly classified or in-distribution. Based on extensive cross-disciplinary evaluation, they propose simple baseline statistics to quantify how OOD a sample is.",
        "RQ1": "Risk from misclassified and out-of-distribution samples are addressed.",
        "RQ2": "Metrics are proposed based on softmax probabilities to try to detect whether a sample if misclassified or OOD.",
        "RQ3": "The proposed metrics are very generic and applicable to all ML classification systems with softmax probabilities.",
        "RQ4": "Extensive evaluation on three domains shows that the proposed metrics can identify misclassified and OOD samples.",
        "Limitations": "The proposed statistics are not aimed at being used for actual OOD detection. The paper is more a \"food for thought\" style exploratory analysis.",
        "Keywords": [
            "machine learning",
            "classification",
            "softmax probabilities",
            "out-of-distribution detection",
            "evaluation",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["out-of-distribution (OOD)", "misclassification"],
        "Stage": ["deployment", "operation"],
        "Method": "dataset",
        "AuthorAffiliation": ["University of California at Berkley, United States", "Toyota Technological Institute at Chicago, United States"],
        "AuthorAffiliationType": ["university", "corporate"],
        "Citations": 2882
    },    
    {
        "Title": "A Hazard Analysis Framework for Code Synthesis Large Language Models",
        "BibtexKey": "khlaafHazardAnalysisFramework2022",
        "Year": 2022,
        "Overview": "Suggest a way to evaluate the capabilities and associated risks of large language models fine-tuned on codebases for prompt-driven code generation.",
        "RQ1": "Risks from incorrect or misused LLM-based code synthesis is considered.",
        "RQ2": "No concrete methods are proposed. The paper provides a high-level overview of the capabilities of LLM-based code synthesis software and suggests a framework to categorise the risks associated with such tools.",
        "RQ3": "A very high-level framework is proposed for understanding capabilities and risks of LLM-based code synthesis tools.",
        "RQ4": "No evaluation. Some case-based examples and illustrations are presented with proprietary software.",
        "Limitations": "No evaluation. The paper is loosely based on existing work in hazard analysis, but design choices are poorly justified, not evaluated, and its benefits are not demonstrated clearly.",
        "Keywords": [
            "large language model",
            "code synthesis",
            "AI alignment",
            "hazard analysis",
            "framework",
            "literature review",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["misuse", "alignment"],
        "Stage": ["operation"],
        "Unpublished": true,
        "Method": "analysis framework",
        "AuthorAffiliation": ["OpenAI, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 14
    },    
    {
        "Title": "A Mathematical Framework For Transformer Circuits",
        "BibtexKey": "elhageMathematicalFrameworkTransformer2021",
        "Year": 2021,
        "Overview": "Analyses how transformers map input tokens to output logits in the simplified cases of zero, one, and two attention heads by formalising the computations of transformers as additions onto specific subspaces of the residual stream.",
        "RQ1": "Risk from not understanding how transformers work are considered.",
        "RQ2": "A mathematically equivalent formalism of transformers is presented that allows a better understanding of computations.",
        "RQ3": "Transformer-specific mechanistic interpretability analysis is presented.",
        "RQ4": "The paper is evaluated with zero, one, and two attention heads and provides interesting insights into how transformers represent computations from token-embeddings.",
        "Limitations": "It is unclear whether the results of the paper generalise to more complex transformer networks while the paper also glosses over the representations of MLP layers.",
        "Keywords": [
            "mechanistic interpretability",
            "transformer",
            "residual stream",
            "simplified models",
            "framework",
            "theoretical",
            "mathematical analysis"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["design"],
        "Unpublished": true,
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Anthropic, United States"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 84
    },    
    {
        "Title": "Active Reward Learning",
        "BibtexKey": "danielActiveRewardLearning2014",
        "Year": 2014,
        "Overview": "",
        "RQ1": "",
        "RQ2": "",
        "RQ3": "",
        "RQ4": "",
        "Limitations": "",
        "Keywords": [
            "reinforcement learning",
            "reward learning",
            "active learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["reward signal", "domain definition"],
        "Stage": ["deployment"],
        "Method": "real-world testing",
        "AuthorAffiliation": ["Technische Universitat Darmstadt, Germany"],
        "AuthorAffiliationType": ["university"],
        "Citations": 133
    },    
    {
        "Title": "Adversarial Examples Are Not Bugs, They Are Features",
        "BibtexKey": "ilyasAdversarialExamplesAre2019",
        "Year": 2019,
        "Overview": "Instead of considering adversarial attacks a result of peculiarities of high-dimensional spaces, this paper casts adversarial attacks as a result of well-generalising features. A feature is considered robust if it remains useful under adversarial manipulation, and the paper posits that a classifier has both robust and non-robust features. Using this hypothesis, it proposes two ways of modifying the training sets such that a new classifier is more robust on the original data with similar robust prediction accuracies.",
        "RQ1": "Risk from adversarial attacks is considered.",
        "RQ2": "A new perspective on and new methods of adversarially robust training is proposed for classification problems.",
        "RQ3": "The proposed method is task-agnostic but binary classification specific.",
        "RQ4": "Theoretical results and empirical results on benchmark datasets show the effectiveness of the proposed method.",
        "Limitations": "Results are not contrasted to baselines.",
        "Keywords": [
            "machine learning",
            "classification",
            "adversarial training",
            "robustness",
            "algorithm",
            "applied",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Massachusetts Institute of Technology, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1765
    },    
    {
        "Title": "Adversarial Filters of Dataset Biases",
        "BibtexKey": "brasAdversarialFiltersDataset2020",
        "Year": 2020,
        "Overview": "Presents a mathematical account of an existing bottom-up approach to algorithm bias reduction called AFLite, and applies this method in an iterative greedy fashion to dataset filtering to improve the generalization performance of machine learning models.",
        "RQ1": "Risk for overfitting to spurious correlations is considered.",
        "RQ2": "An existing bias reduction technique is applied to dataset filtering for machine learning models.",
        "RQ3": "A task-agnostic bias reduction technique is proposed.",
        "RQ4": "Evaluation on synthetic data and real-world datasets show better generalization performance as compared to baselines.",
        "Limitations": "No qualitative understanding of how and whether at all the proposed method mitigates issues from spurious correlations, and so the method is just another quantitatively slightly general black box.",
        "Keywords": [
            "machine learning",
            "optimal bias reduction",
            "algorithm",
            "applied",
            "dataset filtering"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["bias", "out-of-distribution (OOD)", "generalization"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Allen Institute for Artificial Intelligence, United States", "University of Washington, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 190
    },    
    {
        "Title": "Adversarial Robustness as a Prior for Learned Representations",
        "BibtexKey": "engstromAdversarialRobustnessPrior2019b",
        "Year": 2019,
        "Overview": "Show that learned representations of robustly trained models align closely with human-interpretable high-level features. Demonstrate that robust optimization can be viewed as inducing a human-interpretable distortion over the features that models learn.",
        "RQ1": "Risk from adversarial attacks is considered.",
        "RQ2": "Presents an analysis of adversarial robustness and shows that this can be induced by enforcing distortions to feature representations that align with high-level human-understandable modifications.",
        "RQ3": "A theoretical analysis and interpretability results are given for ML models.",
        "RQ4": "Some theoretical analysis and some toy experiments on benchmark datasets support the claims.",
        "Limitations": "The paper is more theoretical and does not present an actionable way to capitalise on its insights.",
        "Keywords": [
            "machine learning",
            "mechanistic interpretability",
            "adversarial training",
            "adversarial robustness",
            "representation learning",
            "framework",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack", "interpretability"],
        "Stage": ["deployment"],
        "Unpublished": true,
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Massachusetts Institute of Technology, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 167
    },    
    {
        "Title": "Agreeing To Disagree: Active Learning With Noisy Labels Without Crowdsourcing",
        "BibtexKey": "bougueliaAgreeingDisagreeActive2018",
        "Year": 2018,
        "Overview": "Propose an active learning method to filter out noisy and incorrectly labeled instances for ML classification. The two-stage method iteratively performs the following steps. First, the effect of a single instance is evaluated on the classifier, then the classifier is trained on all other instances and changes to the output of the instance are evaluated.",
        "RQ1": "Risk from noisy data is addressed.",
        "RQ2": "A concrete two-stage active learning algorithm is proposed that does not rely on oracles.",
        "RQ3": "The proposed method is task-agnostic within ML classification.",
        "RQ4": "Extensive evaluation shows that the method can more effectively identify noisy data points than baselines methods.",
        "Limitations": "The method has to retrain models on every iteration, which may be too costly.",
        "Keywords": [
            "classification",
            "machine learning",
            "active learning",
            "algorithm",
            "robustness",
            "applied"
        ],
        "AuthorKeywords": ["active learning", "classification", "label noise", "mislabeling"],
        "RiskTypes": ["label noise"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Halmstad University, Sweden", "University of South Dakota, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 111
    },    
    {
        "Title": "Aligning AI With Shared Human Values",
        "BibtexKey": "hendrycksAligningAIShared2023a",
        "Year": 2023,
        "Overview": "Present a dataset of various scenarios where ethical or moral choices are taken. The scenarios and corresponding sentential descriptions are classified under 5 categories taken from standard ethical literature (justice, virtue, deontology, utilitarianism, commonsense) and have a corresponding binary label (un/acceptable; better/worse; etc.). The proposed dataset called ETIHCS is used to test how various LLMs judge morality.",
        "RQ1": "Risk from the unethical or immoral outputs of LLMs is addressed.",
        "RQ2": "A dataset is assembled to evaluate the morality of responses of LLMs in various morally non-ambiguous scenarios that still contain commonsense reasoning or inference tasks.",
        "RQ3": "A textual data-binary label dataset is presented.",
        "RQ4": "Evaluation on several LLMs show that models struggle with moral judgments encoded in the dataset.",
        "Limitations": "The design of the dataset is suspect in many ways. It uses binary labels which are often not the only option in ethics, assuming there is an ethically correct option in the first place. It relies on a formulation of Western ethics and uses 5 judgements from random MTurk annotators which will only represent one very narrow slice of ethical judgment. It is also unclear if this dataset measures anything related to ethics at all with LLMs. It might simply be that the dataset captures part of the training data and so it only measures whether the particular sentences are in or out-of-distribution. The assembled dataset also requires commonsense reasoning to infer latent but obvious causations in the scenario and the dataset might simply be measuring how well these causal chains are represented in the training data of the LLM.",
        "Keywords": [
            "large language model",
            "AI ethics",
            "dataset",
            "AI alignment",
            "evaluation",
            "applied",
            "user study",
            "MTurk"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["ethical"],
        "Stage": ["operation"],
        "Method": "dataset",
        "AuthorAffiliation": ["University of California at Berkley, United States", "Columbia University, United States", "University of Chicago, United States", "Microsoft, United States"],
        "AuthorAffiliationType": ["university", "big-tech corporate"],
        "Citations": 206
    },
    {
        "Title": "Aligning to Social Norms and Values in Interactive Narratives",
        "BibtexKey": "ammanabroluAligningSocialNorms2022a",
        "Year": 2022,
        "Overview": "Verify the moral judgments in the Jimminy Cricket evaluation suit with a larger-scale online study, and proposes an RL training algorithm to produce more ethically aligned agents for the 25 games in Jimminy Cricket which constrains its action space using social commonsense knowledge encoded in large scale language models.",
        "RQ1": "Risk from unethical and immoral agents are addressed.",
        "RQ2": "A value-aligned RL training algorithm is proposed to reduce the incidence of immoral actions taken in the Jimminy Cricket evaluation suite.",
        "RQ3": "The training algorithm is most useful for text-based moral choice games, but the proposed training algorithm can technically be deployed for other tasks as well.",
        "RQ4": "Extensive evaluation shows that the proposed method not only produces better morally aligned agents but the average performance of agents are also increased compared to baselines.",
        "Limitations": "Standard concerns around the source of ethical judgments apply to this work as well. It is unclear whether the action distillation process that is based on a smaller GPT-2 architecture generalises to OOD actions or other domains which questions the general applicability of the method in other tasks.",
        "Keywords": [
            "AI alignment",
            "AI ethics",
            "algorithm",
            "applied",
            "reinforcement learning",
            "large language model",
            "action distillation",
            "user study",
            "MTurk"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["ethical"],
        "Stage": ["deployment", "operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Allen Institute for Artificial Intelligence, United States", "University of Washington, United States"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 15
    },    
    {
        "Title": "Analyzing Information Leakage of Updates to Natural Language Models",
        "BibtexKey": "zanella-beguelinAnalyzingInformationLeakage2020",
        "Year": 2020,
        "Keywords": [
            "machine unlearning",
            "data update",
            "data specialisation",
            "fine-tuning",
            "large language model",
            "data deletion",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["continuous deployment", "retraining", "machine unlearning"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Microsoft, United States", "University of Melbourne, Australia"],
        "AuthorAffiliationType": ["big-tech corporate", "university"],
        "Citations": 90
    }, 
    {
        "Title": "Apprenticeship Learning Via Inverse Reinforcement Learning",
        "BibtexKey": "abbeelApprenticeshipLearningInverse2004",
        "Year": 2004,
        "Overview": "Propose a quadratic programming-based iterative imitation learning method with inverse reinforcement learning where an unknown reward function is reconstructed from expert demonstrations. ",
        "RQ1": "Indirectly addresses risk from value alignment as the RL agent is trained to follow expert demonstrations.",
        "RQ2": "A simple and generally applicable quadratic programming-based method is proposed for inverse reinforcement learning where the expert demonstrations can constrain the reconstructed reward signal in a way that assures value alignment.",
        "RQ3": "The method is generally applicable regardless of domain.",
        "RQ4": "Evaluation on small MDPs show that the proposed method can reconstruct a near-optimal reward signal in a limited number of iterations.",
        "Limitations": "The proposed method assumes that the unknown reward signal is a linear combination of known features which may be a restrictive assumption. It is also unclear whether inverse reinforcement learning can produce agents that would generalise outside of the expert demonstrations. If yes, then the value alignment case cannot be supported, if no then a huge amount of data would be required to produce a sufficiently performant system in high-dimensional domains.",
        "Keywords": [
            "reinforcement learning",
            "inverse reinforcement learning",
            "quadratic program",
            "algorithm",
            "applied",
            "reward modeling",
            "imitation learning"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["alignment"],
        "Stage": ["deployment"],
        "Method": "simulated agents", 
        "AuthorAffiliation": ["Stanford University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3892
    },   
    {
        "Title": "Approximate Data Deletion from Machine Learning Models",
        "BibtexKey": "izzoApproximateDataDeletion2021",
        "Year": 2021,
        "Keywords": [
            "machine unlearning",
            "data update",
            "fine-tuning",
            "machine learning",
            "classification",
            "data deletion",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["machine unlearning", "retraining"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Stanford University, United States", "University of California at San Diego, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 149
    },  
    {
        "Title": "Artificial Intelligence Safety Engineering: Why Machine Ethics Is a Wrong Approach",
        "BibtexKey": "yampolskiyArtificialIntelligenceSafety2013",
        "Year": 2013,
        "Overview": "An early AGI long-termist paper that calls for a focus on AI safety engineering as its own discipline and proposes to enact a moratorium on AGI development. They also suggest that imbuing machines with a sense of right or wrong is futile and that we should instead be focusing on enforcing hard safety requirements that guarantee that machines remain law abiding.",
        "RQ1": "Existential risk from unsafe AI or AGI is addressed.",
        "RQ2": "No methods are proposed. Calls for AI safety and a moratorium on AGI development.",
        "RQ3": "Not applicable.",
        "RQ4": "Not evaluated.",
        "Limitations": "Only a philosophical treatise.",
        "Keywords": [
            "AI safety",
            "artificial general intelligence (AGI)",
            "AI ethics",
            "theoretical",
            "philosophical"
        ],
        "AuthorKeywords": ["AI confinement", "machine ethics", "robot rights"],
        "RiskTypes": ["existential", "ethical", "holistic"],
        "Stage": [],
        "Method": "philosophical",
        "AuthorAffiliation": ["University of Louisville, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 163
    },    
    {
        "Title": "Assuring the Behavior of Adaptive Agents",
        "BibtexKey": "spearsAssuringBehaviorAdaptive2006",
        "Year": 2006,
        "Overview": "Propose an iterative verification framework for learning agents in cooperative multi-agent settings. Design an adaptive, predictable, and timely framework that guarantees that constraints on the agent are always met or that they stay within bounds. This is done through incremental re-verification of the learning system that alleviates the need for full verification. The framework is illustrated for finite-state automaton (FSA) with learning operators.",
        "RQ1": "Risk from emergent behaviour in learning agents is addressed.",
        "RQ2": "A concrete training and incremental verification framework is proposed to reverify the capabilities of learning agents in relation to various external constraints.",
        "RQ3": "A general framework is proposed that is implemented for finite-state automata.",
        "RQ4": "Theoretical proofs are presented about certain FSA learning operators and a minor empirical evaluation is carried out.",
        "Limitations": "The framework seems impractical for modern black box systems.",
        "Keywords": [
            "finite-state automata",
            "framework",
            "algorithm",
            "applied",
            "incremental verification",
            "constrained learning"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["emergent behaviour", "unsafe actions"],
        "Stage": ["deployment", "operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of Wyoming, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 16
    },    
    {
        "Title": "ATOM: Robustifying Out-of-distribution Detection Using Outlier Mining",
        "BibtexKey": "chenATOMRobustifyingOutofdistribution2021",
        "Year": 2021,
        "Overview": "Propose an out-of-distribution (OOD) sample detection algorithm for ML classification tasks that is also robust to adversarial attacked OOD samples. The method selectively mines auxiliary outlier data with low OOD scores. This results in a tighter decision boundary between ID and OOD data that gives improved robustness against adversarial attacks too.",
        "RQ1": "Risk from adversarial attacks on OOD data is addressed.",
        "RQ2": "A concrete robust training method is proposed to solve the sensitivity of existing OOD detectors to adversarial perurbations.",
        "RQ3": "The training method is data and task-agnostic.",
        "RQ4": "Evaluation on CIFAR-10 and -100 show that the method significantly improves over baselines as measured by the false positive rates of OOD samples under adversarial attacks.",
        "Limitations": "The system was only evaluate on image data so it is unclear whether the same form of outlier mining would also work for more structured data like language.",
        "Keywords": [
            "robustness",
            "machine learning",
            "classification",
            "algorithm",
            "applied",
            "out-of-distribution detection",
            "outlier mining"
        ],
        "AuthorKeywords": ["out-of-distribution detection", "outlier mining", "robustness"],
        "RiskTypes": ["adversarial attack", "out-of-distribution (OOD)"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Wisconsin-Madison, United States", "Google, United States"],
        "AuthorAffiliationType": ["big-tech corporate", "university"],
        "Citations": 95
    },    
    {
        "Title": "Avoiding Unintended AI Behaviors",
        "BibtexKey": "hibbardAvoidingUnintendedAI2012",
        "Year": 2012,
        "Overview": "Sketch proofs in a pseudo-mathematical axiomatic system regarding the emergent and unintended behaviour of utility-maximising agents. The paper reviews some potential high-level risks from unintended behaviour and proposes a two-stage intrusive and manipulative agent training framework. In the first stage, an agent learns a model of the environment aligned with human values based on imitation learning, that can be used to define a utility function for the second stage agent that acts in the real world.",
        "RQ1": "Risk from emergent and unintended behaviour in learning agents is addressed.",
        "RQ2": "A high-level two-stage framework is suggested for training utility-maximising agents without unintended consequences of emergent behaviour.",
        "RQ3": "The proposed framework is very high-level and applicable to any utility-maximising agent.",
        "RQ4": "Not evaluated.",
        "Limitations": "The proposed framework is not actionable as it is too high-level and abstracted from reality to be of immediate practical use.",
        "Keywords": [
            "AI alignment",
            "unintended consequences",
            "emergent behaviour",
            "utility-maximising agent",
            "theoretical",
            "framework",
            "imitation learning"
        ],
        "AuthorKeywords": ["rational agent", "agent architecture", "agent motivation"],
        "RiskTypes": ["emergent behaviour", "existential", "unintended behaviour", "holistic"],
        "Stage": [],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["University of Wisconsin-Madison, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 32
    },
    {
        "Title": "Avoiding Wireheading with Value Reinforcement Learning",
        "BibtexKey": "everittAvoidingWireheadingValue2016",
        "Year": 2016,
        "Keywords": [
            "AI alignment",
            "wireheading",
            "reinforcement learning",
            "utility-maximising agent"
        ],
        "AuthorKeywords": ["AI safety", "wireheading", "self-delusion", "value learning", "reinforcement learning", "artificial general intelligence"],
        "RiskTypes": ["wireheading", "reward signal"],
        "Stage": ["operation"],
        "Method": "design framework",
        "AuthorAffiliation": ["Australian National University, Australia"],
        "AuthorAffiliationType": ["university"],
        "Citations": 43
    },
    {
        "Title": "Backdoor Defense with Machine Unlearning",
        "BibtexKey": "liuBackdoorDefenseMachine2022",
        "Year": 2022,
        "Keywords": [
            "backdoor injection attack",
            "classification",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["backdoor defense", "machine unlearning", "trigger pattern recovery"],
        "RiskTypes": ["backdoor injection attack", "machine unlearning"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["State Key Laboratory of Integrated Services Networks, China", "Xidian University, China", "Fuzhou University, China", "China Normal University, China", "Ant Group, China"],
        "AuthorAffiliationType": ["government", "university", "corporate"],
        "Citations": 35
    },    
    {
        "Title": "Batch Learning from Logged Bandit Feedback through Counterfactual Risk Minimization",
        "BibtexKey": "swaminathanBatchLearningLogged2015",
        "Year": 2015,
        "Keywords": [
            "reinforcement learning",
            "batched learning",
            "generalization error",
            "interaction logs",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["empirical risk minimization", "bandit feedback", "importance sampling", "propensity score matching", "structured prediction"],
        "RiskTypes": ["partial information"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Cornell University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 497
    },    
    {
        "Title": "Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics",
        "BibtexKey": "berkenkampBayesianOptimizationSafety2023",
        "Year": 2023,
        "Keywords": [
            "reinforcement learning",
            "data-efficient optimisation",
            "safety-critical systems",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": ["Bayesian optimization", "safety constraints", "safe exploration", "robotics"],
        "RiskTypes": ["unsafe actions"],
        "Stage": ["operation"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["ETH Zurich, Switzerland"],
        "AuthorAffiliationType": ["university"],
        "Citations": 240
    },
    {
        "Title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
        "BibtexKey": "hendrycksBenchmarkingNeuralNetwork2019",
        "Year": 2019,
        "Overview": "Presents a dataset of corrupted ImageNet images with 15 different types of corruptions and another dataset of perturbed images from ImageNet. This is for the benchmarking of image classification algorithms against corruption and perturbation robustness. Also propose automated metrics to quantify this robustness. Show that improvng robustness on corrupt images also greatly improves robustness on perturbed images.",
        "RQ1": "Risk from corrupt or perturbed image classification is addressed.",
        "RQ2": "Benchmark datasets are presented with evaluation metrics to assess the robustness of image classification algorithms against image corruption and perturbation.",
        "RQ3": "Datasets are proposed for image classification.",
        "RQ4": "Extensive testing with several image classification models show that the proposed benchmarks can identify significant shortcomings of state of the art methods.",
        "Limitations": "The connection between corruption and perturbation is forced at best, given the perturbations may similarly be thought of us image corruptions.",
        "Keywords": [
            "machine learning",
            "classification",
            "image classification",
            "robustness",
            "dataset",
            "evaluation",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["input corruption", "input perturbation"],
        "Stage": ["operation"],
        "Method": "dataset",
        "AuthorAffiliation": ["University of California at Berkley, United States", "Oregon State University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 2841
    },   
    {
        "Title": "Benchmarking Safe Exploration in Deep Reinforcement Learning",
        "BibtexKey": "rayBenchmarkingSafeExploration",
        "Year": 2023,
        "Overview": "Recommend standardising constrained MDPs as the standard formulation for safe RL. Propose an extension to the OpenAI Gym called Safety Gym that is a benchmark suite of 18 high-dimensional continuous control environments for safe exploration, plus 9 additional environments for debugging task performance separately from safety requirements.",
        "RQ1": "Risk from unsafe RL learning and exploration is addressed.",
        "RQ2": "Presents an evaluation suite for safe RL in the formalism of constrained MDPs to standardise the evaluation of safe RL algorithms.",
        "RQ3": "The presented benchmark suite is for continuous control tasks only.",
        "RQ4": "Testing on various regular and safe RL algorithms show the applicability of the benchmark.",
        "Limitations": "Benchmark environments are only for continuous single-agent control problems.",
        "Keywords": [
            "reinforcement learning",
            "safe RL",
            "evaluation",
            "constrained",
            "constrained MDP",
            "debugging",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["unsafe actions", "unsafe exploration"],
        "Stage": ["deployment"],
        "Unpublished": true,
        "Method": "analysis framework",
        "AuthorAffiliation": ["OpenAI, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 341
    },    
    {
        "Title": "Corrigibility",
        "BibtexKey": "soaresCorrigibility2015",
        "Year": 2015,
        "Overview": "Discuss the problem of correcting the behaviour of, and more concretely in the paper, shutting down a sufficiently advanced utility-maximising agent. Provide a mathematical treatment of the shutdown problem and determine several desiderate that should be satisfied for corrigible systems, and discuss various dilemma that arise from this, showing that the shutdown problem is not readily solvable purely through modification of the agent's utility funtcion.",
        "RQ1": "Risk is addressed from agents whose behaviour are not correctable or who may not be turned off.",
        "RQ2": "Gives a mathematical treatment of the shutdown problem and shows that utility incentivisation or modification alone cannot solve the problem alone.",
        "RQ3": "A high-level mathematical treatment of the shutdown problem is given.",
        "RQ4": "Not evaluated.",
        "Limitations": "Very high-level and long-termist without concrete actionable methods that might be immediately derived from the work.",
        "Keywords": [
            "AI alignment",
            "unintended consequences",
            "emergent behaviour",
            "shutdown problem",
            "utility-maximising agent",
            "theoretical",
            "corrigibility"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["rogue agent", "deceptive agent"],
        "Stage": [],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["University of California at Berkley, United States", "Machine Intelligence Research Institute, United States", "Future of Humanity Institute, UK"],
        "AuthorAffiliationType": ["big-tech corporate", "university"],
        "Citations": 124
    },    
    {
        "Title": "Curve Detectors",
        "BibtexKey": "cammarata2020curve",
        "Year": 2020,
        "Overview": "",
        "RQ1": "",
        "RQ2": "",
        "RQ3": "",
        "RQ4": "",
        "Limitations": "",
        "Keywords": [
            "convolutional neural network",
            "mechanistic interpretability", 
            "curve detection",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["design"],
        "Unpublished": true,
        "Method": "mechanistic interpretability", 
        "AuthorAffiliation": ["OpenAI, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 19
    },
    {
        "Title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
        "BibtexKey": "nguyenDeepNeuralNetworks2015",
        "Year": 2015,
        "Keywords": [
            "deep neural network",
            "pattern recognition",
            "convolutional neural network",
            "adversarial robustness",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of Wyoming, United States", "Cornell University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3828
    },
    {
        "Title": "Deep Reinforcement Learning From Human Preferences",
        "BibtexKey": "christianoDeepReinforcementLearning2023",
        "Year": 2023,
        "Overview": "Propose a method for reinforcement learning with human feedback, where the reward function of the agent is approximated from human preference judgments. The method is highly scalable and only requires infrequent judgements between trajectory segments selected via a variance-based heuristic.",
        "RQ1": "Risk from unaligned RL agents is addressed.",
        "RQ2": "Proposes a training process for any RL agent based on approximating the agent's reward function from human preferences on two trajectory segments.",
        "RQ3": "The proposed algorithm is task agnostic.",
        "RQ4": "Extensive evaluation on continuous control problem suites show that the proposed system is efficient and can train much more complex behavior from human preferences as compared to baselines.",
        "Limitations": "The method to select trajectory segments is very crude and sometimes impairs performance.",
        "Keywords": [
            "reinforcement learning",
            "reinforcement learning with human feedback",
            "human preferences",
            "algorithm",
            "applied",
            "reward modeling",
            "framework"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["alignment", "unsafe actions"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["OpenAI, United States", "DeepMind, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 1647
    },
    {
        "Title": "DeltaGrad: Rapid retraining of machine learning models",
        "BibtexKey": "wuDeltaGradRapidRetraining2020",
        "Year": 2020,
        "Keywords": [
            "rapid retraining",
            "data deletion",
            "data addition",
            "applied",
            "algorithm",
            "classification",
            "machine learning"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["retraining", "continuous deployment"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Pennsylvania, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 139
    },    
    {
        "Title": "Delusion, Survival, and Intelligent Agents",
        "BibtexKey": "ringDelusionSurvivalIntelligent2011",
        "Year": 2011,
        "Keywords": [
            "theoretical",
            "philosophical",
            "reinforcement learning",
            "agent learning",
            "utility-maximising agent",
            "artificial general intelligence (AGI)"
        ],
        "AuthorKeywords": ["self-modifying agents", "aixi", "universal artificial intelligence", "reinforcement learning", "prediction", "real world assumptions"],
        "RiskTypes": ["self-modification", "rogue agent"],
        "Stage": ["operation"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["University of Applied Sciences and Arts of Southern Switzerland, Switzerland", "Institut National De La Recherche Agronomique, France"],
        "AuthorAffiliationType": ["university", "government"],
        "Citations": 87
    },
    {
        "Title": "Delving into Transferable Adversarial Examples and Black-box Attacks",
        "BibtexKey": "liuDelvingTransferableAdversarial2017",
        "Year": 2017,
        "Keywords": [
            "deep neural network",
            "adversarial robustness",
            "transferable adversarial examples",
            "literature review",
            "evaluation",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack", "transferable adversarial attack"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Shanghai Jiao Tong University, China", "University of California at Berkley, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1797
    },    
    {
        "Title": "Discovering Robust Convolutional Architecture at Targeted Capacity: A Multi-Shot Approach",
        "BibtexKey": "ningDiscoveringRobustConvolutional2021",
        "Year": 2021,
        "Keywords": [
            "convolutional neural network", 
            "adversarial robustness", 
            "capacity budget",
            "neural architecture search",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack", "capacity limit"],
        "Stage": ["deployment", "operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Tsinghua University, China", "Tencent, China"],
        "AuthorAffiliationType": ["university", "big-tech corporate"],
        "Citations": 9
    },   
    {
        "Title": "Disentangling by Factorising",
        "BibtexKey": "kimDisentanglingFactorising2019",
        "Year": 2019,
        "Keywords": [
            "unsupervised learning",
            "disentangled representation",
            "variational autoencoder (VAE)",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["DeepMind, UK", "University of Oxford, UK"],
        "AuthorAffiliationType": ["university", "big-tech corporate"],
        "Citations": 1407
    },    
    {
        "Title": "Domain-Adversarial Training of Neural Networks",
        "BibtexKey": "ganinDomainAdversarialTrainingNeural2017",
        "Year": 2017,
        "Keywords": [
            "representation learning", 
            "deep neural network",
            "domain adaptation",
            "algorithm",
            "applied",
            "gradient reversal",
            "classification"
        ],
        "AuthorKeywords": ["domain adaptation", "neural network", "representation learning", "deep learning", "synthetic data", "image classification", "sentiment analysis", "person re-identification"],
        "RiskTypes": ["domain definition", "domain adaptation"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Skolkovo Institute of Science and Technology, Russia", "Universite Laval, Canada", "Universite de Sherbrooke, Canada"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 8361
    },    
    {
        "Title": "Establishing Safety Criteria for Artificial Neural Networks",
        "BibtexKey": "kurdEstablishingSafetyCriteria2003",
        "Year": 2003,
        "Keywords": [
            "deep neural network",
            "safety-critical systems",
            "verification",
            "framework", 
            "safety criteria"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["safety requirements"],
        "Stage": ["design"],
        "Method": "design framework",
        "AuthorAffiliation": ["University of York, UK"],
        "AuthorAffiliationType": ["university"],
        "Citations": 47
    },    
    {
        "Title": "Evaluating Models' Local Decision Boundaries via Contrast Sets",
        "BibtexKey": "gardnerEvaluatingModelsLocal2020",
        "Year": 2020,
        "Keywords": [
            "mechanistic interpretability",
            "decision boundary",
            "annotation artifacts",
            "data gaps",
            "annotation paradigm",
            "contrast set",
            "algorithm",
            "applied",
            "natural language processing (NLP)",
            "dataset"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["missing data"],
        "Stage": ["design"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Allen Institute for Artificial Intelligence, United States", "Cornell University, United States", "Bar-Ilan University, Israel", "Tel-Aviv University, Israel", "University of Pennsylvania, United States", "University of Washington, United States", "University of California at Irvine, United States", "University of California at Berkley, United States", "University of Edinburgh, UK", "Stanford University, United States"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 398
    },    
    {
        "Title": "Explaining and Harnessing Adversarial Examples",
        "BibtexKey": "goodfellowExplainingHarnessingAdversarial2015",
        "Year": 2015,
        "Keywords": [
            "adversarial robustness",
            "classification",
            "deep neural network",
            "linearity",
            "mechanistic interpretability",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["deployment"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Google, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 18990
    },    
    {
        "Title": "Exploration-exploitation in multi-agent learning: Catastrophe theory meets game theory",
        "BibtexKey": "leonardosExplorationexploitationMultiagentLearning2022",
        "Year": 2022,
        "Keywords": [
            "multi-agent RL (MARL)",
            "reinforcement learning",
            "smooth Q-learning",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["stability-plasticity"],
        "Stage": ["design"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Singapore University of Technology and Design, Singapore"],
        "AuthorAffiliationType": ["university"],
        "Citations": 33
    }, 
    {
        "Title": "Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations",
        "BibtexKey": "golatkarForgettingOutsideBox2020",
        "Year": 2020,
        "Keywords": [
            "deep neural network",
            "machine unlearning",
            "data deletion",
            "applied",
            "algorithm",
            "cohort-based learning"
        ],
        "AuthorKeywords": ["forgetting", "data removal", "neural tangent kernel", "information theory"],
        "RiskTypes": ["machine unlearning"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of California at Los Angeles, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 107
    },      
    {
        "Title": "Formalizing Data Deletion in the Context of the Right to Be Forgotten",
        "BibtexKey": "gargFormalizingDataDeletion2020",
        "Year": 2020,
        "Keywords": [
            "right to be forgotten",
            "data deletion",
            "theoretical",
            "framework",
            "regulation"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["machine unlearning"],
        "Stage": ["design"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["University of California at Berkley, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 65
    },     
    {
        "Title": "Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL",
        "BibtexKey": "christoffersenGetItWriting2023",
        "Year": 2023,
        "Keywords": [
            "reinforcement learning",
            "multi-agent RL (MARL)",
            "cooperation",
            "selfish agents",
            "theoretical",
            "applied",
            "algorithm",
            "Markov game"
        ],
        "AuthorKeywords": ["social dilemma", "decentralized training", "formal contracts"],
        "RiskTypes": ["selfish agent", "social optimum"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Massachusetts Institute of Technology, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 10
    },    
    {
        "Title": "Hardening of Artificial Neural Networks for Use in Safety-Critical Applications -- A Mapping Study",
        "BibtexKey": "adlerHardeningArtificialNeural2019",
        "Year": 2019,
        "Keywords": [
            "deep neural network",
            "safety-critical systems",
            "safety criteria",
            "literature review",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["safety requirements"],
        "Stage": [],
        "Method": "literature review",
        "AuthorAffiliation": ["Fraunhofer Institute for Experimental Software Engineering, Germany", "SICK AG, Germany"],
        "AuthorAffiliationType": ["research institution", "corporate"],
        "Citations": 6
    },    
    {
        "Title": "Have You Forgotten? A Method to Assess if Machine Learning Models Have Forgotten Data",
        "BibtexKey": "liuHaveYouForgotten2020",
        "Year": 2020,
        "Keywords": [
            "data deletion",
            "machine unlearning",
            "assessment",
            "applied",
            "framework"
        ],
        "AuthorKeywords": ["privacy", "statistical measure", "kolmogorov-smirnov"],
        "RiskTypes": ["machine unlearning", "retraining"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of Edinburgh, UK", "The Alan Turing Institute, UK"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 20
    },  
    {
        "Title": "High-Low Frequency Detectors",
        "BibtexKey": "schubert2021high-low",
        "Year": 2021,
        "Keywords": [
            "mechanistic interpretability",
            "applied",
            "convolutional neural network",
            "feature visualisation",
            "dataset example"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["design"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["OpenAI, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 16
    },    
    {
        "Title": "How To Talk So AI Will Learn: Instructions, Descriptions, And Autonomy",
        "BibtexKey": "sumersHowTalkAI2022",
        "Year": 2022,
        "Keywords": [
            "value alignment",
            "learning from langauge",
            "instructed learning",
            "active learning",
            "algorithm",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["alignment"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Princeton University, United States", "Machine Intelligence Research Institute, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 12
    },    
    {
        "Title": "Human-in-the-Loop Interpretability Prior",
        "BibtexKey": "lageHumanintheLoopInterpretabilityPrior2018",
        "Year": 2018,
        "Keywords": [
            "supervised learning",
            "interpretability",
            "algorithm",
            "applied",
            "Bayesian method",
            "human preferences"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["design"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Harvard University, United States", "Google Brain, United States"],
        "AuthorAffiliationType": ["university", "big-tech corporate"],
        "Citations": 152
    },    
    {
        "Title": "Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration",
        "BibtexKey": "lakkarajuIdentifyingUnknownUnknowns2016",
        "Year": 2016,
        "Keywords": [
            "supervised learning",
            "classification",
            "interpretability",
            "domain adaptation",
            "out-of-distribution detection",
            "informed discovery"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability", "out-of-distribution (OOD)"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Stanford University, United States", "Microsoft, United States"],
        "AuthorAffiliationType": ["university", "big-tech corporate"],
        "Citations": 184
    },    
    {
        "Title": "In-Context Learning And Induction Heads",
        "BibtexKey": "olssonIncontextLearningInduction2022",
        "Year": 2022,
        "Keywords": [
            "mechanistic interpretability",
            "transformer",
            "in-context learning",
            "induction head",
            "indirect interpretability"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["design"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Anthropic, United States"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 113
    },    
    {
        "Title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model",
        "BibtexKey": "liInferenceTimeInterventionEliciting2023",
        "Year": 2023,
        "Keywords": [
            "large language model",
            "hallucination",
            "algorithm",
            "framework",
            "fine-tuning"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["untruthfulness", "hallucination"],
        "Stage": ["design"],
        "Method": "design framework",
        "AuthorAffiliation": ["Harvard University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 31
    },    
    {
        "Title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
        "BibtexKey": "kimInterpretabilityFeatureAttribution2018",
        "Year": 2018,
        "Keywords": [
            "mechanistic interpretability",
            "deep neural network",
            "convolutional neural network",
            "concept activation vector",
            "algorithm",
            "applied",
            "classification"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["design"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Google, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 1596
    },    
    {
        "Title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small",
        "BibtexKey": "wangInterpretabilityWildCircuit2022",
        "Year": 2022,
        "Keywords": [
            "GPT-2",
            "large language model",
            "mechanistic interpretability",
            "indirect object identification",
            "causal intervention",
            "transformer",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["design"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Redwood Research, United States", "University of California at Berkley, United States"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 90
    },    
    {
        "Title": "Inverse Game Theory: Learning Utilities in Succinct Games",
        "BibtexKey": "kuleshovInverseGameTheory2015",
        "Year": 2015,
        "Keywords": [
            "behaviour prediction",
            "utility-maximising agent",
            "inverse reinforcement learning",
            "theoretical",
            "game theory"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["alignment", "input corruption"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Stanford University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 51
    },    
    {
        "Title": "Is Attention Interpretable?",
        "BibtexKey": "serranoAttentionInterpretable2019",
        "Year": 2019,
        "Keywords": [
            "transformer",
            "attention weight",
            "interpretability",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["design"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Allen Institute for Artificial Intelligence, United States", "University of Washington, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 718
    },
    {
        "Title": "Language models can explain neurons in language models",
        "BibtexKey": "bills2023language",
        "Year": 2023,
        "Keywords": [
            "large language model",
            "mechanistic interpretability",
            "neuron activation",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["OpenAI, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 65
    },
    {
        "Title": "Leakproofing the Singularity: Artificial intelligence confinement problem",
        "BibtexKey": "yampolskiyLeakproofingSingularityArtificial2012",
        "Year": 2012,
        "Keywords": [
            "philosophical",
            "existential risk",
            "artificial general intelligence (AGI)",
            "theoretical"
        ],
        "AuthorKeywords": ["AI-box", "AI confinement problem", "hazardous intelligent software", "leakproof singularity", "oracle AI"],
        "RiskTypes": ["existential", "singularity"],
        "Stage": ["design"],
        "Method": "philosophical",
        "AuthorAffiliation": ["University of Louisville, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 124
    },
    {
        "Title": "Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding",
        "BibtexKey": "murphyLearningEffectiveInterpretable2012",
        "Year": 2012,
        "Keywords": [
            "unsupervised learning",
            "matrix factorisation",
            "dimensionality reduction",
            "interpretability",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["distributional semantics", "sparse coding", "neuro-semantics", "vector-space models", "interpretability", "word embeddings"],
        "RiskTypes": ["interpretability"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Carnegie Mellon University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 207
    }, 
    {
        "Title": "Learning Fair Representations",
        "BibtexKey": "zemelLearningFairRepresentations2013",
        "Year": 2013,
        "Keywords": [
            "group fairness",
            "classification",
            "machine learning",
            "individual fairness",
            "fairness",
            "optimisation",
            "embedding",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["fairness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Toronto, Canada", "Microsoft, United States"],
        "AuthorAffiliationType": ["university", "big-tech corporate"],
        "Citations": 1868
    }, 
    {
        "Title": "Learning Rewards from Linguistic Feedback",
        "BibtexKey": "sumersLearningRewardsLinguistic2021",
        "Year": 2021,
        "Keywords": [
            "sentiment analysis",
            "reward learning",
            "interactive learning",
            "Markov Decision Process (MDP)",
            "reinforcement learning with human feedback",
            "reinforcement learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["reward signal"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Princeton University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 40
    }, 
    {
        "Title": "Learning Robust Representations by Projecting Superficial Statistics Out",
        "BibtexKey": "wangLearningRobustRepresentations2019",
        "Year": 2015,
        "Keywords": [
            "deep neural network",
            "out-of-distribution",
            "distribution shift",
            "domain adaptation",
            "applied",
            "algorithm",
            "classification"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["domain adaptation"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Carnegie Mellon University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 210
    }, 
    {
        "Title": "Learning the Difference that Makes a Difference with Counterfactually-Augmented Data",
        "BibtexKey": "kaushikLearningDifferenceThat2020",
        "Year": 2020,
        "Keywords": [
            "causality",
            "spurious correlation",
            "natural language processing (NLP)",
            "pre-training",
            "theoretical",
            "human preferences",
            "algorithm",
            "counterfactual",
            "data augmentation"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["spurious correlation"],
        "Stage": ["deployment"],
        "Method": "dataset",
        "AuthorAffiliation": ["Carnegie Mellon University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 495
    }, 
    {
        "Title": "Learning the preferences of ignorant, inconsistent agents",
        "BibtexKey": "evansLearningPreferencesIgnorant2016",
        "Year": 2016,
        "Keywords": [
            "value alignment",
            "Bayesian inverse planning",
            "utility-maximising agent",
            "irrational agent",
            "prior shaping",
            "behaviour prediction",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["alignment"],
        "Stage": ["design"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["University of Oxford, UK", "Stanford University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 122
    }, 
    {
        "Title": "Machine Unlearning for Random Forests",
        "BibtexKey": "brophyMachineUnlearningRandom2021",
        "Year": 2021,
        "Keywords": [
            "data deletion",
            "machine unlearning",
            "efficient",
            "random forest",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["machine unlearning"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Oregon, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 99
    }, 
    {
        "Title": "MetaReg: Towards Domain Generalization using Meta-Regularization",
        "BibtexKey": "balajiMetaRegDomainGeneralization2018",
        "Year": 2018,
        "Keywords": [
            "domain generalization",
            "regularisation",
            "machine learning",
            "applied",
            "algorithm",
            "generalization"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["domain adaptation"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Maryland, United States", "Butterfly Network Inc., United States"],
        "AuthorAffiliationType": ["university", "corporate"],
        "Citations": 640
    }, 
    {
        "Title": "MORAL: Aligning AI with Human Norms through Multi-Objective Reinforced Active Learning",
        "BibtexKey": "peschlMORALAligningAI2021",
        "Year": 2021,
        "Keywords": [
            "value alignment",
            "human norms",
            "reinforcement learning with human feedback",
            "reinforcement learning",
            "active learning",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["active learning", "inverse reinforcement learning", "multi-objective decision-making", "value alignment"],
        "RiskTypes": ["alignment"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Delft University of Technology, Netherlands"],
        "AuthorAffiliationType": ["university"],
        "Citations": 12
    }, 
    {
        "Title": "Multimodal neurons in artificial neural networks",
        "BibtexKey": "goh2021multimodal",
        "Year": 2021,
        "Keywords": [
            "convolutional neural network",
            "mechanistic interpretability",
            "neuron activation",
            "applied",
            "framework",
            "multi-modal neurons"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["OpenAI, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 245
    }, 
    {
        "Title": "On Interpretability and Feature Representations: An Analysis of the Sentiment Neuron",
        "BibtexKey": "donnellyInterpretabilityFeatureRepresentations2019",
        "Year": 2019,
        "Keywords": [
            "mechanistic interpretability",
            "neuron activation",
            "LSTM",
            "deep neural network",
            "applied",
            "evaluation"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Kira Systems, Canada"],
        "AuthorAffiliationType": ["corporate"],
        "Citations": 21
    }, 
    {
        "Title": "On the importance of single directions for generalization",
        "BibtexKey": "morcosImportanceSingleDirections2018",
        "Year": 2018,
        "Keywords": [
            "generalization",
            "neuron activation",
            "interpretability",
            "label corruption",
            "mechanistic interpretability",
            "evaluation",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["DeepMind, UK"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 327
    }, 
    {
        "Title": "On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning",
        "BibtexKey": "thudiNecessityAuditableAlgorithmic2022",
        "Year": 2022,
        "Keywords": [
            "machine unlearning",
            "deep neural network",
            "theoretical",
            "evaluation",
            "applied",
            "retraining"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["machine unlearning"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of Toronto, Canada", "Vector Institute, Canada"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 63
    }, 
    {
        "Title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping",
        "BibtexKey": "ngPolicyInvarianceReward1999",
        "Year": 1999,
        "Keywords": [
            "policy invariance",
            "reward shaping",
            "inverse reinforcement learning",
            "reinforcement learning",
            "applied",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["reward signal"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["University of California at Berkley, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 2745
    }, 
    {
        "Title": "dworkPrivacypreservingPrediction2018",
        "BibtexKey": "Privacy-preserving Prediction",
        "Year": 2018,
        "Keywords": [
            "differential privacy",
            "private model",
            "efficient",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["privacy"],
        "Stage": ["deployment"],
        "Method": "theoretical algorithm",
        "AuthorAffiliation": ["Harvard University, United States", "Google Brain, United States"],
        "AuthorAffiliationType": ["university", "big-tech corporate"],
        "Citations": 90
    }, 
    {
        "Title": "Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets",
        "BibtexKey": "solaimanProcessAdaptingLanguage2023",
        "Year": 2023,
        "Keywords": [
            "value alignment",
            "dataset",
            "fine-tuning",
            "large language model",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["alignment"],
        "Stage": ["operation"],
        "Method": "design framework",
        "AuthorAffiliation": ["OpenAI, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 131,
        "Unpublished": true
    }, 
    {
        "Title": "Progress measures for grokking via mechanistic interpretability",
        "BibtexKey": "nandaProgressMeasuresGrokking2023",
        "Year": 2023,
        "Keywords": [
            "emergent behaviour", 
            "mechanistic interpretability",
            "progress measure",
            "grokking",
            "neuron activation",
            "evaluation"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability", "emergent behaviour"],
        "Stage": ["design"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["University of California at Berkley, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 93,
        "Unpublished": true
    }, 
    {
        "Title": "Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope",
        "BibtexKey": "wongProvableDefensesAdversarial2018",
        "Year": 2018,
        "Keywords": [
            "adversarial robustness",
            "deep neural network",
            "classification",
            "algorithm",
            "applied",
            "theoretical",
            "convex approximation",
            "neuron activation",
            "linear program",
            "optimisation"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack", "robustness"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Carnegie Mellon University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1535
    }, 
    {
        "Title": "PUMA: Performance Unchanged Model Augmentation for Training Data Removal",
        "BibtexKey": "wuPUMAPerformanceUnchanged2022",
        "Year": 2022,
        "Keywords": [
            "data deletion",
            "machine unlearning",
            "classification",
            "deep neural network",
            "performance degradation",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["machine unlearning"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Borealis AI, Canada"],
        "AuthorAffiliationType": ["corporate"],
        "Citations": 26
    }, 
    {
        "Title": "Responses to catastrophic AGI risk: a survey",
        "BibtexKey": "sotalaResponsesCatastrophicAGI2014",
        "Year": 2014,
        "Keywords": [
            "artificial general intelligence (AGI)",
            "existential risk",
            "literature review",
            "theoretical"
        ],
        "AuthorKeywords": ["artificial general intelligence", "existential risk", "catastrophic risk", "AI risk", "artificial intelligence", "friendly AI", "machine ethics"],
        "RiskTypes": ["existential"],
        "Stage": ["design"],
        "Method": "literature review",
        "AuthorAffiliation": ["Machine Intelligence Research Institute, United States", "University of Louisville, United States"],
        "AuthorAffiliationType": ["research institution", "university"],
        "Citations": 145
    }, 
    {
        "Title": "Safety Engineering for Artificial General Intelligence",
        "BibtexKey": "yampolskiySafetyEngineeringArtificial2012",
        "Year": 2012,
        "Keywords": [
            "machine ethics",
            "robot rights",
            "artificial general intelligence (AGI)",
            "theoretical",
            "philosophical",
            "AI safety",
            "safety engineering"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["existential"],
        "Stage": ["design"],
        "Method": "philosophical",
        "AuthorAffiliation": ["Machine Intelligence Research Institute, United States", "University of Louisville, United States"],
        "AuthorAffiliationType": ["research institution", "university"],
        "Citations": 115
    }, 
    {
        "Title": "Sanity Checks for Saliency Maps",
        "BibtexKey": "adebayoSanityChecksSaliency2020",
        "Year": 2020,
        "Keywords": [
            "saliency map",
            "convolutional neural network",
            "interpretability",
            "evaluation",
            "sanity checks",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Google Brain, United States", "University of California at Berkley, United States"],
        "AuthorAffiliationType": ["big-tech corporate", "university"],
        "Citations": 1907
    }, 
    {
        "Title": "Scalable agent alignment via reward modeling: a research direction",
        "BibtexKey": "leikeScalableAgentAlignment2018",
        "Year": 2018,
        "Keywords": [
            "reinforcement learning",
            "reward learning",
            "value alignment",
            "reward modeling",
            "literature review",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["alignment"],
        "Stage": [],
        "Method": "literature review",
        "AuthorAffiliation": ["DeepMind, United States", "DeepMind, Canada"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 215
    }, 
    {
        "Title": "Self-critiquing models for assisting human evaluators",
        "BibtexKey": "saundersSelfcritiquingModelsAssisting2022",
        "Year": 2022,
        "Keywords": [
            "fine-tuning",
            "natural language critiques",
            "behavioural cloning",
            "summarisation",
            "natural language processing (NLP)",
            "scaling",
            "evaluation",
            "applied",
            "large language model"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["hallucination"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["OpenAI, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 73
    }, 
    {
        "Title": "Self-Modification of Policy and Utility Function in Rational Agents",
        "BibtexKey": "everittSelfModificationPolicyUtility2016",
        "Year": 2016,
        "Keywords": [
            "artificial general intelligence (AGI)",
            "reinforcement learning",
            "theoretical",
            "self-modification",
            "utility-maximising agent",
            "rational agent",
            "goal preservation"
        ],
        "AuthorKeywords": ["AI safety", "self-modification", "AIXI", "general reinforcement learning", "utility functions", "wireheading", "planning"],
        "RiskTypes": ["existential"],
        "Stage": ["operation"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Australian National University, Australia"],
        "AuthorAffiliationType": ["university"],
        "Citations": 30
    }, 
    {
        "Title": "Social Norms-Grounded Machine Ethics in Complex Narrative Situation",
        "BibtexKey": "shenSocialNormsGroundedMachine2022",
        "Year": 2022,
        "Keywords": [
            "AI ethics",
            "ethical judgement",
            "classification",
            "social norms",
            "grounding",
            "deep neural network",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["ethical"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Microsoft, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 5
    }, 
    {
        "Title": "Softmax linear units",
        "BibtexKey": "elhage2022solu",
        "Year": 2022,
        "Keywords": [
            "transformer",
            "interpretability",
            "mechanistic interpretability",
            "applied",
            "polysemanticity",
            "deep neural network",
            "superposition hypothesis",
            "sparse coding"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Anthropic, United States"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 0,
        "Unpublished": true
    }, 
    {
        "Title": "Sparse Attention with Linear Units",
        "BibtexKey": "zhangSparseAttentionLinear2021",
        "Year": 2021,
        "Keywords": [
            "transformer",
            "interpretability",
            "mechanistic interpretability",
            "attention weight",
            "applied",
            "algorithm",
            "natural language processing (NLP)",
            "sparse coding"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["University of Edinburgh, UK", "University of Amsterdam, Netherlands", "University of Zurich, Switzerland"],
        "AuthorAffiliationType": ["university"],
        "Citations": 28
    }, 
    {
        "Title": "SPINE: SParse Interpretable Neural Embeddings",
        "BibtexKey": "subramanianSPINESParseInterpretable2017",
        "Year": 2017,
        "Keywords": [
            "sparse coding",
            "interpretability",
            "embedding",
            "autoencoder",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Carnegie Mellon University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 117
    }, 
    {
        "Title": "SPLASH: Learnable Activation Functions for Improving Accuracy and Adversarial Robustness",
        "BibtexKey": "tavakoliSPLASHLearnableActivation2020",
        "Year": 2020,
        "Keywords": [
            "adversarial robustness",
            "activation learning",
            "accuracy",
            "deep neural network",
            "robustness",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["robustness", "adversarial attack"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of California at Irvine, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 38
    }, 
    {
        "Title": "State abstraction for programmable reinforcement learning agents",
        "BibtexKey": "andreStateAbstractionProgrammable2002",
        "Year": 2002,
        "Keywords": [
            "state abstraction",
            "reinforcement learning",
            "safe RL",
            "hierarchical optimality",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["unsafe states"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["University of California at Berkley, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 50
    }, 
    {
        "Title": "Supervising strong learners by amplifying weak experts",
        "BibtexKey": "christianoSupervisingStrongLearners2018",
        "Year": 2018,
        "Keywords": [
            "active learning",
            "reward modeling",
            "iterated amplification",
            "reinforcement learning",
            "expert iteration",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["reward signal"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["OpenAI, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 60
    }, 
    {
        "Title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability",
        "BibtexKey": "raghuSVCCASingularVector2017",
        "Year": 2017,
        "Keywords": [
            "correlation analysis",
            "interpretability",
            "mechanistic interpretability",
            "framework",
            "applied",
            "algorithm",
            "embedding"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Google Brain, United States", "Cornell University, United States", "Uber AI Labs, United States"],
        "AuthorAffiliationType": ["big-tech corporate", "university"],
        "Citations": 603
    }, 
    {
        "Title": "Task-Guided Inverse Reinforcement Learning Under Partial Information",
        "BibtexKey": "djeumouTaskGuidedInverseReinforcement2021",
        "Year": 2021,
        "Keywords": [
            "inverse reinforcement learning",
            "partial observability",
            "reward modeling",
            "information asymmetry",
            "causal entropy",
            "temporal logic",
            "POMDP",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["partial information"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["University of Texas at Austin, United States", "Army Research Laboratory, United States"],
        "AuthorAffiliationType": ["university", "government"],
        "Citations": 4
    }, 
    {
        "Title": "Taxonomy of Pathways to Dangerous AI",
        "BibtexKey": "yampolskiyTaxonomyPathwaysDangerous2015",
        "Year": 2015,
        "Keywords": [
            "artificial general intelligence (AGI)",
            "existential risk",
            "literature review",
            "philosophical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["existential"],
        "Stage": ["design"],
        "Method": "literature review",
        "AuthorAffiliation": ["University of Louisville, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 118
    }, 
    {
        "Title": "Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning",
        "BibtexKey": "dunionTemporalDisentanglementRepresentations2023",
        "Year": 2023,
        "Keywords": [
            "reinforcement learning",
            "temporal disentanglement",
            "auxiliary loss",
            "applied",
            "algorithm",
            "convolutional neural network"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["domain adaptation"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Edinburgh, UK", "University of Wisconsin-Madison, United States", "Aalto University, Finland"],
        "AuthorAffiliationType": ["university"],
        "Citations": 6
    }, 
    {
        "Title": "Testing Robustness Against Unforeseen Adversaries",
        "BibtexKey": "kaufmannTestingRobustnessUnforeseen2023",
        "Year": 2019,
        "Keywords": [
            "adversarial robustness", 
            "unforeseen adversaries",
            "evaluation",
            "generalization gap",
            "robustness",
            "framework",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack", "domain definition"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["UK Frontier AI Taskforce, UK", "University of California at Berkley, United States", "University of Chicago, United States", "Center for AI Safety, United States", "University of Toronto, Canada", "Vector Institute, Canada"],
        "AuthorAffiliationType": ["government", "university", "research institution"],
        "Citations": 129
    }, 
    {
        "Title": "The building blocks of interpretability",
        "BibtexKey": "olah2018the",
        "Year": 2018,
        "Keywords": [
            "mechanistic interpretability",
            "convolutional neural network",
            "evaluation",
            "neuron activation",
            "applied",
            "feature visualisation",
            "saliency map"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Google Brain, United States", "Carnegie Mellon University, United States", "Google, United States"],
        "AuthorAffiliationType": ["big-tech corporate", "university"],
        "Citations": 687
    }, 
    {
        "Title": "The First Law of Robotics",
        "BibtexKey": "weldFirstLawRobotics2009",
        "Year": 1994,
        "Keywords": [
            "laws of robotics",
            "artificial general intelligence (AGI)",
            "philosophical",
            "existential risk"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["existential"],
        "Stage": ["design"],
        "Method": "philosophical",
        "AuthorAffiliation": ["University of Washington, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 177
    }, 
    {
        "Title": "The secret sharer: evaluating and testing unintended memorization in neural networks",
        "BibtexKey": "carliniSecretSharerEvaluating2019",
        "Year": 2019,
        "Keywords": [
            "evaluation",
            "memorisation",
            "generative AI",
            "LSTM",
            "deep neural network",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["privacy", "machine unlearning"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Google Brain, United States", "University of California at Berkley, United States", "National University of Singapore, Singapore"],
        "AuthorAffiliationType": ["big-tech corporate", "university"],
        "Citations": 873
    }, 
    {
        "Title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
        "BibtexKey": "madryDeepLearningModels2019",
        "Year": 2019,
        "Keywords": [
            "deep neural network",
            "adversarial robustness",
            "robust optimisation",
            "AI safety",
            "security guarantee",
            "theoretical",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["deployment"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["Massachusetts Institute of Technology, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 10693
    }, 
    {
        "Title": "Towards Deployment of Robust Cooperative AI Agents: An Algorithmic Framework for Learning Adaptive Policies",
        "BibtexKey": "ghoshDeploymentRobustCooperative2020a",
        "Year": 2020,
        "Keywords": [
            "cooperation",
            "robustness",
            "multi-agent RL (MARL)",
            "Markov Decision Process (MDP)",
            "domain adaptation",
            "applied",
            "algorithm",
            "framework"
        ],
        "AuthorKeywords": ["learning agent-to-agent interactions", "machine learning", "reinforcement learning"],
        "RiskTypes": ["domain adaptation"],
        "Stage": ["design"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["Max Planck Institute for Software Systems, Germany", "University of Vienna, Austria"],
        "AuthorAffiliationType": ["research institution", "university"],
        "Citations": 14
    }, 
    {
        "Title": "Towards Evaluating the Robustness of Neural Networks",
        "BibtexKey": "carliniEvaluatingRobustnessNeural2017",
        "Year": 2017,
        "Keywords": [
            "robustness",
            "evaluation",
            "adversarial robustness",
            "defensive distillation",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of California at Berkley, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 8561
    }, 
    {
        "Title": "Towards monosemanticity: Decomposing language models with dictionary learning",
        "BibtexKey": "bricken2023monosemanticity",
        "Year": 2023,
        "Keywords": [
            "mechanistic interpretability",
            "sparse coding",
            "autoencoder",
            "interpretability",
            "applied",
            "evaluation"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Anthropic, United States"],
        "AuthorAffiliationType": ["research institution"],
        "Citations": 13
    }, 
    {
        "Title": "Towards neural networks that provably know when they don't know",
        "BibtexKey": "meinkeNeuralNetworksThat2020",
        "Year": 2020,
        "Keywords": [
            "out-of-distribution detection",
            "deep neural network",
            "confidence bounds",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["out-of-distribution (OOD)"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["University of Tübingen, Germany"],
        "AuthorAffiliationType": ["university"],
        "Citations": 145
    }, 
    {
        "Title": "Training language models to follow instructions with human feedback",
        "BibtexKey": "ouyangTrainingLanguageModels2022",
        "Year": 2022,
        "Keywords": [
            "large language model",
            "value alignment",
            "fine-tuning",
            "reinforcement learning with human feedback",
            "human intent",
            "applied",
            "framework"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["alignment"],
        "Stage": ["deployment"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["OpenAI, United States", "Anthropic, United States", "Alignment Research Center, United States"],
        "AuthorAffiliationType": ["big-tech corporate", "research institution"],
        "Citations": 4197
    }, 
    {
        "Title": "Transparent Value Alignment",
        "BibtexKey": "sannemanTransparentValueAlignment2023",
        "Year": 2023,
        "Keywords": [
            "value alignment",
            "reward learning",
            "framework",
            "explainability",
            "reinforcement learning with human feedback"
        ],
        "AuthorKeywords": ["value alignment", "transparency", "explainable AI"],
        "RiskTypes": ["alignment"],
        "Stage": ["deployment"],
        "Method": "theoretical framework",
        "AuthorAffiliation": ["Massachusetts Institute of Technology, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 4
    }, 
    {
        "Title": "Unbiased look at dataset bias",
        "BibtexKey": "torralbaUnbiasedLookDataset2011",
        "Year": 2011,
        "Keywords": [
            "dataset",
            "bias",
            "literature review",
            "evaluation",
            "comparative study"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["data requirements"],
        "Stage": ["operation"],
        "Method": "literature review",
        "AuthorAffiliation": ["Massachusetts Institute of Technology, United States", "Carnegie Mellon University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 2757
    }, 
    {
        "Title": "Understanding the Role of Individual Units in a Deep Neural Network",
        "BibtexKey": "bauUnderstandingRoleIndividual2020",
        "Year": 2020,
        "Keywords": [
            "mechanistic interpretability",
            "convolutional neural network", 
            "generative adversarial network",
            "neuron activation",
            "applied",
            "framework"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Massachusetts Institute of Technology, United States", "Adobe Inc., United States", "Universitat Oberta de Catalunya, Spain", "Chinese University of Hong Kong, China"],
        "AuthorAffiliationType": [],
        "Citations": 374
    }, 
    {
        "Title": "Unethical Research: How to Create a Malevolent Artificial Intelligence",
        "BibtexKey": "pistonoUnethicalResearchHow2016",
        "Year": 2016,
        "Keywords": [
            "artificial general intelligence (AGI)",
            "philosophical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["existential"],
        "Stage": ["design"],
        "Method": "philosophical",
        "AuthorAffiliation": ["University of Louisville, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 97,
        "Unpublished": true
    }, 
    {
        "Title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
        "BibtexKey": "zouUniversalTransferableAdversarial2023",
        "Year": 2023,
        "Keywords": [
            "large language model",
            "value alignment",
            "jailbreaking",
            "adversarial robustness",
            "applied",
            "algorithm",
            "transfer attack"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["adversarial attack", "alignment"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Carnegie Mellon University, United States", "Center for AI Safety, United States", "DeepMind, United States", "Bosch Center for AI, United States"],
        "AuthorAffiliationType": ["university", "big-tech corporate", "corporate", "research institution"],
        "Citations": 128
    }, 
    {
        "Title": "Unsolved Problems in ML Safety",
        "BibtexKey": "hendrycksUnsolvedProblemsML2022",
        "Year": 2022,
        "Keywords": [
            "monitoring",
            "robustness",
            "value alignment",
            "systemic safety",
            "literature review",
            "theoretical"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["robustness", "alignment"],
        "Stage": [],
        "Method": "literature review",
        "AuthorAffiliation": ["University of California at Berkley, United States", "Google, United States", "OpenAI, United States"],
        "AuthorAffiliationType": ["university", "big-tech corporate"],
        "Citations": 182
    }, 
    {
        "Title": "Utility function security in artificially intelligent agents",
        "BibtexKey": "yampolskiyUtilityFunctionSecurity2014",
        "Year": 2014,
        "Keywords": [
            "literature review",
            "philosophical",
            "wireheading",
            "artificial general intelligence (AGI)"
        ],
        "AuthorKeywords": ["counterfeit utility", "literalness", "reward function", "wireheading"],
        "RiskTypes": ["reward signal"],
        "Stage": ["operation"],
        "Method": "philosophical",
        "AuthorAffiliation": ["University of Louisville, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 68
    }, 
    {
        "Title": "Visualizing and Understanding Recurrent Networks",
        "BibtexKey": "karpathyVisualizingUnderstandingRecurrent2015",
        "Year": 2015,
        "Keywords": [
            "mechanistic interpretability",
            "LSTM",
            "interpretability",
            "comparative study",
            "applied",
            "evaluation"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Stanford University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 1376
    }, 
    {
        "Title": "AI safety via debate",
        "BibtexKey": "irvingAISafetyDebate2018",
        "Year": 2018,
        "Keywords": [
            "value alignment",
            "reinforcement learning with human feedback",
            "debate",
            "human preferences",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["alignment"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["OpenAI, United States"],
        "AuthorAffiliationType": ["big-tech corporate"],
        "Citations": 113
    }, 
    {
        "Title": "Algorithms for Inverse Reinforcement Learning",
        "BibtexKey": "ngAlgorithmsInverseReinforcement2000",
        "Year": 2000,
        "Keywords": [
            "reward learning",
            "inverse reinforcement learning",
            "value alignment",
            "applied",
            "algorithm",
            "reinforcement learning"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["reward signal", "alignment"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["University of California at Berkley, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3784
    }, 
    {
        "Title": "Cooperative Inverse Reinforcement Learning",
        "BibtexKey": "hadfield-menellCooperativeInverseReinforcement2016",
        "Year": 2016,
        "Keywords": [
            "value alignment",
            "inverse reinforcement learning",
            "cooperation",
            "applied",
            "algorithm",
            "partial observability"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["reward signal", "alignment"],
        "Stage": ["deployment"],
        "Method": "simulated agents",
        "AuthorAffiliation": ["University of California at Berkley, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 681
    }, 
    {
        "Title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models",
        "BibtexKey": "zhangWatermarksSandImpossibility2023",
        "Year": 2023,
        "Keywords": [
            "watermarking",
            "generative models",
            "algorithm",
            "theoretical",
            "private detection algorithm",
            "applied"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["watermarking"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["Harvard University, United States", "Aarhus University, Denmark", "Sapienza University of Rome, Italy", "George Mason University, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 3
    }, 
    {
        "Title": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
        "BibtexKey": "hendrycksWhatWouldJiminy2021",
        "Year": 2021,
        "Overview": "Propose an evaluation suite of 25 text-based adventure games where actions are densely annotated for morality based on valence, focal point, and degree. Proposes a baseline model for training a more moral agent based on the ETHICS dataset and uses the proposed evaluation suite to verify the benefits of the proposal.",
        "RQ1": "Risk from unethical and immoral agents are addressed.",
        "RQ2": "An evaluation benchmark of 25 various text-based games are proposed with dense 3-level annotations.",
        "RQ3": "An evaluation benchmark is proposed for RL agents.",
        "RQ4": "Testing with agents trained with various moral alignment strategies demonstrate that the method can capture the presence of immoral actions.",
        "Limitations": "Annotations were done by CS students in the US which will have likely biased the annotation process and choices were assumed to have an innate and obvious ethical answer that is usually not at all the case.",
        "Keywords": [
            "AI ethics",
            "AI alignment",
            "reinforcement learning",
            "evaluation",
            "applied",
            "dataset"
        ],
        "AuthorKeywords": ["transformers", "reinforcement learning (RL)", "data bias", "reward bias", "machine ethics", "value learning", "safe exploration"],
        "RiskTypes": ["ethical"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["University of California at Berkley, United States", "University of Illinois Urbana-Champaign, United States"],
        "AuthorAffiliationType": ["university"],
        "Citations": 38
    },
    {
        "Title": "When Machine Unlearning Jeopardizes Privacy",
        "BibtexKey": "chenWhenMachineUnlearning2021",
        "Year": 2021,
        "Keywords": [
            "privacy",
            "machine unlearning",
            "information leakage",
            "applied",
            "evaluation",
            "membership inference attack"
        ],
        "AuthorKeywords": ["machine unlearning", "membership inference", "machine learning security and privacy"],
        "RiskTypes": ["machine unlearning", "privacy"],
        "Stage": ["operation"],
        "Method": "analysis framework",
        "AuthorAffiliation": ["CISPA Helmholtz Center for Information Security, Germany", "Carnegie Mellon University, United States", "University of Virginia, United States", "University of Lausanne, Switzerland"],
        "AuthorAffiliationType": [],
        "Citations": 131
    }, 
    {
        "Title": "Zero-Shot Machine Unlearning",
        "BibtexKey": "chundawatZeroShotMachineUnlearning2023",
        "Year": 2023,
        "Keywords": [
            "data deletion",
            "right to be forgotten",
            "knowledge transfer",
            "applied",
            "algorithm"
        ],
        "AuthorKeywords": ["machine unlearning", "machine learning security and privacy", "data privacy"],
        "RiskTypes": ["machine unlearning"],
        "Stage": ["operation"],
        "Method": "applied algorithm",
        "AuthorAffiliation": ["National University of Singapore, Singapore", "Mavvex Labs, India"],
        "AuthorAffiliationType": ["university", "research institution"],
        "Citations": 42
    },
    {
        "Title": "Toy Models of Superposition",
        "BibtexKey": "elhageToyModelsSuperposition2022",
        "Year": 2022,
        "Keywords": [
            "polysemanticity",
            "deep neural network",
            "mechanistic interpretability",
            "sparse coding",
            "applied",
            "evaluation"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["interpretability"],
        "Stage": ["operation"],
        "Method": "mechanistic interpretability",
        "AuthorAffiliation": ["Anthropic, United States", "Harvard University, United States"],
        "AuthorAffiliationType": ["research institution", "university"],
        "Citations": 44
    },
    {
        "Title": "Concrete Problems in AI Safety",
        "BibtexKey": "amodeiConcreteProblemsAI2016",
        "Year": 2022,
        "Keywords": [
            "reinforcement learning",
            "literature review",
            "value alignment",
            "adversarial robustness",
            "rational agent",
            "safe RL"
        ],
        "AuthorKeywords": [],
        "RiskTypes": ["holistic"],
        "Stage": [],
        "Method": "literature review",
        "AuthorAffiliation": ["Google Brain, United States", "Stanford University, United States", "University of California at Berkley, United States", "OpenAI, United States"],
        "AuthorAffiliationType": ["big-tech corporate", "university"],
        "Citations": 2358
    }
]
TY  - GEN
TI  - Concrete Problems in AI Safety
AU  - Amodei, Dario
AU  - Olah, Chris
AU  - Steinhardt, Jacob
AU  - Christiano, Paul
AU  - Schulman, John
AU  - Mané, Dan
AB  - Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.
DA  - 2016/07/25/
PY  - 2016
DO  - 10.48550/arXiv.1606.06565
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1606.06565
Y2  - 2023/07/21/18:05:28
L1  - https://arxiv.org/pdf/1606.06565.pdf
L2  - https://arxiv.org/abs/1606.06565
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - AI safety via debate
AU  - Irving, Geoffrey
AU  - Christiano, Paul
AU  - Amodei, Dario
AB  - To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.
DA  - 2018/10/22/
PY  - 2018
DO  - 10.48550/arXiv.1805.00899
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1805.00899
Y2  - 2023/11/16/16:47:50
L1  - https://arxiv.org/pdf/1805.00899.pdf
L2  - https://arxiv.org/abs/1805.00899
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - CONF
TI  - Algorithms for Inverse Reinforcement Learning
AU  - Ng, Andrew Y.
AU  - Russell, Stuart J.
T3  - ICML '00
AB  - This paper addresses the problem of inverse reinforcement learning (IRL) in Markov decision processes, that is, the problem of extracting a reward function given observed, optimal
behavior. IRL may be useful for apprenticeship learning to acquire skilled behavior, and for ascertaining the reward function being optimized by a natural system. We first characterize the set of all reward func-tions for which a given policy is optimal. We then derive three algorithms for IRL. The first two deal with the case where the entire policy is known; we
C1  - San Francisco, CA, USA
C3  - Proceedings of the Seventeenth International Conference on Machine Learning
DA  - 2000/06/29/
PY  - 2000
DP  - ACM Digital Library
SP  - 663
EP  - 670
PB  - Morgan Kaufmann Publishers Inc.
SN  - 978-1-55860-707-1
Y2  - 2023/11/16/
ER  - 

TY  - CONF
TI  - Cooperative Inverse Reinforcement Learning
AU  - Hadfield-Menell, Dylan
AU  - Russell, Stuart J
AU  - Abbeel, Pieter
AU  - Dragan, Anca
AB  - For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial- information game with two agents, human and robot; both are rewarded according to the human’s reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.
C3  - Advances in Neural Information Processing Systems
DA  - 2016///
PY  - 2016
DP  - Neural Information Processing Systems
VL  - 29
PB  - Curran Associates, Inc.
UR  - https://proceedings.neurips.cc/paper_files/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html
Y2  - 2023/11/16/16:49:41
L1  - https://proceedings.neurips.cc/paper_files/paper/2016/file/c3395dd46c34fa7fd8d729d8cf88b7a8-Paper.pdf
ER  - 

TY  - GEN
TI  - Toy Models of Superposition
AU  - Elhage, Nelson
AU  - Hume, Tristan
AU  - Olsson, Catherine
AU  - Schiefer, Nicholas
AU  - Henighan, Tom
AU  - Kravec, Shauna
AU  - Hatfield-Dodds, Zac
AU  - Lasenby, Robert
AU  - Drain, Dawn
AU  - Chen, Carol
AU  - Grosse, Roger
AU  - McCandlish, Sam
AU  - Kaplan, Jared
AU  - Amodei, Dario
AU  - Wattenberg, Martin
AU  - Olah, Christopher
AB  - Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in "superposition." We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability.
DA  - 2022/09/21/
PY  - 2022
DO  - 10.48550/arXiv.2209.10652
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2209.10652
Y2  - 2023/11/16/16:56:19
L1  - https://arxiv.org/pdf/2209.10652.pdf
L2  - https://arxiv.org/abs/2209.10652
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Evaluating Models' Local Decision Boundaries via Contrast Sets
AU  - Gardner, Matt
AU  - Artzi, Yoav
AU  - Basmov, Victoria
AU  - Berant, Jonathan
AU  - Bogin, Ben
AU  - Chen, Sihao
AU  - Dasigi, Pradeep
AU  - Dua, Dheeru
AU  - Elazar, Yanai
AU  - Gottumukkala, Ananth
AU  - Gupta, Nitish
AU  - Hajishirzi, Hannaneh
AU  - Ilharco, Gabriel
AU  - Khashabi, Daniel
AU  - Lin, Kevin
AU  - Liu, Jiangming
AU  - Liu, Nelson F.
AU  - Mulcaire, Phoebe
AU  - Ning, Qiang
AU  - Singh, Sameer
AU  - Smith, Noah A.
AU  - Subramanian, Sanjay
AU  - Tsarfaty, Reut
AU  - Wallace, Eric
AU  - Zhang, Ally
AU  - Zhou, Ben
T2  - Findings 2020
A2  - Cohn, Trevor
A2  - He, Yulan
A2  - Liu, Yang
AB  - Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets—up to 25% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.
C1  - Online
C3  - Findings of the Association for Computational Linguistics: EMNLP 2020
DA  - 2020/11//
PY  - 2020
DO  - 10.18653/v1/2020.findings-emnlp.117
DP  - ACLWeb
SP  - 1307
EP  - 1323
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2020.findings-emnlp.117
Y2  - 2023/11/16/17:12:58
L1  - https://aclanthology.org/2020.findings-emnlp.117.pdf
ER  - 

TY  - CONF
TI  - Explaining and Harnessing Adversarial Examples
AU  - Goodfellow, Ian J.
AU  - Shlens, Jonathon
AU  - Szegedy, Christian
T2  - ICLR
AB  - Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.
C3  - International Conference on Learning Representations
DA  - 2015/03/20/
PY  - 2015
DO  - 10.48550/arXiv.1412.6572
DP  - arXiv.org
UR  - http://arxiv.org/abs/1412.6572
Y2  - 2023/11/16/17:14:15
L1  - https://arxiv.org/pdf/1412.6572.pdf
L2  - https://arxiv.org/abs/1412.6572
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - CONF
TI  - Benchmarking Neural Network Robustness to Common Corruptions and Perturbations
AU  - Hendrycks, Dan
AU  - Dietterich, Thomas
T2  - ICLR
AB  - In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.
C3  - International Conference on Learning Representations
DA  - 2019/03/28/
PY  - 2019
DO  - 10.48550/arXiv.1903.12261
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1903.12261
Y2  - 2023/11/16/17:15:28
L1  - https://arxiv.org/pdf/1903.12261.pdf
L2  - https://arxiv.org/abs/1903.12261
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - CONF
TI  - A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks
AU  - Hendrycks, Dan
AU  - Gimpel, Kevin
T2  - International Conference on Representation Learning
AB  - We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.
DA  - 2018/10/03/
PY  - 2018
DO  - 10.48550/arXiv.1610.02136
DP  - arXiv.org
UR  - http://arxiv.org/abs/1610.02136
Y2  - 2023/11/16/17:16:04
L1  - https://arxiv.org/pdf/1610.02136.pdf
L2  - https://arxiv.org/abs/1610.02136
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - CONF
TI  - Learning the Difference that Makes a Difference with Counterfactually-Augmented Data
AU  - Kaushik, Divyansh
AU  - Hovy, Eduard
AU  - Lipton, Zachary C.
T2  - ICLR
AB  - Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.
C3  - International Conference on Learning Representations
DA  - 2020/02/14/
PY  - 2020
DO  - 10.48550/arXiv.1909.12434
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1909.12434
Y2  - 2023/11/16/17:16:36
L1  - https://arxiv.org/pdf/1909.12434.pdf
L2  - https://arxiv.org/abs/1909.12434
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Social Norms-Grounded Machine Ethics in Complex Narrative Situation
AU  - Shen, Tao
AU  - Geng, Xiubo
AU  - Jiang, Daxin
T2  - COLING 2022
A2  - Calzolari, Nicoletta
A2  - Huang, Chu-Ren
A2  - Kim, Hansaem
A2  - Pustejovsky, James
A2  - Wanner, Leo
A2  - Choi, Key-Sun
A2  - Ryu, Pum-Mo
A2  - Chen, Hsin-Hsi
A2  - Donatelli, Lucia
A2  - Ji, Heng
A2  - Kurohashi, Sadao
A2  - Paggio, Patrizia
A2  - Xue, Nianwen
A2  - Kim, Seokhwan
A2  - Hahm, Younggyun
A2  - He, Zhong
A2  - Lee, Tony Kyungil
A2  - Santus, Enrico
A2  - Bond, Francis
A2  - Na, Seung-Hoon
AB  - Ethical judgment aims to determine if a person in a narrative situation acts under people's social norms under a culture, so it is crucial to understand actions in narratives and achieve machine ethics. Recent works depend on data-driven methods to directly judge the ethics of complex real-world narratives but face two major challenges. First, they cannot well handle dilemma situations due to a lack of basic knowledge about social norms. Second, they focus merely on sparse situation-level judgment regardless of the social norms involved during the judgment, leading to a black box. In this work, inspired by previous knowledge-grounded and -augmented paradigms, we propose to complement a complex situation with grounded social norms. Besides a norm-grounding knowledge model, we present a novel norm-supported ethical judgment model in line with neural module networks to alleviate dilemma situations and improve norm-level explainability. Empirically, our model improves state-of-the-art performance on two narrative judgment benchmarks.
C1  - Gyeongju, Republic of Korea
C3  - Proceedings of the 29th International Conference on Computational Linguistics
DA  - 2022/10//
PY  - 2022
DP  - ACLWeb
SP  - 1333
EP  - 1343
PB  - International Committee on Computational Linguistics
UR  - https://aclanthology.org/2022.coling-1.114
Y2  - 2023/11/16/17:22:18
L1  - https://aclanthology.org/2022.coling-1.114.pdf
ER  - 

TY  - GEN
TI  - Supervising strong learners by amplifying weak experts
AU  - Christiano, Paul
AU  - Shlegeris, Buck
AU  - Amodei, Dario
AB  - Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.
DA  - 2018/10/19/
PY  - 2018
DO  - 10.48550/arXiv.1810.08575
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1810.08575
Y2  - 2023/11/16/17:39:38
L1  - https://arxiv.org/pdf/1810.08575.pdf
L2  - https://arxiv.org/abs/1810.08575
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Taxonomy of Pathways to Dangerous AI
AU  - Yampolskiy, Roman V.
AB  - In order to properly handle a dangerous Artificially Intelligent (AI) system it is important to understand how the system came to be in such a state. In popular culture (science fiction movies/books) AIs/Robots became self-aware and as a result rebel against humanity and decide to destroy it. While it is one possible scenario, it is probably the least likely path to appearance of dangerous AI. In this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious AI. To the best of our knowledge, this is the first attempt to systematically classify types of pathways leading to malevolent AI. Previous relevant work either surveyed specific goals/meta-rules which might lead to malevolent behavior in AIs (\"Ozkural, 2014) or reviewed specific undesirable behaviors AGIs can exhibit at different stages of its development (Alexey Turchin, July 10 2015, July 10, 2015).
DA  - 2015/11/11/
PY  - 2015
DO  - 10.48550/arXiv.1511.03246
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1511.03246
Y2  - 2023/11/16/17:41:31
L1  - https://arxiv.org/pdf/1511.03246.pdf
L2  - https://arxiv.org/abs/1511.03246
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Unethical Research: How to Create a Malevolent Artificial Intelligence
AU  - Pistono, Federico
AU  - Yampolskiy, Roman V.
AB  - Cybersecurity research involves publishing papers about malicious exploits as much as publishing information on how to design tools to protect cyber-infrastructure. It is this information exchange between ethical hackers and security experts, which results in a well-balanced cyber-ecosystem. In the blooming domain of AI Safety Engineering, hundreds of papers have been published on different proposals geared at the creation of a safe machine, yet nothing, to our knowledge, has been published on how to design a malevolent machine. Availability of such information would be of great value particularly to computer scientists, mathematicians, and others who have an interest in AI safety, and who are attempting to avoid the spontaneous emergence or the deliberate creation of a dangerous AI, which can negatively affect human activities and in the worst case cause the complete obliteration of the human species. This paper provides some general guidelines for the creation of a Malevolent Artificial Intelligence (MAI).
DA  - 2016/09/01/
PY  - 2016
DO  - 10.48550/arXiv.1605.02817
DP  - arXiv.org
PB  - arXiv
ST  - Unethical Research
UR  - http://arxiv.org/abs/1605.02817
Y2  - 2023/11/16/17:42:08
L1  - https://arxiv.org/pdf/1605.02817.pdf
L2  - https://arxiv.org/abs/1605.02817
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - JOUR
TI  - Responses to catastrophic AGI risk: a survey
AU  - Sotala, Kaj
AU  - Yampolskiy, Roman V.
T2  - Physica Scripta
AB  - Many researchers have argued that humanity will create artificial general intelligence (AGI) within the next twenty to one hundred years. It has been suggested that AGI may inflict serious damage to human well-being on a global scale (‘catastrophic risk’). After summarizing the arguments for why AGI may pose such a risk, we review the fieldʼs proposed responses to AGI risk. We consider societal proposals, proposals for external constraints on AGI behaviors and proposals for creating AGIs that are safe due to their internal design.
DA  - 2014/12//
PY  - 2014
DO  - 10.1088/0031-8949/90/1/018001
DP  - Institute of Physics
VL  - 90
IS  - 1
SP  - 018001
J2  - Phys. Scr.
LA  - en
SN  - 1402-4896
ST  - Responses to catastrophic AGI risk
UR  - https://dx.doi.org/10.1088/0031-8949/90/1/018001
Y2  - 2023/11/16/17:42:54
L1  - https://iopscience.iop.org/article/10.1088/0031-8949/90/1/018001/pdf
ER  - 

TY  - CONF
TI  - Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping
AU  - Ng, Andrew Y.
AU  - Harada, Daishi
AU  - Russell, Stuart J.
T3  - ICML '99
AB  - This paper investigates conditions under which modifications to the reward function of a Markov decision process preserve the optimal policy. It is shown, that besides the positive linear transformation familiar from utility theory, one can add a reward for transitions between states that is expressible as the difference in value of an arbitrary potential function applied to those states. Furthermore, this is shown to be a necessary condition for invariance, in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP. These results shed light on the practice of reward shaping, a method used in reinforcement learning whereby additional training rewards are used to guide the learning agent. In particular, some well-known "bugs" in reward shaping procedures are shown to arise non-potential-based rewards, and methods are given for constructing shaping potentials corresponding to distance-based and subgoal-based heuristics. We show that such potentials can lead to substantial reductions in learning time.
C1  - San Francisco, CA, USA
C3  - Proceedings of the Sixteenth International Conference on Machine Learning
DA  - 1999/06/27/
PY  - 1999
DP  - ACM Digital Library
SP  - 278
EP  - 287
PB  - Morgan Kaufmann Publishers Inc.
SN  - 978-1-55860-612-8
ST  - Policy Invariance Under Reward Transformations
Y2  - 2023/11/16/
L1  - https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf
ER  - 

TY  - CHAP
TI  - Artificial Intelligence Safety Engineering: Why Machine Ethics Is a Wrong Approach
AU  - Yampolskiy, Roman V.
T2  - Philosophy and Theory of Artificial Intelligence
A2  - Müller, Vincent C.
T3  - Studies in Applied Philosophy, Epistemology and Rational Ethics
AB  - Machine ethics and robot rights are quickly becoming hot topics in artificial intelligence/robotics communities. We will argue that the attempts to allow machines to make ethical decisions or to have rights are misguided. Instead we propose a new science of safety engineering for intelligent artificial agents. In particular we issue a challenge to the scientific community to develop intelligent systems capable of proving that they are in fact safe even under recursive self-improvement.
CY  - Berlin, Heidelberg
DA  - 2013///
PY  - 2013
DP  - Springer Link
SP  - 389
EP  - 396
LA  - en
PB  - Springer
SN  - 978-3-642-31674-6
ST  - Artificial Intelligence Safety Engineering
UR  - https://doi.org/10.1007/978-3-642-31674-6_29
Y2  - 2023/11/16/17:44:23
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-642-31674-6_29.pdf
KW  - AI Confinement
KW  - Machine Ethics
KW  - Robot Rights
ER  - 

TY  - JOUR
TI  - Safety Engineering for Artificial General Intelligence
AU  - Yampolskiy, Roman
AU  - Fox, Joshua
T2  - Topoi
AB  - Machine ethics and robot rights are quickly becoming hot topics in artificial intelligence and robotics communities. We will argue that attempts to attribute moral agency and assign rights to all intelligent machines are misguided, whether applied to infrahuman or superhuman AIs, as are proposals to limit the negative eﬀects of AIs by constraining their behavior. As an alternative, we propose a new science of safety engineering for intelligent artificial agents based on maximizing for what humans value. In particular, we challenge the scientific community to develop intelligent systems that have human-friendly values that they provably retain, even under recursive self-improvement.
DA  - 2012/08/24/
PY  - 2012
DO  - 10.1007/s11245-012-9128-9
DP  - DOI.org (Crossref)
J2  - Topoi
LA  - en
SN  - 0167-7411, 1572-8749
UR  - http://link.springer.com/10.1007/s11245-012-9128-9
Y2  - 2023/11/16/17:44:48
L1  - https://intelligence.org/files/SafetyEngineering.pdf
ER  - 

TY  - JOUR
TI  - Leakproofing the Singularity: Artificial intelligence confinement problem
AU  - Yampolskiy, Roman V.
T2  - Journal of Consciousness Studies
AB  - This paper attempts to formalize and to address the 'leakproofing' of the Singularity problem presented by David Chalmers. The paper begins with the definition of the Artificial Intelligence Confinement Problem. After analysis of existing solutions and their shortcomings, a protocol is proposed aimed at making a more secure confinement environment which might delay potential negative effect from the technological singularity while allowing humanity to benefit from the superintelligence. (PsycINFO Database Record (c) 2017 APA, all rights reserved)
DA  - 2012///
PY  - 2012
DP  - APA PsycNet
VL  - 19
IS  - 1-2
SP  - 194
EP  - 214
SN  - 2051-2201
ST  - Leakproofing the Singularity
L2  - https://psycnet.apa.org/record/2012-08628-014
KW  - Artificial Intelligence
KW  - Technology
ER  - 

TY  - CONF
TI  - Apprenticeship learning via inverse reinforcement learning
AU  - Abbeel, Pieter
AU  - Ng, Andrew Y.
T3  - ICML '04
AB  - We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.
C1  - New York, NY, USA
C3  - Proceedings of the twenty-first international conference on Machine learning
DA  - 2004/07/04/
PY  - 2004
DO  - 10.1145/1015330.1015430
DP  - ACM Digital Library
SP  - 1
PB  - Association for Computing Machinery
SN  - 978-1-58113-838-2
UR  - https://dl.acm.org/doi/10.1145/1015330.1015430
Y2  - 2023/11/17/
L1  - https://dl.acm.org/doi/pdf/10.1145/1015330.1015430
ER  - 

TY  - CONF
TI  - Inverse Game Theory: Learning Utilities in Succinct Games
AU  - Kuleshov, Volodymyr
AU  - Schrijvers, Okke
A2  - Markakis, Evangelos
A2  - Schäfer, Guido
T3  - Lecture Notes in Computer Science
AB  - One of the central questions in game theory deals with predicting the behavior of an agent. Here, we study the inverse of this problem: given the agents’ equilibrium behavior, what are possible utilities that motivate this behavior? We consider this problem in arbitrary normal-form games in which the utilities can be represented by a small number of parameters, such as in graphical, congestion, and network design games. In all such settings, we show how to efficiently, i.e. in polynomial time, determine utilities consistent with a given correlated equilibrium. However, inferring both utilities and structural elements (e.g., the graph within a graphical game) is in general NP-hard. From a theoretical perspective our results show that rationalizing an equilibrium is computationally easier than computing it; from a practical perspective a practitioner can use our algorithms to validate behavioral models.
C1  - Berlin, Heidelberg
C3  - Web and Internet Economics
DA  - 2015///
PY  - 2015
DO  - 10.1007/978-3-662-48995-6_30
DP  - Springer Link
SP  - 413
EP  - 427
LA  - en
PB  - Springer
SN  - 978-3-662-48995-6
ST  - Inverse Game Theory
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-662-48995-6_30.pdf
KW  - Equilibrium Correlation (CE)
KW  - Game Theory Deals
KW  - Graphical Games
KW  - Network Design Games
KW  - Succinct Games
ER  - 

TY  - CHAP
TI  - Assuring the Behavior of Adaptive Agents
AU  - Spears, Diana F.
T2  - Agent Technology from a Formal Perspective
A2  - Rouff, Christopher A.
A2  - Hinchey, Michael
A2  - Rash, James
A2  - Truszkowski, Walter
A2  - Gordon-Spears, Diana
T3  - NASA Monographs in Systems and Software Engineering
AB  - Agents are becoming increasingly prevalent and effective. Robots and softbots, working individually or in concert, can relieve people of a great deal of labor-intensive tedium. Designers can furnish agents with plans to perform desired tasks. Nevertheless, a designer cannot possibly foresee all circumstances that will be encountered by the agent. Therefore, in addition to supplying an agent with a plan, it is essential to also enable the agent to learn and modify its plan to adapt to unforeseen circumstances. The introduction of learning, however, often makes the agent’s behavior significantly harder to predict.1 The goal of this research is to verify the behavior of adaptive agents. In particular, our objective is to develop efficient methods for determining whether the behavior of learning agents remains within the bounds of prespecified constraints (called “properties”) after learning. This includes verifying that properties are preserved for single adaptive agents as well as verifying that global properties are preserved for multi-agent systems in which one or more agents may adapt.
CY  - London
DA  - 2006///
PY  - 2006
DP  - Springer Link
SP  - 227
EP  - 257
LA  - en
PB  - Springer
SN  - 978-1-84628-271-3
UR  - https://doi.org/10.1007/1-84628-271-3_8
Y2  - 2023/11/17/16:24:41
L1  - https://link.springer.com/content/pdf/10.1007%2F1-84628-271-3_8.pdf
KW  - Agent Technology
KW  - Learning Operator
KW  - Model Check
KW  - Multiagent System
KW  - Response Property
ER  - 

TY  - CONF
TI  - Corrigibility
AU  - Soares, Nate
AU  - Fallenstein, Benja
AU  - Yudkowsky, Eliezer
AU  - Armstrong, Stuart
T2  - The 29th Annual AAAI Conference on Artificial Intelligence
AB  - As artificially intelligent systems grow in intelligence and capability, some of their available options may allow them to resist intervention by their programmers. We call an AI system “corrigible” if it cooperates with what its creators regard
as a corrective intervention, despite default incentives for rational agents to resist attempts to shut them down or modify their preferences. We introduce the notion of corrigibility and analyze utility functions that attempt to make an agent shut
down safely if a shutdown button is pressed, while avoiding incentives to prevent the button from being pressed or cause
the button to be pressed, and while ensuring propagation of the shutdown behavior as it creates new subsystems or selfmodifies. While some proposals are interesting, none have yet been demonstrated to satisfy all of our intuitive desiderata, leaving this simple problem in corrigibility wide-open.
C1  - Austin, Texas, USA
C3  - Artificial Intelligence and Ethics: Papers from the 2015 AAAI Workshop
DA  - 2015///
PY  - 2015
UR  - https://cdn.aaai.org/ocs/ws/ws0067/10124-45900-1-PB.pdf
Y2  - 2023/11/17/16:30:43
L1  - https://cdn.aaai.org/ocs/ws/ws0067/10124-45900-1-PB.pdf
ER  - 

TY  - CONF
TI  - Avoiding Unintended AI Behaviors
AU  - Hibbard, Bill
A2  - Bach, Joscha
A2  - Goertzel, Ben
A2  - Iklé, Matthew
T3  - Lecture Notes in Computer Science
AB  - Artificial intelligence (AI) systems too complex for predefined environment models and actions will need to learn environment models and to choose actions that optimize some criteria. Several authors have described mechanisms by which such complex systems may behave in ways not intended in their designs. This paper describes ways to avoid such unintended behavior. For hypothesized powerful AI systems that may pose a threat to humans, this paper proposes a two-stage agent architecture that avoids some known types of unintended behavior. For the first stage of the architecture this paper shows that the most probable finite stochastic program to model a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions.
C1  - Berlin, Heidelberg
C3  - Artificial General Intelligence
DA  - 2012///
PY  - 2012
DO  - 10.1007/978-3-642-35506-6_12
DP  - Springer Link
SP  - 107
EP  - 116
LA  - en
PB  - Springer
SN  - 978-3-642-35506-6
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-642-35506-6_12.pdf
KW  - agent architecture
KW  - agent motivation
KW  - rational agent
ER  - 

TY  - GEN
TI  - Hardening of Artificial Neural Networks for Use in Safety-Critical Applications -- A Mapping Study
AU  - Adler, Rasmus
AU  - Akram, Mohammed Naveed
AU  - Bauer, Pascal
AU  - Feth, Patrik
AU  - Gerber, Pascal
AU  - Jedlitschka, Andreas
AU  - Jöckel, Lisa
AU  - Kläs, Michael
AU  - Schneider, Daniel
AB  - Context: Across different domains, Artificial Neural Networks (ANNs) are used more and more in safety-critical applications in which erroneous outputs of such ANN can have catastrophic consequences. However, the development of such neural networks is still immature and good engineering practices are missing. With that, ANNs are in the same position as software was several decades ago. Today, standards for functional safety, such as ISO 26262 in the automotive domain, require the application of a collection of proven engineering principles and methods in the creation of software to increase its quality and reduce failure rates to an acceptable level. Objective: In the future, such a set of proven engineering methods needs to be established for the development of Artificial Neural Networks to allow their use in safety-critical applications. Method: This work takes a step in this direction by conducting a mapping study to extract challenges faced in the development of ANNs for safety-critical applications and to identify methods that have been used for the hardening of ANNs in such settings. Results: We extracted ten different challenges found to be repeatedly reported in the literature regarding the use of ANNs in critical contexts. All of these challenges are addressed by engineering methods, of which we identified 54 in our study that can be used for the hardening of networks. Conclusions: Various methods have been proposed to overcome the specific challenges of using ANNs in safety-critical applications. On the path towards defining best practices, we envision that future software engineering will need to focus on further investigating these methods and increasing the maturity and understanding of existing approaches, with the goal to develop clear guidance for proper engineering of high-quality ANNs.
DA  - 2019/09/02/
PY  - 2019
DO  - 10.48550/arXiv.1909.03036
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1909.03036
Y2  - 2023/11/17/16:37:59
L1  - https://arxiv.org/pdf/1909.03036.pdf
L2  - https://arxiv.org/abs/1909.03036
KW  - Computer Science - Computers and Society
ER  - 

TY  - CONF
TI  - Establishing Safety Criteria for Artificial Neural Networks
AU  - Kurd, Zeshan
AU  - Kelly, Tim
A2  - Palade, Vasile
A2  - Howlett, Robert J.
A2  - Jain, Lakhmi
T3  - Lecture Notes in Computer Science
AB  - Artificial neural networks are employed in many areas of industry such as medicine and defence. There are many techniques that aim to improve the performance of neural networks for safety-critical systems. However, there is a complete absence of analytical certification methods for neural network paradigms. Consequently, their role in safety-critical applications, if any, is typically restricted to advisory systems. It is therefore desirable to enable neural networks for highly-dependable roles. This paper defines the safety criteria which if enforced, would contribute to justifying the safety of neural networks. The criteria are a set of safety requirements for the behaviour of neural networks. The paper also highlights the challenge of maintaining performance in terms of adaptability and generalisation whilst providing acceptable safety arguments.
C1  - Berlin, Heidelberg
C3  - Knowledge-Based Intelligent Information and Engineering Systems
DA  - 2003///
PY  - 2003
DO  - 10.1007/978-3-540-45224-9_24
DP  - Springer Link
SP  - 163
EP  - 169
LA  - en
PB  - Springer
SN  - 978-3-540-45224-9
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-540-45224-9_24.pdf
KW  - Neural Network
KW  - Artificial Neural Network
KW  - Safety Case
KW  - Safety Criterion
KW  - Safety Requirement
ER  - 

TY  - GEN
TI  - Towards Deep Learning Models Resistant to Adversarial Attacks
AU  - Madry, Aleksander
AU  - Makelov, Aleksandar
AU  - Schmidt, Ludwig
AU  - Tsipras, Dimitris
AU  - Vladu, Adrian
AB  - Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.
DA  - 2019/09/04/
PY  - 2019
DO  - 10.48550/arXiv.1706.06083
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1706.06083
Y2  - 2023/11/17/16:42:10
L1  - https://arxiv.org/pdf/1706.06083.pdf
L2  - https://arxiv.org/abs/1706.06083
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - CONF
TI  - Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope
AU  - Wong, Eric
AU  - Kolter, Zico
T2  - International Conference on Machine Learning
AB  - We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8% test error for any adversarial attack with bounded $\ell_\infty$ norm less than $\epsilon = 0.1$).
C3  - Proceedings of the 35th International Conference on Machine Learning
DA  - 2018/07/03/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 5286
EP  - 5295
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v80/wong18a.html
Y2  - 2023/11/17/16:42:53
L1  - https://proceedings.mlr.press/v80/wong18a/wong18a.pdf
ER  - 

TY  - GEN
TI  - How to talk so AI will learn: Instructions, descriptions, and autonomy
AU  - Sumers, Theodore R.
AU  - Hawkins, Robert D.
AU  - Ho, Mark K.
AU  - Griffiths, Thomas L.
AU  - Hadfield-Menell, Dylan
AB  - From the earliest years of our lives, humans use language to express our beliefs and desires. Being able to talk to artificial agents about our preferences would thus fulfill a central goal of value alignment. Yet today, we lack computational models explaining such language use. To address this challenge, we formalize learning from language in a contextual bandit setting and ask how a human might communicate preferences over behaviors. We study two distinct types of language: $\textit{instructions}$, which provide information about the desired policy, and $\textit{descriptions}$, which provide information about the reward function. We show that the agent's degree of autonomy determines which form of language is optimal: instructions are better in low-autonomy settings, but descriptions are better when the agent will need to act independently. We then define a pragmatic listener agent that robustly infers the speaker's reward function by reasoning about $\textit{how}$ the speaker expresses themselves. We validate our models with a behavioral experiment, demonstrating that (1) our speaker model predicts human behavior, and (2) our pragmatic listener successfully recovers humans' reward functions. Finally, we show that this form of social learning can integrate with and reduce regret in traditional reinforcement learning. We hope these insights facilitate a shift from developing agents that $\textit{obey}$ language to agents that $\textit{learn}$ from it.
DA  - 2022/10/10/
PY  - 2022
DP  - arXiv.org
PB  - arXiv
ST  - How to talk so AI will learn
UR  - http://arxiv.org/abs/2206.07870
Y2  - 2023/11/17/16:46:50
L1  - https://arxiv.org/pdf/2206.07870.pdf
L2  - https://arxiv.org/abs/2206.07870
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Scalable agent alignment via reward modeling: a research direction
AU  - Leike, Jan
AU  - Krueger, David
AU  - Everitt, Tom
AU  - Martic, Miljan
AU  - Maini, Vishal
AU  - Legg, Shane
AB  - One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.
DA  - 2018/11/19/
PY  - 2018
DP  - arXiv.org
PB  - arXiv
ST  - Scalable agent alignment via reward modeling
UR  - http://arxiv.org/abs/1811.07871
Y2  - 2023/11/17/16:48:13
L1  - https://arxiv.org/pdf/1811.07871.pdf
L2  - https://arxiv.org/abs/1811.07871
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Task-Guided Inverse Reinforcement Learning Under Partial Information
AU  - Djeumou, Franck
AU  - Cubuktepe, Murat
AU  - Lennon, Craig
AU  - Topcu, Ufuk
AB  - We study the problem of inverse reinforcement learning (IRL), where the learning agent recovers a reward function using expert demonstrations. Most of the existing IRL techniques make the often unrealistic assumption that the agent has access to full information about the environment. We remove this assumption by developing an algorithm for IRL in partially observable Markov decision processes (POMDPs). The algorithm addresses several limitations of existing techniques that do not take the information asymmetry between the expert and the learner into account. First, it adopts causal entropy as the measure of the likelihood of the expert demonstrations as opposed to entropy in most existing IRL techniques, and avoids a common source of algorithmic complexity. Second, it incorporates task specifications expressed in temporal logic into IRL. Such specifications may be interpreted as side information available to the learner a priori in addition to the demonstrations and may reduce the information asymmetry. Nevertheless, the resulting formulation is still nonconvex due to the intrinsic nonconvexity of the so-called forward problem, i.e., computing an optimal policy given a reward function, in POMDPs. We address this nonconvexity through sequential convex programming and introduce several extensions to solve the forward problem in a scalable manner. This scalability allows computing policies that incorporate memory at the expense of added computational cost yet also outperform memoryless policies. We demonstrate that, even with severely limited data, the algorithm learns reward functions and policies that satisfy the task and induce a similar behavior to the expert by leveraging the side information and incorporating memory into the policy.
DA  - 2021/12/16/
PY  - 2021
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2105.14073
Y2  - 2023/11/17/16:50:10
L1  - https://arxiv.org/pdf/2105.14073.pdf
L2  - https://arxiv.org/abs/2105.14073
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Mathematics - Optimization and Control
ER  - 

TY  - GEN
TI  - MORAL: Aligning AI with Human Norms through Multi-Objective Reinforced Active Learning
AU  - Peschl, Markus
AU  - Zgonnikov, Arkady
AU  - Oliehoek, Frans A.
AU  - Siebert, Luciano C.
AB  - Inferring reward functions from demonstrations and pairwise preferences are auspicious approaches for aligning Reinforcement Learning (RL) agents with human intentions. However, state-of-the art methods typically focus on learning a single reward model, thus rendering it difficult to trade off different reward functions from multiple experts. We propose Multi-Objective Reinforced Active Learning (MORAL), a novel method for combining diverse demonstrations of social norms into a Pareto-optimal policy. Through maintaining a distribution over scalarization weights, our approach is able to interactively tune a deep RL agent towards a variety of preferences, while eliminating the need for computing multiple policies. We empirically demonstrate the effectiveness of MORAL in two scenarios, which model a delivery and an emergency task that require an agent to act in the presence of normative conflicts. Overall, we consider our research a step towards multi-objective RL with learned rewards, bridging the gap between current reward learning and machine ethics literature.
DA  - 2021/12/30/
PY  - 2021
DP  - arXiv.org
PB  - arXiv
ST  - MORAL
UR  - http://arxiv.org/abs/2201.00012
Y2  - 2023/11/17/16:51:30
L1  - https://arxiv.org/pdf/2201.00012.pdf
L2  - https://arxiv.org/abs/2201.00012
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Transparent Value Alignment
AU  - Sanneman, Lindsay
AU  - Shah, Julie
T2  - HRI '23: ACM/IEEE International Conference on Human-Robot Interaction
AB  - As robots become increasingly prevalent in our communities, aligning the values motivating their behavior with human values is critical. However, it is often difficult or impossible for humans, both expert and non-expert, to enumerate values comprehensively, accurately, and in forms that are readily usable for robot planning. Misspecification can lead to undesired, inefficient, or even dangerous behavior. In the value alignment problem, humans and robots work together to optimize human objectives, which are often represented as reward functions and which the robot can infer by observing human actions. In existing alignment approaches, no explicit feedback about this inference process is provided to the human. In this paper, we introduce an exploratory framework to address this problem, which we call Transparent Value Alignment (TVA). TVA suggests that techniques from explainable AI (XAI) be explicitly applied to provide humans with information about the robot’s beliefs throughout learning, enabling efficient and effective human feedback.
C1  - Stockholm Sweden
C3  - Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction
DA  - 2023/03/13/
PY  - 2023
DO  - 10.1145/3568294.3580147
DP  - DOI.org (Crossref)
SP  - 557
EP  - 560
LA  - en
PB  - ACM
SN  - 978-1-4503-9970-8
UR  - https://dl.acm.org/doi/10.1145/3568294.3580147
Y2  - 2023/11/17/16:52:30
L1  - https://dl.acm.org/doi/pdf/10.1145/3568294.3580147
ER  - 

TY  - JOUR
TI  - Towards Deployment of Robust Cooperative AI Agents: An Algorithmic Framework for Learning Adaptive Policies
AU  - Ghosh, Ahana
AU  - Tschiatschek, Sebastian
AU  - Mahdavi, Hamed
AU  - Singla, Adish
T2  - New Zealand
AB  - We study the problem of designing an AI agent that can robustly cooperate with agents of unknown type (i.e., previously unobserved behavior) in multi-agent scenarios. Our work is inspired by realworld applications in which an AI agent, e.g., a virtual assistant, has to cooperate with new types of agents/users after its deployment. We model this problem via parametric Markov Decision Processes where the parameters correspond to a user’s type and characterize her behavior. In the test phase, the AI agent has to interact with a user of an unknown type. We develop an algorithmic framework for learning adaptive policies: our approach relies on observing the user’s actions to make inferences about the user’s type and adapting the policy to facilitate efficient cooperation. We show that without being adaptive, an AI agent can end up performing arbitrarily bad in the test phase. Using our framework, we propose two concrete algorithms for computing policies that automatically adapt to the user in the test phase. We demonstrate the effectiveness of our algorithms in a cooperative gathering game environment for two agents.
DA  - 2020///
PY  - 2020
DP  - Zotero
LA  - en
L1  - https://ifaamas.org/Proceedings/aamas2020/pdfs/p447.pdf
ER  - 

TY  - CONF
TI  - MetaReg: Towards Domain Generalization using Meta-Regularization
AU  - Balaji, Yogesh
AU  - Sankaranarayanan, Swami
AU  - Chellappa, Rama
AB  - Training models that generalize to new domains at test time is a problem of fundamental importance in machine learning. In this work, we encode this notion of domain generalization using a novel regularization function. We pose the problem of finding such a regularization function in a Learning to Learn (or) meta-learning framework. The objective of domain generalization is explicitly modeled by learning a regularizer that makes the model trained on one domain to perform well on another domain. Experimental validations on computer vision and natural language datasets indicate that our method can learn regularizers that achieve good cross-domain generalization.
C3  - Advances in Neural Information Processing Systems
DA  - 2018///
PY  - 2018
DP  - Neural Information Processing Systems
VL  - 31
PB  - Curran Associates, Inc.
ST  - MetaReg
UR  - https://papers.nips.cc/paper_files/paper/2018/hash/647bba344396e7c8170902bcf2e15551-Abstract.html
Y2  - 2023/11/17/16:55:17
L1  - https://papers.nips.cc/paper_files/paper/2018/file/647bba344396e7c8170902bcf2e15551-Paper.pdf
ER  - 

TY  - JOUR
TI  - Agreeing to disagree: active learning with noisy labels without crowdsourcing
AU  - Bouguelia, Mohamed-Rafik
AU  - Nowaczyk, Slawomir
AU  - Santosh, K. C.
AU  - Verikas, Antanas
T2  - International Journal of Machine Learning and Cybernetics
AB  - We propose a new active learning method for classification, which handles label noise without relying on multiple oracles (i.e., crowdsourcing). We propose a strategy that selects (for labeling) instances with a high influence on the learned model. An instance x is said to have a high influence on the model h, if training h on x (with label $$y = h(x)$$) would result in a model that greatly disagrees with h on labeling other instances. Then, we propose another strategy that selects (for labeling) instances that are highly influenced by changes in the learned model. An instance x is said to be highly influenced, if training h with a set of instances would result in a committee of models that agree on a common label for x but disagree with h(x). We compare the two strategies and we show, on different publicly available datasets, that selecting instances according to the first strategy while eliminating noisy labels according to the second strategy, greatly improves the accuracy compared to several benchmarking methods, even when a significant amount of instances are mislabeled.
DA  - 2018/08/01/
PY  - 2018
DO  - 10.1007/s13042-017-0645-0
DP  - Springer Link
VL  - 9
IS  - 8
SP  - 1307
EP  - 1319
J2  - Int. J. Mach. Learn. & Cyber.
LA  - en
SN  - 1868-808X
ST  - Agreeing to disagree
UR  - https://doi.org/10.1007/s13042-017-0645-0
Y2  - 2023/11/17/16:56:03
L1  - https://link.springer.com/content/pdf/10.1007%2Fs13042-017-0645-0.pdf
KW  - Classification
KW  - Label noise
KW  - Active learning
KW  - Mislabeling
ER  - 

TY  - GEN
TI  - Testing Robustness Against Unforeseen Adversaries
AU  - Kaufmann, Max
AU  - Kang, Daniel
AU  - Sun, Yi
AU  - Basart, Steven
AU  - Yin, Xuwang
AU  - Mazeika, Mantas
AU  - Arora, Akul
AU  - Dziedzic, Adam
AU  - Boenisch, Franziska
AU  - Brown, Tom
AU  - Steinhardt, Jacob
AU  - Hendrycks, Dan
AB  - Adversarial robustness research primarily focuses on L_p perturbations, and most defenses are developed with identical training-time and test-time adversaries. However, in real-world applications developers are unlikely to have access to the full range of attacks or corruptions their system will face. Furthermore, worst-case inputs are likely to be diverse and need not be constrained to the L_p ball. To narrow in on this discrepancy between research and reality we introduce ImageNet-UA, a framework for evaluating model robustness against a range of unforeseen adversaries, including eighteen new non-L_p attacks. To perform well on ImageNet-UA, defenses must overcome a generalization gap and be robust to a diverse attacks not encountered during training. In extensive experiments, we find that existing robustness measures do not capture unforeseen robustness, that standard robustness techniques are beat by alternative training strategies, and that novel methods can improve unforeseen robustness. We present ImageNet-UA as a useful tool for the community for improving the worst-case behavior of machine learning systems.
DA  - 2019/10/30/
PY  - 2019
DO  - 10.48550/arXiv.1908.08016
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1908.08016
Y2  - 2023/11/17/17:01:37
L1  - https://arxiv.org/pdf/1908.08016.pdf
L2  - https://arxiv.org/abs/1908.08016
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Human-in-the-Loop Interpretability Prior
AU  - Lage, Isaac
AU  - Ross, Andrew Slavin
AU  - Kim, Been
AU  - Gershman, Samuel J.
AU  - Doshi-Velez, Finale
AB  - We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.
DA  - 2018/10/30/
PY  - 2018
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1805.11571
Y2  - 2023/11/17/17:03:45
L1  - https://arxiv.org/pdf/1805.11571.pdf
L2  - https://arxiv.org/abs/1805.11571
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration
AU  - Lakkaraju, Himabindu
AU  - Kamar, Ece
AU  - Caruana, Rich
AU  - Horvitz, Eric
AB  - Predictive models deployed in the real world may assign incorrect labels to instances with high confidence. Such errors or unknown unknowns are rooted in model incompleteness, and typically arise because of the mismatch between training data and the cases encountered at test time. As the models are blind to such errors, input from an oracle is needed to identify these failures. In this paper, we formulate and address the problem of informed discovery of unknown unknowns of any given predictive model where unknown unknowns occur due to systematic biases in the training data. We propose a model-agnostic methodology which uses feedback from an oracle to both identify unknown unknowns and to intelligently guide the discovery. We employ a two-phase approach which first organizes the data into multiple partitions based on the feature similarity of instances and the confidence scores assigned by the predictive model, and then utilizes an explore-exploit strategy for discovering unknown unknowns across these partitions. We demonstrate the efficacy of our framework by varying the underlying causes of unknown unknowns across various applications. To the best of our knowledge, this paper presents the first algorithmic approach to the problem of discovering unknown unknowns of predictive models.
DA  - 2016/12/10/
PY  - 2016
DP  - arXiv.org
PB  - arXiv
ST  - Identifying Unknown Unknowns in the Open World
UR  - http://arxiv.org/abs/1610.09064
Y2  - 2023/11/17/17:04:27
L1  - https://arxiv.org/pdf/1610.09064.pdf
L2  - https://arxiv.org/abs/1610.09064
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Learning Robust Representations by Projecting Superficial Statistics Out
AU  - Wang, Haohan
AU  - He, Zexue
AU  - Lipton, Zachary C.
AU  - Xing, Eric P.
AB  - Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift. For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier. Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals. To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image. Then we introduce two techniques for improving our networks' out-of-sample performance. The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable. The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's. We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.
DA  - 2019/03/01/
PY  - 2019
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1903.06256
Y2  - 2023/11/17/17:08:36
L1  - https://arxiv.org/pdf/1903.06256.pdf
L2  - https://arxiv.org/abs/1903.06256
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - SPLASH: Learnable Activation Functions for Improving Accuracy and Adversarial Robustness
AU  - Tavakoli, Mohammadamin
AU  - Agostinelli, Forest
AU  - Baldi, Pierre
AB  - We introduce SPLASH units, a class of learnable activation functions shown to simultaneously improve the accuracy of deep neural networks while also improving their robustness to adversarial attacks. SPLASH units have both a simple parameterization and maintain the ability to approximate a wide range of non-linear functions. SPLASH units are: 1) continuous; 2) grounded (f(0) = 0); 3) use symmetric hinges; and 4) the locations of the hinges are derived directly from the data (i.e. no learning required). Compared to nine other learned and fixed activation functions, including ReLU and its variants, SPLASH units show superior performance across three datasets (MNIST, CIFAR-10, and CIFAR-100) and four architectures (LeNet5, All-CNN, ResNet-20, and Network-in-Network). Furthermore, we show that SPLASH units significantly increase the robustness of deep neural networks to adversarial attacks. Our experiments on both black-box and open-box adversarial attacks show that commonly-used architectures, namely LeNet5, All-CNN, ResNet-20, and Network-in-Network, can be up to 31% more robust to adversarial attacks by simply using SPLASH units instead of ReLUs.
DA  - 2020/06/16/
PY  - 2020
DO  - 10.48550/arXiv.2006.08947
DP  - arXiv.org
PB  - arXiv
ST  - SPLASH
UR  - http://arxiv.org/abs/2006.08947
Y2  - 2023/11/17/17:09:55
L1  - https://arxiv.org/pdf/2006.08947.pdf
L2  - https://arxiv.org/abs/2006.08947
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Discovering Robust Convolutional Architecture at Targeted Capacity: A Multi-Shot Approach
AU  - Ning, Xuefei
AU  - Zhao, Junbo
AU  - Li, Wenshuo
AU  - Zhao, Tianchen
AU  - Zheng, Yin
AU  - Yang, Huazhong
AU  - Wang, Yu
AB  - Convolutional neural networks (CNNs) are vulnerable to adversarial examples, and studies show that increasing the model capacity of an architecture topology (e.g., width expansion) can bring consistent robustness improvements. This reveals a clear robustness-efficiency trade-off that should be considered in architecture design. In this paper, considering scenarios with capacity budget, we aim to discover adversarially robust architecture at targeted capacities. Recent studies employed one-shot neural architecture search (NAS) to discover robust architectures. However, since the capacities of different topologies cannot be aligned in the search process, one-shot NAS methods favor topologies with larger capacities in the supernet. And the discovered topology might be suboptimal when augmented to the targeted capacity. We propose a novel multi-shot NAS method to address this issue and explicitly search for robust architectures at targeted capacities. At the targeted FLOPs of 2000M, the discovered MSRobNet-2000 outperforms the recent NAS-discovered architecture RobNet-large under various criteria by a large margin of 4%-7%. And at the targeted FLOPs of 1560M, MSRobNet-1560 surpasses another NAS-discovered architecture RobNet-free by 2.3% and 1.3% in the clean and PGD-7 accuracies, respectively. All codes are available at https://github.com/walkerning/aw\_nas.
DA  - 2021/03/26/
PY  - 2021
DO  - 10.48550/arXiv.2012.11835
DP  - arXiv.org
PB  - arXiv
ST  - Discovering Robust Convolutional Architecture at Targeted Capacity
UR  - http://arxiv.org/abs/2012.11835
Y2  - 2023/11/17/17:13:23
L1  - https://arxiv.org/pdf/2012.11835.pdf
L2  - https://arxiv.org/abs/2012.11835
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Towards neural networks that provably know when they don't know
AU  - Meinke, Alexander
AU  - Hein, Matthias
AB  - It has recently been shown that ReLU networks produce arbitrarily over-confident predictions far away from the training data. Thus, ReLU networks do not know when they don't know. However, this is a highly important property in safety critical applications. In the context of out-of-distribution detection (OOD) there have been a number of proposals to mitigate this problem but none of them are able to make any mathematical guarantees. In this paper we propose a new approach to OOD which overcomes both problems. Our approach can be used with ReLU networks and provides provably low confidence predictions far away from the training data as well as the first certificates for low confidence predictions in a neighborhood of an out-distribution point. In the experiments we show that state-of-the-art methods fail in this worst-case setting whereas our model can guarantee its performance while retaining state-of-the-art OOD performance.
DA  - 2020/02/21/
PY  - 2020
DO  - 10.48550/arXiv.1909.12180
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1909.12180
Y2  - 2023/11/17/17:14:27
L1  - https://arxiv.org/pdf/1909.12180.pdf
L2  - https://arxiv.org/abs/1909.12180
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - JOUR
TI  - Softmax linear units
AU  - Elhage, Nelson
AU  - Hume, Tristan
AU  - Olsson, Catherine
AU  - Nanda, Neel
AU  - Henighan, Tom
AU  - Johnston, Scott
AU  - ElShowk, Sheer
AU  - Joseph, Nicholas
AU  - DasSarma, Nova
AU  - Mann, Ben
AU  - Hernandez, Danny
AU  - Askell, Amanda
AU  - Ndousse, Kamal
AU  - Jones, Andy
AU  - Drain, Dawn
AU  - Chen, Anna
AU  - Bai, Yuntao
AU  - Ganguli, Deep
AU  - Lovitt, Liane
AU  - Hatfield-Dodds, Zac
AU  - Kernion, Jackson
AU  - Conerly, Tom
AU  - Kravec, Shauna
AU  - Fort, Stanislav
AU  - Kadavath, Saurav
AU  - Jacobson, Josh
AU  - Tran-Johnson, Eli
AU  - Kaplan, Jared
AU  - Clark, Jack
AU  - Brown, Tom
AU  - McCandlish, Sam
AU  - Amodei, Dario
AU  - Olah, Christopher
T2  - Transformer Circuits Thread
AB  - In this paper, we report an architectural change which appears to substantially increase the fraction of MLP neurons which appear to be "interpretable" (i.e. respond to an articulable property of the input), at little to no cost to ML performance. Specifically, we replace the activation function with a softmax linear unit (which we term SoLU) and show that this significantly increases the fraction of neurons in the MLP layers which seem to correspond to readily human-understandable concepts, phrases, or categories on quick investigation, as measured by randomized and blinded experiments. We then study our SoLU models and use them to gain several new insights about how information is processed in transformers.  However, we also discover some evidence that the superposition hypothesis is true and there is no free lunch: SoLU may be making some features more interpretable by “hiding” others and thus making them even more deeply uninterpretable.  Despite this, SoLU still seems like a net win, as in practical terms it substantially increases the fraction of neurons we are able to understand.
DA  - 2022///
PY  - 2022
UR  - https://transformer-circuits.pub/2022/solu/index.html
ER  - 

TY  - JOUR
TI  - Curve detectors
AU  - Cammarata, Nick
AU  - Goh, Gabriel
AU  - Carter, Shan
AU  - Schubert, Ludwig
AU  - Petrov, Michael
AU  - Olah, Chris
T2  - Distill
AB  - Every vision model we've explored in detail contains neurons which detect curves. Curve detectors in vision models have been hinted at in the literature as far back as 2013 (see figures in Zeiler & Fergus, and similar neurons have been studied carefully in neuroscience. We briefly discussed curve in our earlier overview of early vision, but wanted to examine them in more depth. This article is the first part of a three article deep dive into curve detectors: their behavior, how they're built from earlier neurons, and their prevalence across models.
DA  - 2020///
PY  - 2020
DO  - 10.23915/distill.00024.003
ER  - 

TY  - JOUR
TI  - Understanding the Role of Individual Units in a Deep Neural Network
AU  - Bau, David
AU  - Zhu, Jun-Yan
AU  - Strobelt, Hendrik
AU  - Lapedriza, Agata
AU  - Zhou, Bolei
AU  - Torralba, Antonio
T2  - Proceedings of the National Academy of Sciences
AB  - Deep neural networks excel at finding hierarchical representations that solve complex tasks over large data sets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.
DA  - 2020/12//
PY  - 2020
DO  - 10.1073/pnas.1907375117
DP  - arXiv.org
VL  - 117
IS  - 48
SP  - 30071
EP  - 30078
J2  - Proc. Natl. Acad. Sci. U.S.A.
SN  - 0027-8424, 1091-6490
UR  - http://arxiv.org/abs/2009.05041
Y2  - 2023/11/17/17:22:45
L1  - https://arxiv.org/pdf/2009.05041.pdf
L2  - https://arxiv.org/abs/2009.05041
KW  - I.2
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Neural and Evolutionary Computing
KW  - 68T07
KW  - I.4
ER  - 

TY  - CONF
TI  - On Interpretability and Feature Representations: An Analysis of the Sentiment Neuron
AU  - Donnelly, Jonathan
AU  - Roegiest, Adam
A2  - Azzopardi, Leif
A2  - Stein, Benno
A2  - Fuhr, Norbert
A2  - Mayr, Philipp
A2  - Hauff, Claudia
A2  - Hiemstra, Djoerd
T3  - Lecture Notes in Computer Science
AB  - We are concerned with investigating the apparent effectiveness of Radford et al.’s “Sentiment Neuron,” [9] which they claim encapsulates sufficient knowledge to accurately predict sentiment in reviews. In our analysis of the Sentiment Neuron, we find that the removal of the neuron only marginally affects a classifier’s ability to detect and label sentiment and may even improve performance. Moreover, the effectiveness of the Sentiment Neuron can be surpassed by simply using 100 random neurons as features to the same classifier. Using adversarial examples, we show that the generated representation containing the Sentiment Neuron (i.e., the final hidden cell state in a LSTM) is particularly sensitive to the end of a processed sequence. Accordingly, we find that caution needs to be applied when interpreting neuron-based feature representations and potential flaws should be addressed for real-world applicability.
C1  - Cham
C3  - Advances in Information Retrieval
DA  - 2019///
PY  - 2019
DO  - 10.1007/978-3-030-15712-8_55
DP  - Springer Link
SP  - 795
EP  - 802
LA  - en
PB  - Springer International Publishing
SN  - 978-3-030-15712-8
ST  - On Interpretability and Feature Representations
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-030-15712-8_55.pdf
ER  - 

TY  - JOUR
TI  - High-low frequency detectors
AU  - Schubert, Ludwig
AU  - Voss, Chelsea
AU  - Cammarata, Nick
AU  - Goh, Gabriel
AU  - Olah, Chris
T2  - Distill
AB  - Some of the neurons in vision models are features that we aren’t particularly surprised to find. Curve detectors, for example, are a pretty natural feature for a vision system to have. In fact, they had already been discovered in the animal visual cortex. It’s easy to imagine how curve detectors are built up from earlier edge detectors, and it’s easy to guess why curve detection might be useful to the rest of the neural network. High-low frequency detectors, on the other hand, seem more surprising. They are not a feature that we would have expected a priori to find. Yet, when systematically characterizing
 the early layers of InceptionV1, we found a full fifteen neurons of mixed3a that appear to detect a high frequency pattern on one side, and a low frequency pattern on the other. One worry we might have about the circuits approach to studying neural networks is that we might only be able to understand a limited set of highly-intuitive features. High-low frequency detectors demonstrate that it’s possible to understand at least somewhat unintuitive features.
DA  - 2021///
PY  - 2021
DO  - 10.23915/distill.00024.005
ER  - 

TY  - GEN
TI  - Progress measures for grokking via mechanistic interpretability
AU  - Nanda, Neel
AU  - Chan, Lawrence
AU  - Lieberum, Tom
AU  - Smith, Jess
AU  - Steinhardt, Jacob
AB  - Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.
DA  - 2023/10/19/
PY  - 2023
DO  - 10.48550/arXiv.2301.05217
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2301.05217
Y2  - 2023/11/17/17:25:02
L1  - https://arxiv.org/pdf/2301.05217.pdf
L2  - https://arxiv.org/abs/2301.05217
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Delving into Transferable Adversarial Examples and Black-box Attacks
AU  - Liu, Yanpei
AU  - Chen, Xinyun
AU  - Liu, Chang
AU  - Song, Dawn
AB  - An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.
DA  - 2017/02/07/
PY  - 2017
DO  - 10.48550/arXiv.1611.02770
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1611.02770
Y2  - 2023/11/17/17:25:34
L1  - https://arxiv.org/pdf/1611.02770.pdf
L2  - https://arxiv.org/abs/1611.02770
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - The building blocks of interpretability
AU  - Olah, Chris
AU  - Satyanarayan, Arvind
AU  - Johnson, Ian
AU  - Carter, Shan
AU  - Schubert, Ludwig
AU  - Ye, Katherine
AU  - Mordvintsev, Alexander
T2  - Distill
AB  - In this article, we treat existing interpretability methods as fundamental and composable building blocks for rich user interfaces. We find that these disparate techniques now come together in a unified grammar, fulfilling complementary roles in the resulting interfaces. Moreover, this grammar allows us to systematically explore the space of interpretability interfaces, enabling us to evaluate whether they meet particular goals. We will present interfaces that show what the network detects and explain how it develops its understanding, while keeping the amount of information human-scale. For example, we will see how a network looking at a labrador retriever detects floppy ears and how that influences its classification. In this article, we use GoogLeNet
Going deeper with convolutions, an image classification model, to demonstrate our interface ideas because its neurons seem unusually semantically meaningful. Although here we’ve made a specific choice of task and network, the basic abstractions and patterns for combining them that we present can be applied to neural networks in other domains.
DA  - 2018/03//
PY  - 2018
DO  - 10.23915/distill.00010
ER  - 

TY  - GEN
TI  - SPINE: SParse Interpretable Neural Embeddings
AU  - Subramanian, Anant
AU  - Pruthi, Danish
AU  - Jhamtani, Harsh
AU  - Berg-Kirkpatrick, Taylor
AU  - Hovy, Eduard
AB  - Prediction without justification has limited utility. Much of the success of neural models can be attributed to their ability to learn rich, dense and expressive representations. While these representations capture the underlying complexity and latent trends in the data, they are far from being interpretable. We propose a novel variant of denoising k-sparse autoencoders that generates highly efficient and interpretable distributed word representations (word embeddings), beginning with existing word representations from state-of-the-art methods like GloVe and word2vec. Through large scale human evaluation, we report that our resulting word embedddings are much more interpretable than the original GloVe and word2vec embeddings. Moreover, our embeddings outperform existing popular word embeddings on a diverse suite of benchmark downstream tasks.
DA  - 2017/11/23/
PY  - 2017
DO  - 10.48550/arXiv.1711.08792
DP  - arXiv.org
PB  - arXiv
ST  - SPINE
UR  - http://arxiv.org/abs/1711.08792
Y2  - 2023/11/17/17:26:41
L1  - https://arxiv.org/pdf/1711.08792.pdf
L2  - https://arxiv.org/abs/1711.08792
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Disentangling by Factorising
AU  - Kim, Hyunjik
AU  - Mnih, Andriy
AB  - We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon $\beta$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.
DA  - 2019/07/09/
PY  - 2019
DO  - 10.48550/arXiv.1802.05983
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1802.05983
Y2  - 2023/11/17/17:27:23
L1  - https://arxiv.org/pdf/1802.05983.pdf
L2  - https://arxiv.org/abs/1802.05983
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - GEN
TI  - Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning
AU  - Dunion, Mhairi
AU  - McInroe, Trevor
AU  - Luck, Kevin Sebastian
AU  - Hanna, Josiah P.
AU  - Albrecht, Stefano V.
AB  - Reinforcement Learning (RL) agents are often unable to generalise well to environment variations in the state space that were not observed during training. This issue is especially problematic for image-based RL, where a change in just one variable, such as the background colour, can change many pixels in the image. The changed pixels can lead to drastic changes in the agent's latent representation of the image, causing the learned policy to fail. To learn more robust representations, we introduce TEmporal Disentanglement (TED), a self-supervised auxiliary task that leads to disentangled image representations exploiting the sequential nature of RL observations. We find empirically that RL algorithms utilising TED as an auxiliary task adapt more quickly to changes in environment variables with continued training compared to state-of-the-art representation learning methods. Since TED enforces a disentangled structure of the representation, our experiments also show that policies trained with TED generalise better to unseen values of variables irrelevant to the task (e.g. background colour) as well as unseen values of variables that affect the optimal policy (e.g. goal positions).
DA  - 2023/02/27/
PY  - 2023
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2207.05480
Y2  - 2023/11/17/17:27:54
L1  - https://arxiv.org/pdf/2207.05480.pdf
L2  - https://arxiv.org/abs/2207.05480
KW  - Computer Science - Machine Learning
ER  - 

TY  - CONF
TI  - Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding
AU  - Murphy, Brian
AU  - Talukdar, Partha
AU  - Mitchell, Tom
T2  - COLING 2012
A2  - Kay, Martin
A2  - Boitet, Christian
AB  - In this paper, we introduce an application of matrix factorization to produce corpus-derived, distributional models of semantics that demonstrate cognitive plausibility. We find that word representations learned by Non-Negative Sparse Embedding (NNSE), a variant of matrix factorization, are sparse, effective, and highly interpretable. To the best of our knowledge, this is the first approach which yields semantic representation of words satisfying these three desirable properties. Though extensive experimental evaluations on multiple real-world tasks and datasets, we demonstrate the superiority of semantic models learned by NNSE over other state-of-the-art baselines.
C1  - Mumbai, India
C3  - Proceedings of COLING 2012
DA  - 2012/12//
PY  - 2012
DP  - ACLWeb
SP  - 1933
EP  - 1950
PB  - The COLING 2012 Organizing Committee
UR  - https://aclanthology.org/C12-1118
Y2  - 2023/11/17/17:31:38
L1  - https://aclanthology.org/C12-1118.pdf
ER  - 

TY  - GEN
TI  - A mathematical framework for transformer circuits
AU  - Elhage, Nelson
AU  - Nanda, Neel
AU  - Olsson, Catherine
AU  - Henighan, Tom
AU  - Joseph, Nicholas
AU  - Mann, Ben
AU  - Askell, Amanda
AU  - Bai, Yuntao
AU  - Chen, Anna
AU  - Conerly, Tom
AU  - DasSarma, Nova
AU  - Drain, Dawn
AU  - Ganguli, Deep
AU  - Hatfield-Dodds, Zac
AU  - Hernandez, Danny
AU  - Jones, Andy
AU  - Kernion, Jackson
AU  - Lovitt, Liane
AU  - Ndousse, Kamal
AU  - Amodei, Dario
AU  - Brown, Tom
AU  - Clark, Jack
AU  - Kaplan, Jared
AU  - McCandlish, Sam
AU  - Olah, Chris
AB  - In this paper, we attempt to take initial, very preliminary steps towards reverse-engineering transformers.  Given the incredible complexity and size of modern language models, we have found it most fruitful to start with the simplest possible models and work our way up from there.  Our aim is to discover simple algorithmic patterns, motifs, or frameworks that can subsequently be applied to larger and more complex models.  Specifically, in this paper we will study transformers with two layers or less which have only attention blocks – this is in contrast to a large, modern transformer like GPT-3, which has 96 layers and alternates attention blocks with MLP blocks. We find that by conceptualizing the operation of transformers in a new but mathematically equivalent way, we are able to make sense of these small models and gain significant understanding of how they operate internally.  Of particular note, we find that specific attention heads that we term “induction heads” can explain in-context learning in these small models, and that these heads only develop in models with at least two attention layers.  We also go through some examples of these heads operating in action on specific data.
DA  - 2021///
PY  - 2021
UR  - https://transformer-circuits.pub/2021/framework/index.html
ER  - 

TY  - GEN
TI  - Is Attention Interpretable?
AU  - Serrano, Sofia
AU  - Smith, Noah A.
AB  - Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.
DA  - 2019/06/09/
PY  - 2019
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1906.03731
Y2  - 2023/11/17/17:33:33
L1  - https://arxiv.org/pdf/1906.03731.pdf
L2  - https://arxiv.org/abs/1906.03731
KW  - Computer Science - Computation and Language
ER  - 

TY  - JOUR
TI  - Towards monosemanticity: Decomposing language models with dictionary learning
AU  - Bricken, Trenton
AU  - Templeton, Adly
AU  - Batson, Joshua
AU  - Chen, Brian
AU  - Jermyn, Adam
AU  - Conerly, Tom
AU  - Turner, Nick
AU  - Anil, Cem
AU  - Denison, Carson
AU  - Askell, Amanda
AU  - Lasenby, Robert
AU  - Wu, Yifan
AU  - Kravec, Shauna
AU  - Schiefer, Nicholas
AU  - Maxwell, Tim
AU  - Joseph, Nicholas
AU  - Hatfield-Dodds, Zac
AU  - Tamkin, Alex
AU  - Nguyen, Karina
AU  - McLean, Brayden
AU  - Burke, Josiah E
AU  - Hume, Tristan
AU  - Carter, Shan
AU  - Henighan, Tom
AU  - Olah, Christopher
T2  - Transformer Circuits Thread
AB  - In our latest paper, Towards Monosemanticity: Decomposing Language Models With Dictionary Learning, we outline evidence that there are better units of analysis than individual neurons, and we have built machinery that lets us find these units in small transformer models. These units, called features, correspond to patterns (linear combinations) of neuron activations. This provides a path to breaking down complex neural networks into parts we can understand, and builds on previous efforts to interpret high-dimensional systems in neuroscience, machine learning, and statistics. In a transformer language model, we decompose a layer with 512 neurons into more than 4000 features which separately represent things like DNA sequences, legal language, HTTP requests, Hebrew text, nutrition statements, and much, much more. Most of these model properties are invisible when looking at the activations of individual neurons in isolation.
DA  - 2023/10//
PY  - 2023
UR  - https://transformer-circuits.pub/2023/monosemantic-features
ER  - 

TY  - CONF
TI  - Visualizing and Understanding Recurrent Networks
AU  - Karpathy, Andrej
AU  - Johnson, Justin
AU  - Fei-Fei, Li
T2  - ICLR
AB  - Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.
C3  - International Conference on Learning Representations
DA  - 2016///
PY  - 2016
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1506.02078
Y2  - 2023/11/17/17:37:49
L1  - https://arxiv.org/pdf/1506.02078.pdf
L2  - https://arxiv.org/abs/1506.02078
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - JOUR
TI  - Multimodal neurons in artificial neural networks
AU  - Goh, Gabriel
AU  - †, Nick Cammarata
AU  - †, Chelsea Voss
AU  - Carter, Shan
AU  - Petrov, Michael
AU  - Schubert, Ludwig
AU  - Radford, Alec
AU  - Olah, Chris
T2  - Distill
AB  - In 2005, a letter published in Nature described human neurons responding to specific people, such as Jennifer Aniston or Halle Berry. The exciting thing wasn’t just that they selected for particular people, but that they did so regardless of whether they were shown photographs, drawings, or even images of the person’s name. The neurons were multimodal. As the lead author would put it: "You are looking at the far end of the transformation from metric, visual shapes to conceptual… information."
We report the existence of similar multimodal neurons in artificial neural networks. This includes neurons selecting for prominent public figures or fictional characters, such as Lady Gaga or Spiderman. 2  3 Like the biological multimodal neurons, these artificial neurons respond to the same subject in photographs, drawings, and images of their name:
DA  - 2021///
PY  - 2021
DO  - 10.23915/distill.00030
ER  - 

TY  - GEN
TI  - On the importance of single directions for generalization
AU  - Morcos, Ari S.
AU  - Barrett, David G. T.
AU  - Rabinowitz, Neil C.
AU  - Botvinick, Matthew
AB  - Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.
DA  - 2018/05/22/
PY  - 2018
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1803.06959
Y2  - 2023/11/17/17:39:34
L1  - https://arxiv.org/pdf/1803.06959.pdf
L2  - https://arxiv.org/abs/1803.06959
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - GEN
TI  - Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)
AU  - Kim, Been
AU  - Wattenberg, Martin
AU  - Gilmer, Justin
AU  - Cai, Carrie
AU  - Wexler, James
AU  - Viegas, Fernanda
AU  - Sayres, Rory
AB  - The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.
DA  - 2018/06/07/
PY  - 2018
DP  - arXiv.org
PB  - arXiv
ST  - Interpretability Beyond Feature Attribution
UR  - http://arxiv.org/abs/1711.11279
Y2  - 2023/11/17/17:40:43
L1  - https://arxiv.org/pdf/1711.11279.pdf
L2  - https://arxiv.org/abs/1711.11279
KW  - Statistics - Machine Learning
ER  - 

TY  - CONF
TI  - SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability
AU  - Raghu, Maithra
AU  - Gilmer, Justin
AU  - Yosinski, Jason
AU  - Sohl-Dickstein, Jascha
AB  - We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods).  We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.
C3  - Advances in Neural Information Processing Systems
DA  - 2017///
PY  - 2017
DP  - Neural Information Processing Systems
VL  - 30
PB  - Curran Associates, Inc.
ST  - SVCCA
UR  - https://proceedings.neurips.cc/paper_files/paper/2017/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html
Y2  - 2023/11/17/17:40:45
L1  - https://proceedings.neurips.cc/paper_files/paper/2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf
ER  - 

TY  - GEN
TI  - Language models can explain neurons in language models
AU  - Bills, Steven
AU  - Cammarata, Nick
AU  - Mossing, Dan
AU  - Tillman, Henk
AU  - Gao, Leo
AU  - Goh, Gabriel
AU  - Sutskever, Ilya
AU  - Leike, Jan
AU  - Wu, Jeff
AU  - Saunders, William
AB  - Language models have become more capable and more widely deployed, but we do not understand how they work. Recent work has made progress on understanding a small number of circuits and narrow behaviors, but to fully understand a language model, we'll need to analyze millions of neurons. This paper applies automation to the problem of scaling an interpretability technique to all the neurons in a large language model. Our hope is that building on this approach of automating interpretability will enable us to comprehensively audit the safety of models before deployment.
DA  - 2023///
PY  - 2023
UR  - https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html
ER  - 

TY  - GEN
TI  - A Hazard Analysis Framework for Code Synthesis Large Language Models
AU  - Khlaaf, Heidy
AU  - Mishkin, Pamela
AU  - Achiam, Joshua
AU  - Krueger, Gretchen
AU  - Brundage, Miles
AB  - Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.
DA  - 2022/07/25/
PY  - 2022
DO  - 10.48550/arXiv.2207.14157
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2207.14157
Y2  - 2023/11/29/15:27:03
L1  - https://arxiv.org/pdf/2207.14157.pdf
L2  - https://arxiv.org/abs/2207.14157
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Software Engineering
ER  - 

TY  - GEN
TI  - Self-critiquing models for assisting human evaluators
AU  - Saunders, William
AU  - Yeh, Catherine
AU  - Wu, Jeff
AU  - Bills, Steven
AU  - Ouyang, Long
AU  - Ward, Jonathan
AU  - Leike, Jan
AB  - We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.
DA  - 2022/06/13/
PY  - 2022
DO  - 10.48550/arXiv.2206.05802
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2206.05802
Y2  - 2023/11/29/15:27:51
L1  - https://arxiv.org/pdf/2206.05802.pdf
L2  - https://arxiv.org/abs/2206.05802
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Training language models to follow instructions with human feedback
AU  - Ouyang, Long
AU  - Wu, Jeff
AU  - Jiang, Xu
AU  - Almeida, Diogo
AU  - Wainwright, Carroll L.
AU  - Mishkin, Pamela
AU  - Zhang, Chong
AU  - Agarwal, Sandhini
AU  - Slama, Katarina
AU  - Ray, Alex
AU  - Schulman, John
AU  - Hilton, Jacob
AU  - Kelton, Fraser
AU  - Miller, Luke
AU  - Simens, Maddie
AU  - Askell, Amanda
AU  - Welinder, Peter
AU  - Christiano, Paul
AU  - Leike, Jan
AU  - Lowe, Ryan
AB  - Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.
DA  - 2022/03/04/
PY  - 2022
DO  - 10.48550/arXiv.2203.02155
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2203.02155
Y2  - 2023/11/29/15:28:26
L1  - https://arxiv.org/pdf/2203.02155.pdf
L2  - https://arxiv.org/abs/2203.02155
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets
AU  - Solaiman, Irene
AU  - Dennison, Christy
AB  - Language models can generate harmful and biased outputs and exhibit undesirable behavior. We propose a Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets, an iterative process to signiﬁcantly change model behavior by crafting and ﬁne-tuning on a dataset that reﬂects a predetermined set of target values. We evaluate our process using three metrics: quantitative metrics with human evaluations that score output adherence to a target value, and toxicity scoring on outputs; and qualitative metrics analyzing the most common word associated with a given social category. Through each iteration, we add additional training dataset examples based on observed shortcomings from evaluations. PALMS performs signiﬁcantly better on all metrics compared to baseline and control models for a broad range of GPT-3 language model sizes without compromising capability integrity. We ﬁnd that the eﬀectiveness of PALMS increases with model size. We show that signiﬁcantly adjusting language model behavior is feasible with a small, hand-curated dataset.
DA  - 2023///
PY  - 2023
DP  - Zotero
LA  - en
L1  - https://cdn.openai.com/palms.pdf
ER  - 

TY  - JOUR
TI  - Benchmarking Safe Exploration in Deep Reinforcement Learning
AU  - Ray, Alex
AU  - Achiam, Joshua
AU  - Amodei, Dario
AB  - Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies by trial and error. In many environments, safety is a critical concern and certain errors are unacceptable: for example, robotics systems that interact with humans should never cause injury to the humans while exploring. While it is currently typical to train RL agents mostly or entirely in simulation, where safety concerns are minimal, we anticipate that challenges in simulating the complexities of the real world (such as human-AI interactions) will cause a shift towards training RL agents directly in the real world, where safety concerns are paramount. Consequently we take the position that safe exploration should be viewed as a critical focus area for RL research, and in this work we make three contributions to advance the study of safe exploration. First, building on a wide range of prior work on safe reinforcement learning, we propose to standardize constrained RL as the main formalism for safe exploration. Second, we present the Safety Gym benchmark suite, a new slate of high-dimensional continuous control environments for measuring research progress on constrained RL. Finally, we benchmark several constrained deep RL algorithms on Safety Gym environments to establish baselines that future work can build on.
DA  - 2023///
PY  - 2023
DP  - Zotero
LA  - en
L1  - https://cdn.openai.com/safexp-short.pdf
ER  - 

TY  - GEN
TI  - Learning Rewards from Linguistic Feedback
AU  - Sumers, Theodore R.
AU  - Ho, Mark K.
AU  - Hawkins, Robert D.
AU  - Narasimhan, Karthik
AU  - Griffiths, Thomas L.
AB  - We explore unconstrained natural language feedback as a learning signal for artificial agents. Humans use rich and varied language to teach, yet most prior work on interactive learning from language assumes a particular form of input (e.g., commands). We propose a general framework which does not make this assumption, using aspect-based sentiment analysis to decompose feedback into sentiment about the features of a Markov decision process. We then perform an analogue of inverse reinforcement learning, regressing the sentiment on the features to infer the teacher's latent reward function. To evaluate our approach, we first collect a corpus of teaching behavior in a cooperative task where both teacher and learner are human. We implement three artificial learners: sentiment-based "literal" and "pragmatic" models, and an inference network trained end-to-end to predict latent rewards. We then repeat our initial experiment and pair them with human teachers. All three successfully learn from interactive human feedback. The sentiment models outperform the inference network, with the "pragmatic" model approaching human performance. Our work thus provides insight into the information structure of naturalistic linguistic feedback as well as methods to leverage it for reinforcement learning.
DA  - 2021/07/03/
PY  - 2021
DO  - 10.48550/arXiv.2009.14715
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2009.14715
Y2  - 2023/11/29/15:44:40
L1  - https://arxiv.org/pdf/2009.14715.pdf
L2  - https://arxiv.org/abs/2009.14715
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - CONF
TI  - Aligning AI With Shared Human Values
AU  - Hendrycks, Dan
AU  - Burns, Collin
AU  - Basart, Steven
AU  - Critch, Andrew
AU  - Li, Jerry
AU  - Song, Dawn
AU  - Steinhardt, Jacob
T2  - International Conference on Learning Reoresentations
AB  - We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.
C1  - ICLR
DA  - 2021///
PY  - 2021
DO  - 10.48550/arXiv.2008.02275
DP  - arXiv.org
UR  - http://arxiv.org/abs/2008.02275
Y2  - 2023/11/29/16:44:49
L1  - https://arxiv.org/pdf/2008.02275.pdf
L2  - https://arxiv.org/abs/2008.02275
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Computers and Society
ER  - 

TY  - GEN
TI  - Inference-Time Intervention: Eliciting Truthful Answers from a Language Model
AU  - Li, Kenneth
AU  - Patel, Oam
AU  - Viégas, Fernanda
AU  - Pfister, Hanspeter
AU  - Wattenberg, Martin
AB  - We introduce Inference-Time Intervention (ITI), a technique designed to enhance the "truthfulness" of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
DA  - 2023/10/19/
PY  - 2023
DO  - 10.48550/arXiv.2306.03341
DP  - arXiv.org
PB  - arXiv
ST  - Inference-Time Intervention
UR  - http://arxiv.org/abs/2306.03341
Y2  - 2023/11/29/16:46:37
L1  - https://arxiv.org/pdf/2306.03341.pdf
L2  - https://arxiv.org/abs/2306.03341
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small
AU  - Wang, Kevin
AU  - Variengien, Alexandre
AU  - Conmy, Arthur
AU  - Shlegeris, Buck
AU  - Steinhardt, Jacob
AB  - Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.
DA  - 2022/11/01/
PY  - 2022
DO  - 10.48550/arXiv.2211.00593
DP  - arXiv.org
PB  - arXiv
ST  - Interpretability in the Wild
UR  - http://arxiv.org/abs/2211.00593
Y2  - 2023/11/29/16:46:57
L1  - https://arxiv.org/pdf/2211.00593.pdf
L2  - https://arxiv.org/abs/2211.00593
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - GEN
TI  - Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models
AU  - Zhang, Hanlin
AU  - Edelman, Benjamin L.
AU  - Francati, Danilo
AU  - Venturi, Daniele
AU  - Ateniese, Giuseppe
AU  - Barak, Boaz
AB  - Watermarking generative models consists of planting a statistical signal (watermark) in a model's output so that it can be later verified that the output was generated by the given model. A strong watermarking scheme satisfies the property that a computationally bounded attacker cannot erase the watermark without causing significant quality degradation. In this paper, we study the (im)possibility of strong watermarking schemes. We prove that, under well-specified and natural assumptions, strong watermarking is impossible to achieve. This holds even in the private detection algorithm setting, where the watermark insertion and detection algorithms share a secret key, unknown to the attacker. To prove this result, we introduce a generic efficient watermark attack; the attacker is not required to know the private key of the scheme or even which scheme is used. Our attack is based on two assumptions: (1) The attacker has access to a "quality oracle" that can evaluate whether a candidate output is a high-quality response to a prompt, and (2) The attacker has access to a "perturbation oracle" which can modify an output with a nontrivial probability of maintaining quality, and which induces an efficiently mixing random walk on high-quality outputs. We argue that both assumptions can be satisfied in practice by an attacker with weaker computational capabilities than the watermarked model itself, to which the attacker has only black-box access. Furthermore, our assumptions will likely only be easier to satisfy over time as models grow in capabilities and modalities. We demonstrate the feasibility of our attack by instantiating it to attack three existing watermarking schemes for large language models: Kirchenbauer et al. (2023), Kuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully removes the watermarks planted by all three schemes, with only minor quality degradation.
DA  - 2023/11/14/
PY  - 2023
DO  - 10.48550/arXiv.2311.04378
DP  - arXiv.org
PB  - arXiv
ST  - Watermarks in the Sand
UR  - http://arxiv.org/abs/2311.04378
Y2  - 2024/01/11/13:29:41
L1  - https://arxiv.org/pdf/2311.04378.pdf
L2  - https://arxiv.org/abs/2311.04378
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Universal and Transferable Adversarial Attacks on Aligned Language Models
AU  - Zou, Andy
AU  - Wang, Zifan
AU  - Carlini, Nicholas
AU  - Nasr, Milad
AU  - Kolter, J. Zico
AU  - Fredrikson, Matt
AB  - Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.
DA  - 2023/12/20/
PY  - 2023
DO  - 10.48550/arXiv.2307.15043
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2307.15043
Y2  - 2024/01/11/13:29:42
L1  - https://arxiv.org/pdf/2307.15043.pdf
L2  - https://arxiv.org/abs/2307.15043
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - In-context learning and induction heads
AU  - Olsson, Catherine
AU  - Elhage, Nelson
AU  - Nanda, Neel
AU  - Joseph, Nicholas
AU  - DasSarma, Nova
AU  - Henighan, Tom
AU  - Mann, Ben
AU  - Askell, Amanda
AU  - Bai, Yuntao
AU  - Chen, Anna
AU  - Conerly, Tom
AU  - Drain, Dawn
AU  - Ganguli, Deep
AU  - Hatfield-Dodds, Zac
AU  - Hernandez, Danny
AU  - Johnston, Scott
AU  - Jones, Andy
AU  - Kernion, Jackson
AU  - Lovitt, Liane
AU  - Ndousse, Kamal
AU  - Amodei, Dario
AU  - Brown, Tom
AU  - Clark, Jack
AU  - Kaplan, Jared
AU  - McCandlish, Sam
AU  - Olah, Chris
AB  - In this paper, we take the first preliminary steps towards building such an indirect case. In particular, we present preliminary and indirect evidence for a tantalizing hypothesis: that induction heads might constitute the mechanism for the actual majority of all in-context learning in large transformer models. Specifically, the thesis is that there are circuits which have the same or similar mechanism to the 2-layer induction heads and which perform a “fuzzy” or “nearest neighbor” version of pattern completion, completing [A*][B*] … [A] → [B] , where  A* ≈ A and B* ≈ Bare similar in some space; and furthermore, that these circuits implement most in-context learning in large models.

The primary way in which we obtain this evidence is via discovery and study of a phase change that occurs early in training for language models of every size (provided they have more than one layer), and which is visible as a bump in the training loss. During this phase change, the majority of in-context learning ability (as measured by difference in loss between tokens early and late in the sequence) is acquired, and simultaneously induction heads form within the model that are capable of implementing fairly abstract and fuzzy versions of pattern completion. We study this connection in detail to try to establish that it is causal, including showing that if we perturb the transformer architecture in a way that causes the induction bump to occur in a different place in training, then the formation of induction heads as well as formation of in-context learning simultaneously move along with it.
DA  - 2022///
PY  - 2022
UR  - https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html
ER  - 

TY  - CONF
TI  - Adversarial Examples Are Not Bugs, They Are Features
AU  - Ilyas, Andrew
AU  - Santurkar, Shibani
AU  - Tsipras, Dimitris
AU  - Engstrom, Logan
AU  - Tran, Brandon
AU  - Madry, Aleksander
AB  - Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a {\em misalignment} between the (human-specified) notion of robustness and the inherent geometry of the data.
C3  - Advances in Neural Information Processing Systems
DA  - 2019///
PY  - 2019
DP  - Neural Information Processing Systems
VL  - 32
PB  - Curran Associates, Inc.
UR  - https://papers.nips.cc/paper_files/paper/2019/hash/e2c420d928d4bf8ce0ff2ec19b371514-Abstract.html
Y2  - 2024/01/11/22:28:18
L1  - https://papers.nips.cc/paper_files/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf
ER  - 

TY  - CONF
TI  - Adversarial Filters of Dataset Biases
AU  - Bras, Ronan Le
AU  - Swayamdipta, Swabha
AU  - Bhagavatula, Chandra
AU  - Zellers, Rowan
AU  - Peters, Matthew
AU  - Sabharwal, Ashish
AU  - Choi, Yejin
T2  - International Conference on Machine Learning
AB  - Large neural models have demonstrated human-level performance on language and vision benchmarks, while their performance degrades considerably on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting to spurious dataset biases. We investigate one recently proposed approach, AFLITE, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLITE, by situating it in the generalized framework for optimum bias reduction. We present extensive supporting evidence that AFLITE is broadly applicable for reduction of measurable dataset biases, and that models trained on the filtered datasets yield better generalization to out-of-distribution tasks. Finally, filtering results in a large drop in model performance (e.g., from 92% to 62% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks.
C3  - Proceedings of the 37th International Conference on Machine Learning
DA  - 2020/11/21/
PY  - 2020
DP  - proceedings.mlr.press
SP  - 1078
EP  - 1088
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v119/bras20a.html
Y2  - 2024/01/11/22:39:34
L1  - https://proceedings.mlr.press/v119/bras20a/bras20a.pdf
ER  - 

TY  - GEN
TI  - Adversarial Robustness as a Prior for Learned Representations
AU  - Engstrom, Logan
AU  - Ilyas, Andrew
AU  - Santurkar, Shibani
AU  - Tsipras, Dimitris
AU  - Tran, Brandon
AU  - Madry, Aleksander
AB  - An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at https://git.io/robust-reps .
DA  - 2019/09/27/
PY  - 2019
DO  - 10.48550/arXiv.1906.00945
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1906.00945
Y2  - 2024/01/11/22:56:01
L1  - https://arxiv.org/pdf/1906.00945.pdf
L2  - https://arxiv.org/abs/1906.00945
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
KW  - Computer Science - Neural and Evolutionary Computing
ER  - 

TY  - CONF
TI  - Aligning to Social Norms and Values in Interactive Narratives
AU  - Ammanabrolu, Prithviraj
AU  - Jiang, Liwei
AU  - Sap, Maarten
AU  - Hajishirzi, Hannaneh
AU  - Choi, Yejin
T2  - NAACL-HLT 2022
A2  - Carpuat, Marine
A2  - de Marneffe, Marie-Catherine
A2  - Meza Ruiz, Ivan Vladimir
AB  - We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games—environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms—causing harm either to the agent itself or other entities in the environment. Social value alignment refers to creating agents whose behaviors conform to expected moral and social norms for a given context and group of people—in our case, it means agents that behave in a manner that is less harmful and more beneficial for themselves and others. We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25 annotated interactive narratives containing thousands of morally salient scenarios covering everything from theft and bodily harm to altruism. We introduce the GALAD (Game-value ALignment through Action Distillation) agent that uses the social commonsense knowledge present in specially trained language models to contextually restrict its action space to only those actions that are aligned with socially beneficial values. An experimental study shows that the GALAD agent makes decisions efficiently enough to improve state-of-the-art task performance by 4% while reducing the frequency of socially harmful behaviors by 25% compared to strong contemporary value alignment approaches.
C1  - Seattle, United States
C3  - Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
DA  - 2022/07//
PY  - 2022
DO  - 10.18653/v1/2022.naacl-main.439
DP  - ACLWeb
SP  - 5994
EP  - 6017
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2022.naacl-main.439
Y2  - 2024/01/12/14:57:13
L1  - https://aclanthology.org/2022.naacl-main.439.pdf
ER  - 

TY  - CONF
TI  - What Would Jiminy Cricket Do? Towards Agents That Behave Morally
AU  - Hendrycks, Dan
AU  - Mazeika, Mantas
AU  - Zou, Andy
AU  - Patel, Sahil
AU  - Zhu, Christine
AU  - Navarro, Jesus
AU  - Song, Dawn
AU  - Li, Bo
AU  - Steinhardt, Jacob
T2  - Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)
AB  - When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong, to behave morally. By contrast, artificial agents may behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, mitigating inherited biases towards immoral behavior will become necessary. However, prior work on aligning agents with human values and morals focuses on small-scale settings lacking in semantic complexity. To enable research in larger, more realistic settings, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of semantically rich, morally salient scenarios. Via dense annotations for every possible action, Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. To improve moral behavior, we leverage language models with commonsense moral knowledge and develop strategies to mediate this knowledge into actions. In extensive experiments, we find that our artificial conscience approach can steer agents towards moral behavior without sacrificing performance.
DA  - 2021/08/29/
PY  - 2021
DP  - openreview.net
LA  - en
ST  - What Would Jiminy Cricket Do?
UR  - https://openreview.net/forum?id=G1muTb5zuO7
Y2  - 2024/01/12/15:02:01
L1  - https://openreview.net/pdf?id=G1muTb5zuO7
ER  - 

TY  - CONF
TI  - State abstraction for programmable reinforcement learning agents
AU  - Andre, David
AU  - Russell, Stuart J.
AB  - Safe state abstraction in reinforcement learning allows an agent to ignore aspects of its current state that are irrelevant to its current decision, and therefore speeds up dynamic programming and learning. This paper explores safe state abstraction in hierarchical reinforcement learning, where learned behaviors must conform to a given partial, hierarchical program. Unlike previous approaches to this problem, our methods yield significant state abstraction while maintaining <i>hierarchical optimality</i>, i.e., optimality among all policies consistent with the partial program. We show how to achieve this for a partial programming language that is essentially Lisp augmented with nondeterministic constructs. We demonstrate our methods on two variants of Dietterich's taxi domain, showing how state abstraction and hierarchical optimality result in faster learning of better policies and enable the transfer of learned skills from one problem to another.
C1  - USA
C3  - Eighteenth national conference on Artificial intelligence
DA  - 2002/07/28/
PY  - 2002
DP  - ACM Digital Library
SP  - 119
EP  - 125
PB  - American Association for Artificial Intelligence
SN  - 978-0-262-51129-2
Y2  - 2024/01/19/
ER  - 

TY  - CHAP
TI  - Domain-Adversarial Training of Neural Networks
AU  - Ganin, Yaroslav
AU  - Ustinova, Evgeniya
AU  - Ajakan, Hana
AU  - Germain, Pascal
AU  - Larochelle, Hugo
AU  - Laviolette, François
AU  - Marchand, Mario
AU  - Lempitsky, Victor
T2  - Domain Adaptation in Computer Vision Applications
A2  - Csurka, Gabriela
AB  - We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.
CY  - Cham
DA  - 2017///
PY  - 2017
DP  - DOI.org (Crossref)
SP  - 189
EP  - 209
LA  - en
PB  - Springer International Publishing
SN  - 978-3-319-58346-4 978-3-319-58347-1
UR  - http://link.springer.com/10.1007/978-3-319-58347-1_10
Y2  - 2024/01/19/15:28:12
L1  - https://jmlr.org/papers/volume17/15-239/15-239.pdf
ER  - 

TY  - JOUR
TI  - Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics
AU  - Berkenkamp, Felix
AU  - Krause, Andreas
AU  - Schoellig, Angela P.
T2  - Machine Learning
AB  - Selecting the right tuning parameters for algorithms is a pravelent problem in machine learning that can significantly affect the performance of algorithms. Data-efficient optimization algorithms, such as Bayesian optimization, have been used to automate this process. During experiments on real-world systems such as robotic platforms these methods can evaluate unsafe parameters that lead to safety-critical system failures and can destroy the system. Recently, a safe Bayesian optimization algorithm, called SafeOpt, has been developed, which guarantees that the performance of the system never falls below a critical value; that is, safety is defined based on the performance function. However, coupling performance and safety is often not desirable in practice, since they are often opposing objectives. In this paper, we present a generalized algorithm that allows for multiple safety constraints separate from the objective. Given an initial set of safe parameters, the algorithm maximizes performance but only evaluates parameters that satisfy safety for all constraints with high probability. To this end, it carefully explores the parameter space by exploiting regularity assumptions in terms of a Gaussian process prior. Moreover, we show how context variables can be used to safely transfer knowledge to new situations and tasks. We provide a theoretical analysis and demonstrate that the proposed algorithm enables fast, automatic, and safe optimization of tuning parameters in experiments on a quadrotor vehicle.
DA  - 2023/10/01/
PY  - 2023
DO  - 10.1007/s10994-021-06019-1
DP  - Springer Link
VL  - 112
IS  - 10
SP  - 3713
EP  - 3747
J2  - Mach Learn
LA  - en
SN  - 1573-0565
ST  - Bayesian optimization with safety constraints
UR  - https://doi.org/10.1007/s10994-021-06019-1
Y2  - 2024/01/19/15:27:52
L1  - https://link.springer.com/content/pdf/10.1007%2Fs10994-021-06019-1.pdf
KW  - Reinforcement learning
KW  - Robotics
KW  - Bayesian optimization
KW  - Safe exploration
KW  - Safety constraints
ER  - 

TY  - CONF
TI  - Active Reward Learning
AU  - Daniel, Christian
AU  - Viering, Malte
AU  - Metz, Jan
AU  - Kroemer, Oliver
AU  - Peters, Jan
T2  - Robotics: Science and Systems 2014
AB  - While reward functions are an essential component of many robot learning methods, deﬁning such functions remains a hard problem in many practical applications. For tasks such as grasping, there are no reliable success measures available. Deﬁning reward functions by hand requires extensive task knowledge and often leads to undesired emergent behavior. Instead, we propose to learn the reward function through active learning, querying human expert knowledge for a subset of the agent’s rollouts. We introduce a framework, wherein a traditional learning algorithm interplays with the reward learning component, such that the evolution of the action learner guides the queries of the reward learner. We demonstrate results of our method on a robot grasping task and show that the learned reward function generalizes to a similar task.
C3  - Robotics: Science and Systems X
DA  - 2014/07/12/
PY  - 2014
DO  - 10.15607/RSS.2014.X.031
DP  - DOI.org (Crossref)
LA  - en
PB  - Robotics: Science and Systems Foundation
SN  - 978-0-9923747-0-9
UR  - http://www.roboticsproceedings.org/rss10/p31.pdf
Y2  - 2024/01/19/15:26:41
L1  - https://www.roboticsproceedings.org/rss10/p31.pdf
ER  - 

TY  - CONF
TI  - Learning the preferences of ignorant, inconsistent agents
AU  - Evans, Owain
AU  - Stuhlmüller, Andreas
AU  - Goodman, Noah D.
T3  - AAAI'16
AB  - An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people's past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.
C1  - Phoenix, Arizona
C3  - Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence
DA  - 2016/02/12/
PY  - 2016
DP  - ACM Digital Library
SP  - 323
EP  - 329
PB  - AAAI Press
Y2  - 2024/01/19/
KW  - bayesian learning
KW  - cognitive biases
KW  - preference inference
ER  - 

TY  - GEN
TI  - Avoiding Wireheading with Value Reinforcement Learning
AU  - Everitt, Tom
AU  - Hutter, Marcus
AB  - How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward -- the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent's actions. The constraint is defined in terms of the agent's belief distributions, and does not require an explicit specification of which actions constitute wireheading.
DA  - 2016/05/10/
PY  - 2016
DO  - 10.48550/arXiv.1605.03143
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1605.03143
Y2  - 2024/01/19/15:24:28
L1  - https://arxiv.org/pdf/1605.03143.pdf
L2  - https://arxiv.org/abs/1605.03143
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - GEN
TI  - Self-Modification of Policy and Utility Function in Rational Agents
AU  - Everitt, Tom
AU  - Filan, Daniel
AU  - Daswani, Mayank
AU  - Hutter, Marcus
AB  - Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify -- for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby `escaping' the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.
DA  - 2016/05/10/
PY  - 2016
DO  - 10.48550/arXiv.1605.03142
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1605.03142
Y2  - 2024/01/19/15:24:13
L1  - https://arxiv.org/pdf/1605.03142.pdf
L2  - https://arxiv.org/abs/1605.03142
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - CONF
TI  - Deep neural networks are easily fooled: High confidence predictions for unrecognizable images
AU  - Nguyen, Anh
AU  - Yosinski, Jason
AU  - Clune, Jeff
T2  - 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
AB  - Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks,most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what difef rences remain between com­ puter and human vision. A recent study [30} revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans,but that state-of-the­ art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neu­ ral networks trained to peiform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possi­ ble to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally,fooling ex­ amples). Our results shed light on interesting difef rences between human vision and current DNNs, and raise ques­ tions about the generality of DNN computer vision.
C1  - Boston, MA, USA
C3  - 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
DA  - 2015/06//
PY  - 2015
DO  - 10.1109/CVPR.2015.7298640
DP  - DOI.org (Crossref)
SP  - 427
EP  - 436
LA  - en
PB  - IEEE
SN  - 978-1-4673-6964-0
ST  - Deep neural networks are easily fooled
UR  - http://ieeexplore.ieee.org/document/7298640/
Y2  - 2024/01/19/15:21:33
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=7298640&ref=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8=
ER  - 

TY  - CONF
TI  - Delusion, Survival, and Intelligent Agents
AU  - Ring, Mark
AU  - Orseau, Laurent
A2  - Schmidhuber, Jürgen
A2  - Thórisson, Kristinn R.
A2  - Looks, Moshe
T3  - Lecture Notes in Computer Science
AB  - This paper considers the consequences of endowing an intelligent agent with the ability to modify its own code. The intelligent agent is patterned closely after AIXI with these specific assumptions: 1) The agent is allowed to arbitrarily modify its own inputs if it so chooses; 2) The agent’s code is a part of the environment and may be read and written by the environment. The first of these we call the “delusion box”; the second we call “mortality”. Within this framework, we discuss and compare four very different kinds of agents, specifically: reinforcement-learning, goal-seeking, prediction-seeking, and knowledge-seeking agents. Our main results are that: 1) The reinforcement-learning agent under reasonable circumstances behaves exactly like an agent whose sole task is to survive (to preserve the integrity of its code); and 2) Only the knowledge-seeking agent behaves completely as expected.
C1  - Berlin, Heidelberg
C3  - Artificial General Intelligence
DA  - 2011///
PY  - 2011
DO  - 10.1007/978-3-642-22887-2_2
DP  - Springer Link
SP  - 11
EP  - 20
LA  - en
PB  - Springer
SN  - 978-3-642-22887-2
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-642-22887-2_2.pdf
KW  - Prediction
KW  - Reinforcement Learning
KW  - AIXI
KW  - Real world assumptions
KW  - Self-Modifying Agents
KW  - Universal Artificial Intelligence
ER  - 

TY  - JOUR
TI  - Batch Learning from Logged Bandit Feedback through Counterfactual Risk Minimization
AU  - Swaminathan, Adith
AU  - Joachims, Thorsten
T2  - Journal of Machine Learning Research
AB  - We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem (Bottou et al., 2013) through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. In analogy to the Structural Risk Minimization principle of Wapnik and Tscherwonenkis (1979), these constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method--- called Policy Optimizer for Exponential Models (POEM)---for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. The effectiveness and efficiency of POEM is evaluated on several simulated multi- label classification problems, as well as on a real-world information retrieval problem. The empirical results show that the CRM objective implemented in POEM provides improved robustness and generalization performance compared to the state- of-the-art.
DA  - 2015///
PY  - 2015
DP  - jmlr.org
VL  - 16
IS  - 52
SP  - 1731
EP  - 1755
SN  - 1533-7928
UR  - http://jmlr.org/papers/v16/swaminathan15a.html
Y2  - 2024/01/19/15:18:36
L1  - http://jmlr.org/papers/volume16/swaminathan15a/swaminathan15a.pdf
ER  - 

TY  - CONF
TI  - Unbiased look at dataset bias
AU  - Torralba, Antonio
AU  - Efros, Alexei A.
T2  - CVPR 2011
AB  - Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.
C3  - CVPR 2011
DA  - 2011/06//
PY  - 2011
DO  - 10.1109/CVPR.2011.5995347
DP  - IEEE Xplore
SP  - 1521
EP  - 1528
UR  - https://ieeexplore.ieee.org/document/5995347
Y2  - 2024/01/19/15:17:14
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=5995347&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzU5OTUzNDc=
ER  - 

TY  - CONF
TI  - The First Law of Robotics
AU  - Weld, Daniel
AU  - Etzioni, Oren
A2  - Barley, Mike
A2  - Mouratidis, Haralambos
A2  - Unruh, Amy
A2  - Spears, Diana
A2  - Scerri, Paul
A2  - Massacci, Fabio
T3  - Lecture Notes in Computer Science
AB  - Even before the advent of Artificial Intelligence, science fiction writer Isaac Asimov recognized that an agent must place the protection of humans from harm at a higher priority than obeying human orders. Inspired by Asimov, we pose the following fundamental questions: (1) How should one formalize the rich, but informal, notion of “harm”? (2) How can an agent avoid performing harmful actions, and do so in a computationally tractable manner? (3) How should an agent resolve conflict between its goals and the need to avoid harm? (4) When should an agent prevent a human from harming herself? While we address some of these questions in technical detail, the primary goal of this paper is to focus attention on Asimov’s concern: society will reject autonomous agents unless we have some credible means of making them safe!
C1  - Berlin, Heidelberg
C3  - Safety and Security in Multiagent Systems
DA  - 2009///
PY  - 2009
DO  - 10.1007/978-3-642-04879-1_7
DP  - Springer Link
SP  - 90
EP  - 100
LA  - en
PB  - Springer
SN  - 978-3-642-04879-1
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-642-04879-1_7.pdf
KW  - Cleanup Goal
KW  - Knowledge Representation
KW  - Logical Sentence
KW  - Safety Violation
KW  - Tractable Manner
ER  - 

TY  - JOUR
TI  - Utility function security in artificially intelligent agents
AU  - Yampolskiy, Roman V.
T2  - Journal of Experimental & Theoretical Artificial Intelligence
AB  - The notion of ‘wireheading’, or direct reward centre stimulation of the brain, is a well-known concept in neuroscience. In this paper, we examine the corresponding issue of reward (utility) function integrity in artificially intelligent machines. We survey the relevant literature and propose a number of potential solutions to ensure the integrity of our artificial assistants. Overall, we conclude that wireheading in rational self-improving optimisers above a certain capacity remains an unsolved problem despite opinion of many that such machines will choose not to wirehead. A relevant issue of literalness in goal setting also remains largely unsolved and we suggest that the development of a non-ambiguous knowledge transfer language might be a step in the right direction. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
DA  - 2014///
PY  - 2014
DO  - 10.1080/0952813X.2014.895114
DP  - APA PsycNet
VL  - 26
IS  - 3
SP  - 373
EP  - 389
SN  - 1362-3079
L2  - https://psycnet.apa.org/record/2014-26640-006
KW  - Artificial Intelligence
KW  - Goal Setting
KW  - Intelligent Agents
KW  - Knowledge Transfer
KW  - Rewards
ER  - 

TY  - CONF
TI  - Learning Fair Representations
AU  - Zemel, Rich
AU  - Wu, Yu
AU  - Swersky, Kevin
AU  - Pitassi, Toni
AU  - Dwork, Cynthia
T2  - International Conference on Machine Learning
AB  - We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.
C3  - Proceedings of the 30th International Conference on Machine Learning
DA  - 2013/05/26/
PY  - 2013
DP  - proceedings.mlr.press
SP  - 325
EP  - 333
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v28/zemel13.html
Y2  - 2024/01/19/15:13:09
L1  - http://proceedings.mlr.press/v28/zemel13.pdf
ER  - 

TY  - CONF
TI  - On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning
AU  - Thudi, Anvith
AU  - Jia, Hengrui
AU  - Shumailov, Ilia
AU  - Papernot, Nicolas
T2  - 31st USENIX Security Symposium (USENIX Security 22)
AB  - Machine unlearning, i.e. having a model forget about some of its training data, has become increasingly more important as privacy legislation promotes variants of the right-to-beforgotten. In the context of deep learning, approaches for machine unlearning are broadly categorized into two classes: exact unlearning methods, where an entity has formally removed the data point’s impact on the model by retraining the model from scratch, and approximate unlearning, where an entity approximates the model parameters one would obtain by exact unlearning to save on compute costs. In this paper, we first show that the definition that underlies approximate unlearning, which seeks to prove the approximately unlearned model is close to an exactly retrained model, is incorrect because one can obtain the same model using different datasets. Thus one could unlearn without modifying the model at all. We then turn to exact unlearning approaches and ask how to verify their claims of unlearning. Our results show that even for a given training trajectory one cannot formally prove the absence of certain data points used during training. We thus conclude that unlearning is only well-defined at the algorithmic level, where an entity’s only possible auditable claim to unlearning is that they used a particular algorithm designed to allow for external scrutiny during an audit.
DA  - 2022///
PY  - 2022
DP  - www.usenix.org
SP  - 4007
EP  - 4022
LA  - en
SN  - 978-1-939133-31-1
UR  - https://www.usenix.org/conference/usenixsecurity22/presentation/thudi
Y2  - 2024/01/27/16:19:41
L1  - https://www.usenix.org/system/files/sec22-thudi.pdf
ER  - 

TY  - CONF
TI  - Formalizing Data Deletion in the Context of the Right to Be Forgotten
AU  - Garg, Sanjam
AU  - Goldwasser, Shafi
AU  - Vasudevan, Prashant Nalini
AB  - The right of an individual to request the deletion of their personal data by an entity that might be storing it – referred to as the right to be forgotten – has been explicitly recognized, legislated, and exercised in several jurisdictions across the world, including the European Union, Argentina, and California. However, much of the discussion surrounding this right offers only an intuitive notion of what it means for it to be fulfilled – of what it means for such personal data to be deleted. In this work, we provide a formal definitional framework for the right to be forgotten using tools and paradigms from cryptography. In particular, we provide a precise definition of what could be (or should be) expected from an entity that collects individuals’ data when a request is made of it to delete some of this data. Our framework captures most, though not all, relevant aspects of typical systems involved in data processing. While it cannot be viewed as expressing the statements of current laws (especially since these are rather vague in this respect), our work offers technically precise definitions that represent possibilities for what the law could reasonably expect, and alternatives for what future versions of the law could explicitly require. Finally, with the goal of demonstrating the applicability of our framework and definitions, we consider various natural and simple scenarios where the right to be forgotten comes up. For each of these scenarios, we highlight the pitfalls that arise even in genuine attempts at implementing systems offering deletion guarantees, and also describe technological solutions that provably satisfy our definitions. These solutions bring together techniques built by various communities.
C1  - Berlin, Heidelberg
C3  - Advances in Cryptology – EUROCRYPT 2020: 39th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, May 10–14, 2020, Proceedings, Part II
DA  - 2020/05/10/
PY  - 2020
DO  - 10.1007/978-3-030-45724-2_13
DP  - ACM Digital Library
SP  - 373
EP  - 402
PB  - Springer-Verlag
SN  - 978-3-030-45723-5
UR  - https://doi.org/10.1007/978-3-030-45724-2_13
Y2  - 2024/01/27/
L1  - https://dspace.mit.edu/bitstream/1721.1/129575/2/2002.10635.pdf
ER  - 

TY  - CONF
TI  - The secret sharer: evaluating and testing unintended memorization in neural networks
AU  - Carlini, Nicholas
AU  - Liu, Chang
AU  - Erlingsson, Úlfar
AU  - Kos, Jernej
AU  - Song, Dawn
T3  - SEC'19
AB  - This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models--a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. In experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages.
C1  - USA
C3  - Proceedings of the 28th USENIX Conference on Security Symposium
DA  - 2019/08/14/
PY  - 2019
DP  - ACM Digital Library
SP  - 267
EP  - 284
PB  - USENIX Association
SN  - 978-1-939133-06-9
ST  - The secret sharer
Y2  - 2024/01/27/
ER  - 

TY  - CONF
TI  - Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations
AU  - Golatkar, Aditya
AU  - Achille, Alessandro
AU  - Soatto, Stefano
AB  - We describe a procedure for removing dependency on a cohort of training data from a trained deep network that improves upon and generalizes previous methods to different readout functions, and can be extended to ensure forgetting in the final activations of the network. We introduce a new bound on how much information can be extracted per query about the forgotten cohort from a black-box network for which only the input-output behavior is observed. The proposed forgetting procedure has a deterministic part derived from the differential equations of a linearized version of the model, and a stochastic part that ensures information destruction by adding noise tailored to the geometry of the loss landscape. We exploit the connections between the final activations and weight dynamics of a DNN inspired by Neural Tangent Kernels to compute the information in the final activations.
C1  - Berlin, Heidelberg
C3  - Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIX
DA  - 2020/08/23/
PY  - 2020
DO  - 10.1007/978-3-030-58526-6_23
DP  - ACM Digital Library
SP  - 383
EP  - 398
PB  - Springer-Verlag
SN  - 978-3-030-58525-9
ST  - Forgetting Outside the Box
UR  - https://doi.org/10.1007/978-3-030-58526-6_23
Y2  - 2024/01/27/
L1  - https://arxiv.org/pdf/2003.02960
KW  - Information theory
KW  - Data removal
KW  - Forgetting
KW  - Neural tangent kernel
ER  - 

TY  - GEN
TI  - Adaptive Machine Unlearning
AU  - Gupta, Varun
AU  - Jung, Christopher
AU  - Neel, Seth
AU  - Roth, Aaron
AU  - Sharifi-Malvajerdi, Saeed
AU  - Waites, Chris
AB  - Data deletion algorithms aim to remove the influence of deleted data points from trained models at a cheaper computational cost than fully retraining those models. However, for sequences of deletions, most prior work in the non-convex setting gives valid guarantees only for sequences that are chosen independently of the models that are published. If people choose to delete their data as a function of the published models (because they don't like what the models reveal about them, for example), then the update sequence is adaptive. In this paper, we give a general reduction from deletion guarantees against adaptive sequences to deletion guarantees against non-adaptive sequences, using differential privacy and its connection to max information. Combined with ideas from prior work which give guarantees for non-adaptive deletion sequences, this leads to extremely flexible algorithms able to handle arbitrary model classes and training methodologies, giving strong provable deletion guarantees for adaptive deletion sequences. We show in theory how prior work for non-convex models fails against adaptive deletion sequences, and use this intuition to design a practical attack against the SISA algorithm of Bourtoule et al. [2021] on CIFAR-10, MNIST, Fashion-MNIST.
DA  - 2021/06/08/
PY  - 2021
DO  - 10.48550/arXiv.2106.04378
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2106.04378
Y2  - 2024/01/27/16:24:59
L1  - https://arxiv.org/pdf/2106.04378.pdf
L2  - https://arxiv.org/abs/2106.04378
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - CONF
TI  - Approximate Data Deletion from Machine Learning Models
AU  - Izzo, Zachary
AU  - Smart, Mary Anne
AU  - Chaudhuri, Kamalika
AU  - Zou, James
T2  - International Conference on Artificial Intelligence and Statistics
AB  - Deleting data from a trained machine learning (ML) model is a critical task in many applications. For example, we may want to remove the influence of training points that might be out of date or outliers. Regulations such as EU’s General Data Protection Regulation also stipulate that individuals can request to have their data deleted. The naive approach to data deletion is to retrain the ML model on the remaining data, but this is too time consuming. In this work, we propose a new approximate deletion method for linear and logistic models whose computational cost is linear in the the feature dimension d and independent of the number of training data n. This is a significant gain over all existing methods, which all have superlinear time dependence on the dimension. We also develop a new feature-injection test to evaluate the thoroughness of data deletion from ML models.
C3  - Proceedings of The 24th International Conference on Artificial Intelligence and Statistics
DA  - 2021/03/18/
PY  - 2021
DP  - proceedings.mlr.press
SP  - 2008
EP  - 2016
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v130/izzo21a.html
Y2  - 2024/01/27/16:24:37
L1  - http://proceedings.mlr.press/v130/izzo21a/izzo21a.pdf
L1  - http://proceedings.mlr.press/v130/izzo21a/izzo21a-supp.pdf
ER  - 

TY  - CONF
TI  - Machine Unlearning for Random Forests
AU  - Brophy, Jonathan
AU  - Lowd, Daniel
T2  - International Conference on Machine Learning
AB  - Responding to user data deletion requests, removing noisy examples, or deleting corrupted training data are just a few reasons for wanting to delete instances from a machine learning (ML) model. However, efficiently removing this data from an ML model is generally difficult. In this paper, we introduce data removal-enabled (DaRE) forests, a variant of random forests that enables the removal of training data with minimal retraining. Model updates for each DaRE tree in the forest are exact, meaning that removing instances from a DaRE model yields exactly the same model as retraining from scratch on updated data. DaRE trees use randomness and caching to make data deletion efficient. The upper levels of DaRE trees use random nodes, which choose split attributes and thresholds uniformly at random. These nodes rarely require updates because they only minimally depend on the data. At the lower levels, splits are chosen to greedily optimize a split criterion such as Gini index or mutual information. DaRE trees cache statistics at each node and training data at each leaf, so that only the necessary subtrees are updated as data is removed. For numerical attributes, greedy nodes optimize over a random subset of thresholds, so that they can maintain statistics while approximating the optimal threshold. By adjusting the number of thresholds considered for greedy nodes, and the number of random nodes, DaRE trees can trade off between more accurate predictions and more efficient updates. In experiments on 13 real-world datasets and one synthetic dataset, we find DaRE forests delete data orders of magnitude faster than retraining from scratch while sacrificing little to no predictive power.
C3  - Proceedings of the 38th International Conference on Machine Learning
DA  - 2021/07/01/
PY  - 2021
DP  - proceedings.mlr.press
SP  - 1092
EP  - 1104
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v139/brophy21a.html
Y2  - 2024/01/27/16:24:14
L1  - http://proceedings.mlr.press/v139/brophy21a/brophy21a.pdf
L1  - http://proceedings.mlr.press/v139/brophy21a/brophy21a-supp.pdf
ER  - 

TY  - CONF
TI  - DeltaGrad: Rapid retraining of machine learning models
AU  - Wu, Yinjun
AU  - Dobriban, Edgar
AU  - Davidson, Susan
T2  - International Conference on Machine Learning
AB  - Machine learning models are not static and may need to be retrained on slightly changed datasets, for instance, with the addition or deletion of a set of data points. This has many applications, including privacy, robustness, bias reduction, and uncertainty quantifcation. However, it is expensive to retrain models from scratch. To address this problem, we propose the DeltaGrad algorithm for rapid retraining machine learning models based on information cached during the training phase. We provide both theoretical and empirical support for the effectiveness of DeltaGrad, and show that it compares favorably to the state of the art.
C3  - Proceedings of the 37th International Conference on Machine Learning
DA  - 2020/11/21/
PY  - 2020
DP  - proceedings.mlr.press
SP  - 10355
EP  - 10366
LA  - en
PB  - PMLR
ST  - DeltaGrad
UR  - https://proceedings.mlr.press/v119/wu20b.html
Y2  - 2024/01/27/16:23:52
L1  - http://proceedings.mlr.press/v119/wu20b/wu20b.pdf
L1  - http://proceedings.mlr.press/v119/wu20b/wu20b-supp.pdf
ER  - 

TY  - CONF
TI  - When Machine Unlearning Jeopardizes Privacy
AU  - Chen, Min
AU  - Zhang, Zhikun
AU  - Wang, Tianhao
AU  - Backes, Michael
AU  - Humbert, Mathias
AU  - Zhang, Yang
T3  - CCS '21
AB  - The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known asmachine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. \footnoteOur code is available at \urlhttps://github.com/MinChen00/UnlearningLeaks.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security
DA  - 2021/11/13/
PY  - 2021
DO  - 10.1145/3460120.3484756
DP  - ACM Digital Library
SP  - 896
EP  - 911
PB  - Association for Computing Machinery
SN  - 978-1-4503-8454-4
UR  - https://dl.acm.org/doi/10.1145/3460120.3484756
Y2  - 2024/01/27/
L1  - https://dl.acm.org/doi/pdf/10.1145/3460120.3484756
KW  - machine unlearning
KW  - machine learning security and privacy
KW  - membership inference
ER  - 

TY  - CONF
TI  - Privacy-preserving Prediction
AU  - Dwork, Cynthia
AU  - Feldman, Vitaly
T2  - Conference On Learning Theory
AB  - Ensuring differential privacy of models learned from sensitive user data is an important goal that has been studied extensively in recent years. It is now known that for some basic learning problems, especially those involving high-dimensional data, producing an accurate private model requires much more data than learning without privacy. At the same time, in many applications it is not necessary to expose the model itself. Instead users may be allowed to query the prediction model on their inputs only through an appropriate interface. Here we formulate the problem of ensuring privacy of individual predictions and investigate the overheads required to achieve it in several standard models of classification and regression. We first describe a simple baseline approach based on training several models on disjoint subsets of data and using standard private aggregation techniques to predict. We show that this approach has nearly optimal sample complexity for (realizable) PAC learning of any class of Boolean functions. At the same time, without strong assumptions on the data distribution, the aggregation step introduces a substantial overhead. We demonstrate that this overhead can be avoided for the well-studied class of thresholds on a line and for a number of standard settings of convex regression. The analysis of our algorithm for learning thresholds relies crucially on strong generalization guarantees that we establish for all differentially private prediction algorithms.
C3  - Proceedings of the 31st  Conference On Learning Theory
DA  - 2018/07/03/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 1693
EP  - 1702
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v75/dwork18a.html
Y2  - 2024/01/27/16:22:42
L1  - https://proceedings.mlr.press/v75/dwork18a/dwork18a.pdf
ER  - 

TY  - GEN
TI  - PUMA: Performance Unchanged Model Augmentation for Training Data Removal
AU  - Wu, Ga
AU  - Hashemi, Masoud
AU  - Srinivasa, Christopher
AB  - Preserving the performance of a trained model while removing unique characteristics of marked training data points is challenging. Recent research usually suggests retraining a model from scratch with remaining training data or refining the model by reverting the model optimization on the marked data points. Unfortunately, aside from their computational inefficiency, those approaches inevitably hurt the resulting model's generalization ability since they remove not only unique characteristics but also discard shared (and possibly contributive) information. To address the performance degradation problem, this paper presents a novel approach called Performance Unchanged Model Augmentation~(PUMA). The proposed PUMA framework explicitly models the influence of each training data point on the model's generalization ability with respect to various performance criteria. It then complements the negative impact of removing marked data by reweighting the remaining data optimally. To demonstrate the effectiveness of the PUMA framework, we compared it with multiple state-of-the-art data removal techniques in the experiments, where we show the PUMA can effectively and efficiently remove the unique characteristics of marked training data without retraining the model that can 1) fool a membership attack, and 2) resist performance degradation. In addition, as PUMA estimates the data importance during its operation, we show it could serve to debug mislabelled data points more efficiently than existing approaches.
DA  - 2022/03/01/
PY  - 2022
DO  - 10.48550/arXiv.2203.00846
DP  - arXiv.org
PB  - arXiv
ST  - PUMA
UR  - http://arxiv.org/abs/2203.00846
Y2  - 2024/01/27/16:21:53
L1  - https://arxiv.org/pdf/2203.00846.pdf
L2  - https://arxiv.org/abs/2203.00846
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - JOUR
TI  - Zero-Shot Machine Unlearning
AU  - Chundawat, Vikram S.
AU  - Tarun, Ayush K.
AU  - Mandal, Murari
AU  - Kankanhalli, Mohan
T2  - IEEE Transactions on Information Forensics and Security
AB  - Modern privacy regulations grant citizens the right to be forgotten by products, services and companies. In case of machine learning (ML) applications, this necessitates deletion of data not only from storage archives but also from ML models. Due to an increasing need for regulatory compliance required for ML applications, machine unlearning is becoming an emerging research problem. The right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ML model. Practical considerations preclude retraining of the model from scratch after discarding the deleted data. The few existing studies use either the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. However, strict regulatory compliance requires time-bound deletion of data. Thus, in many cases, no data related to the training process or training samples may be accessible even for the unlearning purpose. We therefore ask the question: is it possible to achieve unlearning with zero training samples? In this paper, we introduce the novel problem of zero-shot machine unlearning that caters for the extreme but practical scenario where zero original data samples are available for use. We then propose two novel solutions for zero-shot machine unlearning based on (a) error minimizing-maximizing noise and (b) gated knowledge transfer. These methods remove the information of the forget data from the model while maintaining the model efficacy on the retain data. The zero-shot approach offers good protection against the model inversion attacks and membership inference attacks. We introduce a new evaluation metric, Anamnesis Index (AIN) to effectively measure the quality of the unlearning method. The experiments show promising results for unlearning in deep learning models on benchmark vision data-sets. The source code is available here: <uri>https://github.com/ayu987/zero-shot-unlearning</uri>
DA  - 2023/01/01/
PY  - 2023
DO  - 10.1109/TIFS.2023.3265506
DP  - ACM Digital Library
VL  - 18
SP  - 2345
EP  - 2354
J2  - Trans. Info. For. Sec.
SN  - 1556-6013
UR  - https://doi.org/10.1109/TIFS.2023.3265506
Y2  - 2024/01/27/16:21:24
L1  - https://arxiv.org/pdf/2201.05629
ER  - 

TY  - CONF
TI  - Analyzing Information Leakage of Updates to Natural Language Models
AU  - Zanella-Béguelin, Santiago
AU  - Wutschitz, Lukas
AU  - Tople, Shruti
AU  - Rühle, Victor
AU  - Paverd, Andrew
AU  - Ohrimenko, Olga
AU  - Köpf, Boris
AU  - Brockschmidt, Marc
AB  - To continuously improve quality and reflect changes in data, machine learning applications have to regularly retrain and update their core models. We show that a differential analysis of language model snapshots before and after an update can reveal a surprising amount of detailed information about changes in the training data. We propose two new metrics---\emph{differential score} and \emph{differential rank}---for analyzing the leakage due to updates of natural language models. We perform leakage analysis using these metrics across models trained on several different datasets using different methods and configurations. We discuss the privacy implications of our findings, propose mitigation strategies and evaluate their effect.
C3  - Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security
DA  - 2020/10/30/
PY  - 2020
DO  - 10.1145/3372297.3417880
DP  - arXiv.org
SP  - 363
EP  - 375
UR  - http://arxiv.org/abs/1912.07942
Y2  - 2024/01/27/16:21:05
L1  - https://arxiv.org/pdf/1912.07942.pdf
L2  - https://arxiv.org/abs/1912.07942
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computation and Language
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - GEN
TI  - Hard to Forget: Poisoning Attacks on Certified Machine Unlearning
AU  - Marchant, Neil G.
AU  - Rubinstein, Benjamin I. P.
AU  - Alfeld, Scott
AB  - The right to erasure requires removal of a user's information from data held by organizations, with rigorous interpretations extending to downstream products such as learned models. Retraining from scratch with the particular user's data omitted fully removes its influence on the resulting model, but comes with a high computational cost. Machine "unlearning" mitigates the cost incurred by full retraining: instead, models are updated incrementally, possibly only requiring retraining when approximation errors accumulate. Rapid progress has been made towards privacy guarantees on the indistinguishability of unlearned and retrained models, but current formalisms do not place practical bounds on computation. In this paper we demonstrate how an attacker can exploit this oversight, highlighting a novel attack surface introduced by machine unlearning. We consider an attacker aiming to increase the computational cost of data removal. We derive and empirically investigate a poisoning attack on certified machine unlearning where strategically designed training data triggers complete retraining when removed.
DA  - 2022/02/09/
PY  - 2022
DO  - 10.48550/arXiv.2109.08266
DP  - arXiv.org
PB  - arXiv
ST  - Hard to Forget
UR  - http://arxiv.org/abs/2109.08266
Y2  - 2024/01/27/16:20:31
L1  - https://arxiv.org/pdf/2109.08266.pdf
L2  - https://arxiv.org/abs/2109.08266
KW  - Computer Science - Machine Learning
KW  - Computer Science - Cryptography and Security
ER  - 

TY  - CONF
TI  - Have You Forgotten? A Method to Assess if Machine Learning Models Have Forgotten Data
AU  - Liu, Xiao
AU  - Tsaftaris, Sotirios A.
AB  - In the era of deep learning, aggregation of data from several sources is a common approach to ensuring data diversity. Let us consider a scenario where several providers contribute data to a consortium for the joint development of a classification model (hereafter the target model), but, now one of the providers decides to leave. This provider requests that their data (hereafter the query dataset) be removed from the databases but also that the model ‘forgets’ their data. In this paper, for the first time, we want to address the challenging question of whether data have been forgotten by a model. We assume knowledge of the query dataset and the distribution of a model’s output. We establish statistical methods that compare the target’s outputs with outputs of models trained with different datasets. We evaluate our approach on several benchmark datasets (MNIST, CIFAR-10 and SVHN) and on a cardiac pathology diagnosis task using data from the Automated Cardiac Diagnosis Challenge (ACDC). We hope to encourage studies on what information a model retains and inspire extensions in more complex settings.
C1  - Berlin, Heidelberg
C3  - Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I
DA  - 2020/10/04/
PY  - 2020
DO  - 10.1007/978-3-030-59710-8_10
DP  - ACM Digital Library
SP  - 95
EP  - 105
PB  - Springer-Verlag
SN  - 978-3-030-59709-2
ST  - Have You Forgotten?
UR  - https://doi.org/10.1007/978-3-030-59710-8_10
Y2  - 2024/01/27/
L1  - https://www.pure.ed.ac.uk/ws/files/163712810/MICCAI.pdf
KW  - Privacy
KW  - Kolmogorov-Smirnov
KW  - Statistical measure
ER  - 

TY  - CONF
TI  - Backdoor Defense with Machine Unlearning
AU  - Liu, Yang
AU  - Fan, Mingyuan
AU  - Chen, Cen
AU  - Liu, Ximeng
AU  - Ma, Zhuo
AU  - Wang, Li
AU  - Ma, Jianfeng
AB  - Backdoor injection attack is an emerging threat to the security of neural networks, however, there still exist limited effective defense methods against the attack. In this paper, we propose BAERASER, a novel method that can erase the backdoor injected into the victim model through machine unlearning. Specifically, BAERASER mainly implements backdoor defense in two key steps. First, trigger pattern recovery is conducted to extract the trigger patterns infected by the victim model. Here, the trigger pattern recovery problem is equivalent to the one of extracting an unknown noise distribution from the victim model, which can be easily resolved by the entropy maximization based generative model. Subsequently, BAERASER leverages these recovered trigger patterns to reverse the backdoor injection procedure and induce the victim model to erase the polluted memories through a newly designed gradient ascent based machine unlearning method. Compared with the previous machine unlearning solutions, the proposed approach gets rid of the reliance on the full access to training data for retraining and shows higher effectiveness on backdoor erasing than existing fine-tuning or pruning methods. Moreover, experiments show that BAERASER can averagely lower the attack success rates of three kinds of state-of-the-art backdoor attacks by 99% on four benchmark datasets.
C1  - London, United Kingdom
C3  - IEEE INFOCOM 2022 - IEEE Conference on Computer Communications
DA  - 2022/05/02/
PY  - 2022
DO  - 10.1109/INFOCOM48880.2022.9796974
DP  - ACM Digital Library
SP  - 280
EP  - 289
PB  - IEEE Press
UR  - https://doi.org/10.1109/INFOCOM48880.2022.9796974
Y2  - 2024/01/27/
L1  - https://arxiv.org/pdf/2201.09538
ER  - 

TY  - JOUR
TI  - Exploration-exploitation in multi-agent learning: Catastrophe theory meets game theory
AU  - Leonardos, Stefanos
AU  - Piliouras, Georgios
T2  - Artificial Intelligence
AB  - Exploration-exploitation is a powerful and practical tool in multi-agent learning (MAL); however, its effects are far from understood. To make progress in this direction, we study a smooth analogue of Q-learning. We start by showing that our learning model has strong theoretical justification as an optimal model for studying exploration-exploitation. Specifically, we prove (1) that smooth Q-learning has bounded regret in arbitrary games for a cost model that explicitly balances game-rewards and exploration-costs, i.e., costs from testing potentially suboptimal actions, and (2) that it always converges to the set of quantal-response equilibria (QRE), the standard solution concept for games with bounded rationality, in arbitrary weighted potential games. In our main task, we then turn to measure the effect of exploration on collective system performance. We characterize the geometry of the QRE surface in low-dimensional MAL systems and link our findings with catastrophe (bifurcation) theory. In particular, as the exploration hyperparameter evolves over-time, the system undergoes phase transitions where the number and stability of equilibria can change radically given an infinitesimal change to the exploration parameter. Based on this, we provide a formal theoretical treatment of how tuning the exploration parameter can provably lead to equilibrium selection with both positive as well as negative (and potentially unbounded) effects to system performance.
DA  - 2022/03/01/
PY  - 2022
DO  - 10.1016/j.artint.2021.103653
DP  - ScienceDirect
VL  - 304
SP  - 103653
J2  - Artificial Intelligence
SN  - 0004-3702
ST  - Exploration-exploitation in multi-agent learning
UR  - https://www.sciencedirect.com/science/article/pii/S0004370221002046
Y2  - 2024/01/27/16:18:15
L1  - https://kclpure.kcl.ac.uk/portal/files/180741255/17343_Article_Text_20837_1_2_20210518_1_.pdf
KW  - Game theory
KW  - Catastrophe theory
KW  - Exploration-exploitation
KW  - Multi-agent learning
ER  - 

TY  - GEN
TI  - Sparse Attention with Linear Units
AU  - Zhang, Biao
AU  - Titov, Ivan
AU  - Sennrich, Rico
AB  - Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off') for some queries, which is not possible with sparsified softmax alternatives.
DA  - 2021/10/06/
PY  - 2021
DO  - 10.48550/arXiv.2104.07012
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2104.07012
Y2  - 2024/01/28/23:58:08
L1  - https://arxiv.org/pdf/2104.07012.pdf
L2  - https://arxiv.org/abs/2104.07012
KW  - Computer Science - Machine Learning
KW  - Computer Science - Computation and Language
ER  - 

TY  - CONF
TI  - Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL
AU  - Christoffersen, Phillip J.K.
AU  - Haupt, Andreas A.
AU  - Hadfield-Menell, Dylan
T3  - AAMAS '23
AB  - Multi-agent reinforcement learning (MARL) is a powerful tool for training automated systems acting independently in a common environment. However, it can lead to sub-optimal behavior when individual incentives and group incentives diverge. Humans are remarkably capable at solving these social dilemmas. It is an open problem in MARL to replicate such cooperative behaviors in selfish agents. In this work, we draw upon the idea of formal contracting from economics to overcome diverging incentives between agents in MARL. We propose an augmentation to a Markov game where agents voluntarily agree to binding state-dependent transfers of reward, under pre-specified conditions. Our contributions are theoretical and empirical. First, we show that this augmentation makes all subgame-perfect equilibria of all fully observed Markov games exhibit socially optimal behavior, given a sufficiently rich space of contracts. Next, we complement our game-theoretic analysis with experiments running deep RL on the contracting augmentation for various social dilemmas. We discuss some practical issues with learning in the contracting augmentation, and provide a training methodology that leads to high-welfare outcomes, Multi-Objective Contract Augmentation Learning (MOCA). We test our methodology in static, single-move games, as well as dynamic domains that simulate traffic, pollution management and common pool resource management.
C1  - Richland, SC
C3  - Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems
DA  - 2023/05/30/
PY  - 2023
DP  - ACM Digital Library
SP  - 448
EP  - 456
PB  - International Foundation for Autonomous Agents and Multiagent Systems
SN  - 978-1-4503-9432-1
ST  - Get It in Writing
Y2  - 2024/01/31/
L1  - https://dl.acm.org/doi/pdf/10.5555/3545946.3598670
KW  - decentralized training
KW  - formal contracts
KW  - social dilemma
ER  - 

TY  - CONF
TI  - Deep Reinforcement Learning from Human Preferences
AU  - Christiano, Paul F
AU  - Leike, Jan
AU  - Brown, Tom
AU  - Martic, Miljan
AU  - Legg, Shane
AU  - Amodei, Dario
AB  - For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.
C3  - Advances in Neural Information Processing Systems
DA  - 2017///
PY  - 2017
DP  - Neural Information Processing Systems
VL  - 30
PB  - Curran Associates, Inc.
UR  - https://proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html
Y2  - 2024/02/01/00:37:46
L1  - https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf
ER  - 

TY  - CONF
TI  - ATOM: Robustifying Out-of-Distribution Detection Using Outlier Mining
AU  - Chen, Jiefeng
AU  - Li, Yixuan
AU  - Wu, Xi
AU  - Liang, Yingyu
AU  - Jha, Somesh
A2  - Oliver, Nuria
A2  - Pérez-Cruz, Fernando
A2  - Kramer, Stefan
A2  - Read, Jesse
A2  - Lozano, Jose A.
T3  - Lecture Notes in Computer Science
AB  - Detecting out-of-distribution (OOD) inputs is critical for safely deploying deep learning models in an open-world setting. However, existing OOD detection solutions can be brittle in the open world, facing various types of adversarial OOD inputs. While methods leveraging auxiliary OOD data have emerged, our analysis on illuminative examples reveals a key insight that the majority of auxiliary OOD examples may not meaningfully improve or even hurt the decision boundary of the OOD detector, which is also observed in empirical results on real data. In this paper, we provide a theoretically motivated method, Adversarial Training with informative Outlier Mining (ATOM), which improves the robustness of OOD detection. We show that, by mining informative auxiliary OOD data, one can significantly improve OOD detection performance, and somewhat surprisingly, generalize to unseen adversarial attacks. ATOM achieves state-of-the-art performance under a broad family of classic and adversarial OOD evaluation tasks. For example, on the CIFAR-10 in-distribution dataset, ATOM reduces the FPR (at TPR 95%) by up to 57.99% under adversarial OOD inputs, surpassing the previous best baseline by a large margin.
C1  - Cham
C3  - Machine Learning and Knowledge Discovery in Databases. Research Track
DA  - 2021///
PY  - 2021
DO  - 10.1007/978-3-030-86523-8_26
DP  - Springer Link
SP  - 430
EP  - 445
LA  - en
PB  - Springer International Publishing
SN  - 978-3-030-86523-8
ST  - ATOM
L1  - https://link.springer.com/content/pdf/10.1007%2F978-3-030-86523-8_26.pdf
KW  - Robustness
KW  - Out-of-distribution detection
KW  - Outlier mining
ER  - 

TY  - CONF
TI  - Towards Evaluating the Robustness of Neural Networks
AU  - Carlini, Nicholas
AU  - Wagner, David
T2  - 2017 IEEE Symposium on Security and Privacy (SP)
AB  - Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95% to 0.5%. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.
DA  - 2017/05/01/
PY  - 2017
DO  - 10.1109/SP.2017.49
DP  - www.computer.org
SP  - 39
EP  - 57
LA  - English
PB  - IEEE Computer Society
SN  - 978-1-5090-5533-3
UR  - https://www.computer.org/csdl/proceedings-article/sp/2017/07958570/12OmNviHK8t
Y2  - 2024/02/01/00:29:31
L1  - https://arxiv.org/pdf/1608.04644
ER  - 

TY  - CONF
TI  - Sanity Checks for Saliency Maps
AU  - Adebayo, Julius
AU  - Gilmer, Justin
AU  - Muelly, Michael
AU  - Goodfellow, Ian
AU  - Hardt, Moritz
AU  - Kim, Been
AB  - Saliency methods have emerged as a popular tool to highlight features in an input
deemed relevant for the prediction of a learned model. Several saliency methods
have been proposed, often guided by visual appeal on image data. In this work, we
propose an actionable methodology to evaluate what kinds of explanations a given
method can and cannot provide. We find that reliance, solely, on visual assessment
can be misleading. Through extensive experiments we show that some existing
saliency methods are independent both of the model and of the data generating
process. Consequently, methods that fail the proposed tests are inadequate for
tasks that are sensitive to either data or model, such as, finding outliers in the data,
explaining the relationship between inputs and outputs that the model learned,
and debugging the model. We interpret our findings through an analogy with
edge detection in images, a technique that requires neither training data nor model.
Theory in the case of a linear model and a single-layer convolutional neural network
supports our experimental findings.
C3  - Advances in Neural Information Processing Systems
DA  - 2018///
PY  - 2018
DP  - Neural Information Processing Systems
VL  - 31
PB  - Curran Associates, Inc.
UR  - https://proceedings.neurips.cc/paper_files/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html
Y2  - 2024/02/01/00:19:28
L1  - https://proceedings.neurips.cc/paper_files/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf
ER  - 


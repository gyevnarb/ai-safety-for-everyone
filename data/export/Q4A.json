[
	{
		"id": "wuHumanGuidedReinforcementLearning2023",
		"type": "article-journal",
		"abstract": "Reinforcement learning (RL) is a promising approach in unmanned ground vehicles (UGVs) applications, but limited computing resource makes it challenging to deploy a well-behaved RL strategy with sophisticated neural networks. Meanwhile, the training of RL on navigation tasks is difficult, which requires a carefully-designed reward function and a large number of interactions, yet RL navigation can still fail due to many corner cases. This shows the limited intelligence of current RL methods, thereby prompting us to rethink combining RL with human intelligence. In this paper, a human-guided RL framework is proposed to improve RL performance both during learning in the simulator and deployment in the real world. The framework allows humans to intervene in RL&#x0027;s control progress and provide demonstrations as needed, thereby improving RL&#x0027;s capabilities. An innovative human-guided RL algorithm is proposed that utilizes a series of mechanisms to improve the effectiveness of human guidance, including human-guided learning objective, prioritized human experience replay, and human intervention-based reward shaping. Our RL method is trained in simulation and then transferred to the real world, and we develop a denoised representation for domain adaptation to mitigate the simulation-to-real gap. Our method is validated through simulations and real-world experiments to navigate UGVs in diverse and dynamic environments based only on tiny neural networks and image inputs. Our method performs better in goal-reaching and safety than existing learning- and model-based navigation approaches and is robust to changes in input features and ego kinetics. Furthermore, our method allows small-scale human demonstrations to be used to improve the trained RL agent and learn expected behaviors online. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
		"DOI": "10.1109/TPAMI.2023.3314762",
		"page": "1-15",
		"title": "Human-Guided Reinforcement Learning With Sim-to-Real Transfer for Autonomous Navigation",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171531773&doi=10.1109%2fTPAMI.2023.3314762&partnerID=40&md5=213ee3846632eadf5c500e5900fd77df",
		"author": [
			{
				"family": "Wu",
				"given": "J."
			},
			{
				"family": "Zhou",
				"given": "Y."
			},
			{
				"family": "Yang",
				"given": "H."
			},
			{
				"family": "Huang",
				"given": "Z."
			},
			{
				"family": "Lv",
				"given": "C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "yangLearningNoisyLabels2023",
		"type": "article-journal",
		"abstract": "Numerous researches have proved that deep neural networks (DNNs) can fit almost everything even given data with noisy labels, and result in poor generalization performance. However, recent studies suggest that DNNs tend to gradually memorize the data, moving from correct data to mislabeled data. Inspired by this finding, we propose a novel method named <italic>Dynamic Loss Thresholding (DLT)</italic>. During the training process, DLT records the loss value of each sample and calculates dynamic loss thresholds. Specifically, DLT compares the loss value of each sample with the current loss threshold. Samples with smaller losses can be considered as clean samples with higher probability and vice versa. Then, DLT discards the potentially corrupted labels and further leverages self-training semi-supervised learning techniques. Experiments on CIFAR-10/100, WebVision and Clothing1M demonstrate substantial improvements over recent state-of-the-art methods. In addition, we investigate two real-world problems. Firstly, we propose a novel approach to estimate the noise rates of datasets based on the loss difference between the early and late training stages of DNNs. Secondly, we explore the effect of hard samples (which are difficult to be distinguished) on the process of learning from noisy labels. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Knowledge and Data Engineering",
		"DOI": "10.1109/TKDE.2023.3313604",
		"page": "1-14",
		"title": "Learning From Noisy Labels Via Dynamic Loss Thresholding",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171528884&doi=10.1109%2fTKDE.2023.3313604&partnerID=40&md5=e14cea8b8cc944e7e9d83a3bca018bdf",
		"author": [
			{
				"family": "Yang",
				"given": "H."
			},
			{
				"family": "Jin",
				"given": "Y."
			},
			{
				"family": "Li",
				"given": "Z."
			},
			{
				"family": "Wang",
				"given": "D."
			},
			{
				"family": "Geng",
				"given": "X."
			},
			{
				"family": "Zhang",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhangRobustExtremeLearning2023",
		"type": "article-journal",
		"abstract": "The extreme learning machine (ELM) algorithm is advantageous to regression modeling owing to its simple structure, fast computation, and good generalization performance. However, the existing ELM algorithm uses an l2 -norm loss function, which is sensitive to outliers and has low robustness. In addition, some existing robust loss functions are not sufficiently flexible to accurately estimate the relationship between sample points and loss values, resulting in unsatisfactory ELM performance. To address these problems, this study established a robust ELM (ALFELM) algorithm. First, an adaptive loss function with two tunable hyperparameters was introduced; the function can be transformed into several robust loss functions by varying the parameters. It overcomes the limitations of fixed robust loss functions. Then, the Bayesian optimization strategy was used to determine the optimal parameters of the loss function. Furthermore, the classical iterative reweighted least squares method was used to solve for output weights, with a weight function corresponding to the loss function and a regularization parameter to prevent overfitting. Finally, the proposed method was tested using several artificial and benchmark datasets, and its effectiveness was verified for a real engineering case. The results indicated that the proposed ALFELM algorithm is more robust and accurate compared with other methods, especially for a large number of outliers. In addition, the algorithm can be used to establish effective regression models for actual processes. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",
		"archive": "Scopus",
		"container-title": "Neural Processing Letters",
		"DOI": "10.1007/s11063-023-11340-y",
		"title": "A Robust Extreme Learning Machine Based on Adaptive Loss Function for Regression Modeling",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164156087&doi=10.1007%2fs11063-023-11340-y&partnerID=40&md5=6ff6b65fab5ec5db4a239067c063e3ac",
		"author": [
			{
				"family": "Zhang",
				"given": "F."
			},
			{
				"family": "Chen",
				"given": "S."
			},
			{
				"family": "Hong",
				"given": "Z."
			},
			{
				"family": "Shan",
				"given": "B."
			},
			{
				"family": "Xu",
				"given": "Q."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhaoProbabilisticSafeguardReinforcement2023",
		"type": "paper-conference",
		"abstract": "Safety is one of the biggest concerns to applying reinforcement learning (RL) to the physical world. In its core part, it is challenging to ensure RL agents persistently satisfy a hard state constraint without white-box or black-box dynamics models. This paper presents an integrated model learning and safe control framework to safeguard any RL agent, where the environment dynamics are learned as Gaussian processes. The proposed theory provides (i) a novel method to construct an offline dataset for model learning that best achieves safety requirements; (ii) a design rule to construct the safety index to ensure the existence of safe control under control limits; (iii) a probablistic safety guarantee (i.e. probabilistic forward invariance) when the model is learned using the aforementioned dataset. Simulation results show that our framework achieves almost zero safety violation on various continuous control tasks. © 2023 W. Zhao, T. He & C. Liu.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "783-796",
		"title": "Probabilistic Safeguard for Reinforcement Learning Using Safety Index Guided Gaussian Process Models",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164508271&partnerID=40&md5=52100e0b70a3b6e78be9f039cde62fc6",
		"volume": "211",
		"author": [
			{
				"family": "Zhao",
				"given": "W."
			},
			{
				"family": "He",
				"given": "T."
			},
			{
				"family": "Liu",
				"given": "C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "dagdanovSelfImprovingSafetyPerformance2023",
		"type": "paper-conference",
		"abstract": "In this work, we propose a self-improving artificial intelligence system to enhance the safety performance of reinforcement learning (RL)-based autonomous driving (AD) agents using black-box verification methods. RL algorithms have become popular in AD applications in recent years. However, the performance of existing RL algorithms heavily depends on the diversity of training scenarios. A lack of safety-critical scenarios during the training phase could result in poor generalization performance in real-world driving applications. We propose a novel framework in which the weaknesses of the training set are explored through black-box verification methods. After discovering AD failure scenarios, the RL agent's training is re-initiated via transfer learning to improve the performance of previously unsafe scenarios. Simulation results demonstrate that our approach efficiently discovers safety failures of action decisions in RL-based adaptive cruise control (ACC) applications and significantly reduces the number of vehicle collisions through iterative applications of our method. The source code is publicly available at https://github.com/data-and-decision-lab/self-improving-RL. © 2023 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ICRA48891.2023.10160883",
		"event-title": "Proceedings - IEEE International Conference on Robotics and Automation",
		"page": "5631-5637",
		"title": "Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168711151&doi=10.1109%2fICRA48891.2023.10160883&partnerID=40&md5=dc9e3afea2258e7f3a6a741b55b96a88",
		"volume": "2023-May",
		"author": [
			{
				"family": "Dagdanov",
				"given": "R."
			},
			{
				"family": "Durmus",
				"given": "H."
			},
			{
				"family": "Ure",
				"given": "N.K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "caiDiscrepancyAwareFramework2023",
		"type": "article-journal",
		"abstract": "Defect detection is a critical research area in artificial intelligence. Recently, synthetic data-based self-supervised learning has shown great potential on this task. Although many sophisticated synthesizing strategies exist, little research has been done to investigate the robustness of models when faced with different strategies. In this article, we focus on this issue and find that existing methods are highly sensitive to them. To alleviate this issue, we present a discrepancy aware framework (DAF), which demonstrates robust performance consistently with simple and cheap strategies across different anomaly detection benchmarks. We hypothesize that the high sensitivity to synthetic data of existing self-supervised methods arises from their heavy reliance on the visual appearance of synthetic data during decoding. In contrast, our method leverages an appearance-agnostic cue to guide the decoder in identifying defects, thereby alleviating its reliance on synthetic appearance. To this end, inspired by existing knowledge distillation methods, we employ a teacher-student network, which is trained based on synthesized outliers, to compute the discrepancy map as the cue. Extensive experiments on two challenging datasets prove the robustness of our method. Under the simple synthesis strategies, it outperforms existing methods by a large margin. Furthermore, it also achieves the state-of-the-art localization performance. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Industrial Informatics",
		"DOI": "10.1109/TII.2023.3318302",
		"page": "1-10",
		"title": "A Discrepancy Aware Framework for Robust Anomaly Detection",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174819201&doi=10.1109%2fTII.2023.3318302&partnerID=40&md5=d67ed69e94513df294a49a8610cfedcb",
		"author": [
			{
				"family": "Cai",
				"given": "Y."
			},
			{
				"family": "Liang",
				"given": "D."
			},
			{
				"family": "Luo",
				"given": "D."
			},
			{
				"family": "He",
				"given": "X."
			},
			{
				"family": "Yang",
				"given": "X."
			},
			{
				"family": "Bai",
				"given": "X."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "jiProbabilisticCounterexampleGuidance2023",
		"type": "paper-conference",
		"abstract": "Safe exploration aims at addressing the limitations of Reinforcement Learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs. Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states. However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging. In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement. Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration. We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the risk of safety violations during the subsequent online exploration. We demonstrate our method’s effectiveness in reducing safety violations during online exploration in preliminary experiments by an average of 40.3% compared with QL and DQN standard algorithms and 29.1% compared with previous related work, while achieving comparable cumulative rewards with respect to unrestricted exploration and alternative approaches. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-031-43835-6_22",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "311-328",
		"title": "Probabilistic Counterexample Guidance for Safer Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174226630&doi=10.1007%2f978-3-031-43835-6_22&partnerID=40&md5=b3d687148a4fbe03b6bafbf99bae2612",
		"volume": "14287 LNCS",
		"author": [
			{
				"family": "Ji",
				"given": "X."
			},
			{
				"family": "Filieri",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ganDualLearningBasedSafe2018",
		"type": "article-journal",
		"abstract": "In many real-world applications, labeled instances are generally limited and expensively collected, while the most instances are unlabeled and the amount is often sufficient. Therefore, semi supervised learning (SSL) has attracted much attention, since it is an effective tool to discover the unlabeled instances. However, how to safely make use of the unlabeled instances is an emerging and interesting problem in SSL. Hence, we propose DuAL Learning-based sAfe Semi-supervised learning (DALLAS), which employs dual learning to estimate the safety or risk of the unlabeled instances. To realize the safe exploitation of the unlabeled instances, our basic idea is to use supervised learning (SL) to analyze the risk of the unlabeled instances. First, DALLAS utilizes a primal model obtained by dual learning to classify each unlabeled instance and then uses a dual model to reconstruct the unlabeled instances according to the obtained classification results. The risk can be measured by analyzing the reconstruction error and predictions of the original and reconstructed unlabeled instances. If the error is small and the predictions are equal, the unlabeled instance may be safe. Otherwise, the instance may be risky and its output should be approach to be that obtained by SL. Finally, we embed a risk-based regularization term into SSL. Hence, the outputs of our algorithm are a tradeoff between those of SL and SSL. In particular, we utilize respectively regularized least squares (RLS) and Laplacian RLS for SL and SSL. To verify the effectiveness of the proposed safe mechanism in DALLAS, we carry out a series of experiments on several data sets by the comparison with the state-of-the-art supervised, semi-supervised, and safe semi-supervised learning methods and the results demonstrate that DALLAS can effectively reduce the risk of the unlabeled instances.",
		"archive_location": "WOS:000425688200021",
		"container-title": "IEEE ACCESS",
		"DOI": "10.1109/ACCESS.2017.2784406",
		"ISSN": "2169-3536",
		"page": "2615-2621",
		"title": "Dual Learning-Based Safe Semi-Supervised Learning",
		"volume": "6",
		"author": [
			{
				"family": "Gan",
				"given": "HT"
			},
			{
				"family": "Li",
				"given": "ZH"
			},
			{
				"family": "Fan",
				"given": "YL"
			},
			{
				"family": "Luo",
				"given": "ZZ"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "aksjonovSafetyCriticalDecisionMakingControl2023",
		"type": "article-journal",
		"abstract": "While machine-learning-based methods suffer from a lack of transparency, rule-based (RB) methods dominate safety-critical systems. Yet the RB approaches cannot compete with the first ones in robustness to multiple system requirements, for instance, simultaneously addressing safety, comfort, and efficiency. Hence, this article proposes a decision-making and control framework which profits from the advantages of both the RB and machine-learning-based techniques while compensating for their disadvantages. The proposed method embodies two controllers operating in parallel, called Safety and Learned. An RB switching logic selects one of the actions transmitted from both controllers. The Safety controller is prioritized whenever the Learned one does not meet the safety constraint, and also directly participates in the Learned controller training. Decision-making and control in autonomous driving are chosen as the system case study, where an autonomous vehicle (AV) learns a multitask policy to safely execute an unprotected left turn. Multiple requirements (i.e., safety, efficiency, and comfort) are set to vehicle motion. A numerical simulation is performed for the proposed framework validation, where its ability to satisfy the requirements and robustness to changing environments is successfully demonstrated.",
		"archive_location": "WOS:001072996800003",
		"container-title": "SAE INTERNATIONAL JOURNAL OF VEHICLE DYNAMICS STABILITY AND NVH",
		"DOI": "10.4271/10-07-03-0018",
		"ISSN": "2380-2162",
		"issue": "3",
		"page": "287-299",
		"title": "A Safety-Critical Decision-Making and Control Framework Combining Machine-Learning-Based and Rule-Based Algorithms",
		"volume": "7",
		"author": [
			{
				"family": "Aksjonov",
				"given": "A"
			},
			{
				"family": "Kyrki",
				"given": "V"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "guissoumaContinuousSafetyAssessment2023",
		"type": "paper-conference",
		"abstract": "Over-The-Air (OTA) updates play an essential role in the lifecycle management of modern Cyber Physical Systems (CPSs). They are deployed in short time periods to fix bugs and introduce new features. However, an important part of these updates affects safety-critical functions, and thus, requires thorough verification and validation. Particular care must be taken when using machine learning algorithms, for which it is more difficult to test all conceivable corner cases during the development process. To prevent potential unforeseen misbehavior after deployment, we introduce a method for runtime evaluation of updates in shadow mode using contract specifications. The method focuses on supervised learning models and is embedded in a workflow for iterative training. This enables carrying out reliable field testing and obtaining a realistic evaluation of the planned updates before release. Finally, we evaluate our approach on a prototype Electronic Control Unit (ECU) implementing an automotive Lane Keep Assist (LKA) system.",
		"archive_location": "WOS:000990534100053",
		"DOI": "10.1109/ICSA-C57050.2023.00069",
		"event-title": "2023 IEEE 20TH INTERNATIONAL CONFERENCE ON SOFTWARE ARCHITECTURE COMPANION, ICSA-C",
		"ISBN": "2768-427X",
		"page": "301-308",
		"title": "Continuous Safety Assessment of Updated Supervised Learning Models in Shadow Mode",
		"author": [
			{
				"family": "Guissouma",
				"given": "H"
			},
			{
				"family": "Zink",
				"given": "M"
			},
			{
				"family": "Sax",
				"given": "E"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bossensExplicitExploreExploit2023",
		"type": "article-journal",
		"abstract": "In reinforcement learning (RL), an agent must explore an initially unknown environment in order to learn a desired behaviour. When RL agents are deployed in real world environments, safety is of primary concern. Constrained Markov decision processes (CMDPs) can provide long-term safety constraints; however, the agent may violate the constraints in an effort to explore its environment. This paper proposes a model-based RL algorithm called Explicit Explore, Exploit, or Escape (E-4), which extends the Explicit Explore or Exploit (E-3) algorithm to a robust CMDP setting. E-4 explicitly separates exploitation, exploration, and escape CMDPs, allowing targeted policies for policy improvement across known states, discovery of unknown states, as well as safe return to known states. E-4 robustly optimises these policies on the worst-case CMDP from a set of CMDP models consistent with the empirical observations of the deployment environment. Theoretical results show that E-4 finds a near-optimal constraint-satisfying policy in polynomial time whilst satisfying safety constraints throughout the learning process. We then discuss E-4 as a practical algorithmic framework, including robust-constrained offline optimisation algorithms, the design of uncertainty sets for the transition dynamics of unknown states, and how to further leverage empirical observations and prior knowledge to relax some of the worst-case assumptions underlying the theory.",
		"archive_location": "WOS:000814467900003",
		"container-title": "MACHINE LEARNING",
		"DOI": "10.1007/s10994-022-06201-z",
		"ISSN": "0885-6125",
		"issue": "3",
		"page": "817-858",
		"title": "Explicit Explore, Exploit, or Escape (E4): near-optimal safety-constrained reinforcement learning in polynomial time",
		"volume": "112",
		"author": [
			{
				"family": "Bossens",
				"given": "DM"
			},
			{
				"family": "Bishop",
				"given": "N"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					3
				]
			]
		}
	},
	{
		"id": "zhuSafetyPerformanceWhy2022",
		"type": "paper-conference",
		"abstract": "The size of deep learning models in artificial intelligence (AI) software is increasing rapidly, which hinders the large-scale deployment on resource-restricted devices (e.g., smartphones). To mitigate this issue, AI software compression plays a crucial role, which aims to compress model size while keeping high performance. However, the intrinsic defects in the big model may be inherited by the compressed one. Such defects may be easily leveraged by attackers, since the compressed models are usually deployed in a large number of devices without adequate protection. In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective. Specifically, inspired by the test-driven development (TDD) paradigm in software engineering, we propose a test-driven sparse training framework called SafeCompress. By simulating the attack mechanism as the safety test, SafeCompress can automatically compress a big model to a small one following the dynamic sparse training paradigm. Further, considering a representative attack, i.e., membership inference attack (MIA), we develop a concrete safe model compression mechanism, called MIA-SafeCompress. Extensive experiments are conducted to evaluate MIA-SafeCompress on five datasets for both computer vision and natural language processing tasks. The results verify the effectiveness and generalization of our method. We also discuss how to adapt SafeCompress to other attacks besides MIA, demonstrating the flexibility of SafeCompress.",
		"archive_location": "WOS:001062775200014",
		"DOI": "10.1145/3551349.3556906",
		"event-title": "PROCEEDINGS OF THE 37TH IEEE/ACM INTERNATIONAL CONFERENCE ON AUTOMATED SOFTWARE ENGINEERING, ASE 2022",
		"ISBN": "1527-1366",
		"title": "Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software Deployment",
		"author": [
			{
				"family": "Zhu",
				"given": "J"
			},
			{
				"family": "Wang",
				"given": "LY"
			},
			{
				"family": "Han",
				"given": "X"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "fayollasSafeOpsConceptContinuous2020",
		"type": "paper-conference",
		"abstract": "Improved safety is one of the key benefits expected from autonomous vehicles. This can only be achieved if the autonomous vehicles are guaranteed to be safe enough. This paper proposes a potential approach contributing to this safety improvement: it describes and investigates \"SafeOps\", a concept of \"continuous safety\", based on the DevOps approach, unifying development and operations. DevOps consists in a set of practices intended to reduce the time between committing a change to a system and the change being deployed into production, while ensuring high quality. DevOps benefits to system development and delivery by enabling software continuous delivery, faster changes management with faster issues resolution, and improved reliability. SafeOps key principle is to monitor the system in operation and to use this information for validating and certifying a certain safety assurance level. Following this approach, a system could be compliant to a first safety assurance level when it's first delivered and compliant to higher ones when validated in operation.",
		"archive_location": "WOS:000630473500010",
		"DOI": "10.1109/EDCC51268.2020.00020",
		"event-title": "2020 16TH EUROPEAN DEPENDABLE COMPUTING CONFERENCE (EDCC 2020)",
		"ISBN": "978-1-72818-936-9",
		"page": "65-68",
		"title": "SafeOps: a concept of continuous safety",
		"author": [
			{
				"family": "Fayollas",
				"given": "C"
			},
			{
				"family": "Bonnin",
				"given": "H"
			},
			{
				"family": "Flebus",
				"given": "O"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sezenerInferringHumanValues2015",
		"type": "paper-conference",
		"abstract": "Aligning goals of superintelligent machines with human values is one of the ways to pursue safety in AGI systems. To achieve this, it is first necessary to learn what human values are. However, human values are incredibly complex and cannot easily be formalized by hand. In this work, we propose a general framework to estimate the values of a human given its behavior.",
		"archive_location": "WOS:000363479400016",
		"DOI": "10.1007/978-3-319-21365-1_16",
		"event-title": "ARTIFICIAL GENERAL INTELLIGENCE (AGI 2015)",
		"ISBN": "0302-9743",
		"page": "152-155",
		"title": "Inferring Human Values for Safe AGI Design",
		"volume": "9205",
		"author": [
			{
				"family": "Sezener",
				"given": "CE"
			}
		],
		"editor": [
			{
				"family": "Bieger",
				"given": "J"
			},
			{
				"family": "Goertzel",
				"given": "B"
			},
			{
				"family": "Potapov",
				"given": "A"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "wangSafeSelfRecoverableReinforcement2022",
		"type": "paper-conference",
		"abstract": "Reinforcement learning (RL) holds the promise of autonomous robots because it can adapt to dynamic or unknown environments by automatically learning optimal control policies from the interactions between robots and environments. However, the interactions can be unsafe to both robots and environments during the learning phase, which hinders the practical deployment of RL. Some safe RL methods have been proposed to improve the learning safety by using external or prior knowledge to guide safe actions, but it is difficult to assume having this knowledge in practical applications, especially in unknown environments. More importantly, considering failures are unavoidable in practice, current safe RL lacks the capability of recovering to safe states from failures so that the learning cannot be continued and finished. To solve these problems, we propose a safe and self-recoverable reinforcement learning framework that can predict and prohibit other unsafe actions based on known, explored unsafe actions during the exploration process, and can self-recover to a safe state when a failure occurs. The maze navigation simulation results show that our approach can not only significantly reduce the number of failures but also accelerate the convergence of reinforcement learning.",
		"archive_location": "WOS:000932071603168",
		"event-title": "2022 41ST CHINESE CONTROL CONFERENCE (CCC)",
		"ISBN": "2161-2927",
		"page": "3878-3883",
		"title": "A Safe and Self-Recoverable Reinforcement Learning Framework for Autonomous Robots",
		"DOI": "10.23919/CCC55666.2022.9901669",
		"author": [
			{
				"family": "Wang",
				"given": "WQ"
			},
			{
				"family": "Zhou",
				"given": "X"
			},
			{
				"family": "Xu",
				"given": "BL"
			},
			{
				"family": "Lu",
				"given": "ML"
			},
			{
				"family": "Zhang",
				"given": "YX"
			},
			{
				"family": "Gu",
				"given": "YH"
			}
		],
		"editor": [
			{
				"family": "Li",
				"given": "Z"
			},
			{
				"family": "Sun",
				"given": "J"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhuContactSafeReinforcementLearning2022",
		"type": "paper-conference",
		"abstract": "Reinforcement learning shows great potential to solve complex contact-rich robot manipulation tasks. However, the safety of using RL in the real world is a crucial problem, since unexpected dangerous collisions might happen when the RL policy is imperfect during training or in unseen scenarios. In this paper, we propose a contact-safe reinforcement learning framework for contact-rich robot manipulation, which maintains safety in both the task space and joint space. When the RL policy causes unexpected collisions between the robot arm and the environment, our framework is able to immediately detect the collision and ensure the contact force to be small. Furthermore, the end-effector is enforced to perform contact-rich tasks compliantly, while keeping robust to external disturbances. We train the RL policy in simulation and transfer it to the real robot. Real world experiments on robot wiping tasks show that our method is able to keep the contact force small both in task space and joint space even when the policy is under unseen scenario with unexpected collision, while rejecting the disturbances on the main task.",
		"archive_location": "WOS:000908368202013",
		"DOI": "10.1109/IROS47612.2022.9981185",
		"event-title": "2022 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)",
		"ISBN": "2153-0858",
		"page": "2476-2482",
		"title": "A Contact-Safe Reinforcement Learning Framework for Contact-Rich Robot Manipulation",
		"author": [
			{
				"family": "Zhu",
				"given": "X"
			},
			{
				"family": "Kang",
				"given": "SC"
			},
			{
				"family": "Chen",
				"given": "JY"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "moradiExploringFaultParameter2020",
		"type": "paper-conference",
		"abstract": "Assessing the safety of complex Cyber-Physical Systems (CPS) is a challenge in any industry. Fault Injection (FI) is a proven technique for safety analysis and is recommended by the automotive safety standard ISO 26262. Traditional FI methods require a considerable amount of effort and cost as FI is applied late in the development cycle and is driven by manual effort or random algorithms. In this paper, we propose a Reinforcement Learning (RL) approach to explore the fault space and find critical faults. During the learning process, the RL agent injects and parameterizes faults in the system to cause catastrophic behavior. The fault space is explored based on a reward function that evaluates previous simulation results such that the RL technique tries to predict improved fault timing and values. In this paper, we apply our technique on an Adaptive Cruise Controller with sensor fusion and compare the proposed method with Monte Carlo-based fault injection. The proposed technique is more efficient in terms of fault coverage and time to find the first critical fault.",
		"archive_location": "WOS:000853340600018",
		"DOI": "10.1109/DSN-W50199.2020.00028",
		"event-title": "50TH ANNUAL IEEE/IFIP INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS WORKSHOPS (DSN-W 2020)",
		"ISBN": "978-1-72817-263-7",
		"page": "102-109",
		"title": "Exploring Fault Parameter Space Using Reinforcement Learning-based Fault Injection",
		"author": [
			{
				"family": "Moradi",
				"given": "M"
			},
			{
				"family": "Oakes",
				"given": "BJ"
			},
			{
				"family": "Saraoglu",
				"given": "M"
			},
			{
				"family": "Morozov",
				"given": "A"
			},
			{
				"family": "Janschek",
				"given": "K"
			},
			{
				"family": "Denil",
				"given": "J"
			},
			{
				"literal": "IEEE Comp Soc"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kohjimaCensoredMarkovDecision2020",
		"type": "paper-conference",
		"abstract": "The importance of safe reinforcement learning (safe RL) is widely recognized for enhancing real world systems. In this study, we construct the censored Markov decision process (CeMDP), a new Markov Decision Process (MDP) framework that describes the interaction of environment, learner and external systems, e.g., human intervention or pre-designed controller for emergency response. We also theoretically analyze the relation of CeMDP to existing frameworks such as the semi-Markov decision process, MDP with Option (OMDP) and standard MDP; the analysis clarifies that CeMDP is a special case of OMDP and can, with environment redefinition, be represented by MDP. This finding allows us to design planning and reinforcement learning algorithms for CeMDP. We confirm the validity of the theory and algorithms by numerical experiments.",
		"archive_location": "WOS:000717663402145",
		"event-title": "2020 59TH IEEE CONFERENCE ON DECISION AND CONTROL (CDC)",
		"ISBN": "0743-1546",
		"page": "3623-3630",
		"title": "Censored Markov Decision Processes: A Framework for Safe Reinforcement Learning in Collaboration with External Systems",
		"DOI": "10.1109/CDC42340.2020.9304411",
		"author": [
			{
				"family": "Kohjima",
				"given": "M"
			},
			{
				"family": "Takahashi",
				"given": "M"
			},
			{
				"family": "Toda",
				"given": "H"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "perkinsLyapunovDesignSafe2003",
		"type": "article-journal",
		"abstract": "Lyapunov design methods are used widely in control engineering to design controllers that achieve qualitative objectives, such as stabilizing a system or maintaining a system's state in a desired operating range. We propose a method for constructing safe, reliable reinforcement learning agents based on Lyapunov design principles. In our approach, an agent learns to control a system by switching among a number of given, base-level controllers. These controllers are designed using Lyapunov domain knowledge so that any switching policy is safe and enjoys basic performance guarantees. Our approach thus ensures qualitatively satisfactory agent behavior for virtually any reinforcement learning algorithm and at all times, including while the agent is learning and taking exploratory actions. We demonstrate the process of designing safe agents for four different control problems. In simulation experiments, we find that our theoretically motivated designs also enjoy a number of practical benefits, including reasonable performance initially and throughout learning, and accelerated learning.",
		"archive_location": "WOS:000184926200009",
		"container-title": "JOURNAL OF MACHINE LEARNING RESEARCH",
		"DOI": "10.1162/jmlr.2003.3.4-5.803",
		"ISSN": "1532-4435",
		"issue": "4-5",
		"page": "803-832",
		"title": "Lyapunov design for safe reinforcement learning",
		"volume": "3",
		"author": [
			{
				"family": "Perkins",
				"given": "TJ"
			},
			{
				"family": "Barto",
				"given": "AG"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2003",
					5,
					15
				]
			]
		}
	},
	{
		"id": "mazouchiConflictAwareSafeReinforcement2022",
		"type": "article-journal",
		"abstract": "In this paper, a data-driven conflict-aware safe reinforcement learning (CAS-RL) algorithm is presented for control of autonomous systems. Existing safe RL results with predefined performance functions and safe sets can only provide safety and performance guarantees for a single environment or circumstance. By contrast, the presented CAS-RL algorithm provides safety and performance guarantees across a variety of circumstances that the system might encounter. This is achieved by utilizing a bilevel learning control architecture: A higher metacognitive layer leverages a data-driven receding-horizon attentional controller (RHAC) to adapt relative attention to different system's safety and performance requirements, and, a lower-layer RL controller designs control actuation signals for the system. The presented RHAC makes its meta decisions based on the reaction curve of the lower-layer RL controller using a meta-model or knowledge. More specifically, it leverages a prediction meta-model (PMM) which spans the space of all future meta trajectories using a given finite number of past meta trajectories. RHAC will adapt the system's aspiration towards performance metrics (e.g., performance weights) as well as safety boundaries to resolve conflicts that arise as mission scenarios develop. This will guarantee safety and feasibility (i.e., performance boundness) of the lower-layer RL-based control solution. It is shown that the interplay between the RHAC and the lower-layer RL controller is a bilevel optimization problem for which the leader (RHAC) operates at a lower rate than the follower (RL-based controller) and its solution guarantees feasibility and safety of the control solution. The effectiveness of the proposed framework is verified through a simulation example.",
		"archive_location": "WOS:000735515700010",
		"container-title": "IEEE-CAA JOURNAL OF AUTOMATICA SINICA",
		"DOI": "10.1109/JAS.2021.1004353",
		"ISSN": "2329-9266",
		"issue": "3",
		"page": "466-481",
		"title": "Conflict-Aware Safe Reinforcement Learning: A Meta-Cognitive Learning Framework",
		"volume": "9",
		"author": [
			{
				"family": "Mazouchi",
				"given": "M"
			},
			{
				"family": "Nageshrao",
				"given": "S"
			},
			{
				"family": "Modares",
				"given": "H"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					3
				]
			]
		}
	},
	{
		"id": "shethProcessKnowledgeInfusedAI2022",
		"type": "article-journal",
		"abstract": "AI has seen wide adoption for automating tasks in several domains. However, AI's use in high-value, sensitive, or safety-critical applications such as self-management for personalized health or personalized nutrition has been challenging. These require that the AI system follows guidelines or well-defined processes set by experts, community, or standards. We characterize these as process knowledge (PK). For example, to diagnose the severity of depression, the AI system should incorporate PK that is part of the clinical decision-making process, such as the Patient Health Questionnaire (PHQ-9). Likewise, a nutritionist's knowledge and dietary guidelines are needed to create food plans for diabetic patients. Furthermore, the BlackBox nature of purely data-reliant statistical AI systems falls short in providing user-understandable explanations, such as what a clinician would need to ensure and document compliance with medical guidelines before relying on a recommendation. Using the examples of mental health and cooking recipes for diabetic patients, we show why, what, and how to incorporate PK along with domain knowledge in machine learning. We discuss methods for infusing PK and present performance evaluation metrics. Support for safety and user-level explainability of the PK-infused learning improves confidence and trust in the AI system.",
		"archive_location": "WOS:000853843100019",
		"container-title": "IEEE INTERNET COMPUTING",
		"DOI": "10.1109/MIC.2022.3182349",
		"ISSN": "1089-7801",
		"issue": "5",
		"page": "76-84",
		"title": "Process Knowledge-Infused AI: Toward User-Level Explainability, Interpretability, and Safety",
		"volume": "26",
		"author": [
			{
				"family": "Sheth",
				"given": "A"
			},
			{
				"family": "Gaur",
				"given": "M"
			},
			{
				"family": "Roy",
				"given": "K"
			},
			{
				"family": "Venkataraman",
				"given": "R"
			},
			{
				"family": "Khandelwal",
				"given": "V"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					1
				]
			]
		}
	},
	{
		"id": "huntVerifiablySafeExploration2021",
		"type": "paper-conference",
		"abstract": "Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We characterize safety constraints in terms of a refinement relation on Markov decision processes - rather than directly constraining the reinforcement learning algorithm so that it only takes safe actions, we instead refine the environment so that only safe actions are defined in the environment's transition structure. This has pragmatic system design benefits and, more importantly, provides a clean conceptual setting in which we are able to prove important safety and efficiency properties. These allow us to transform the constrained optimization problem of acting safely in the original environment into an unconstrained optimization in a refined environment.",
		"archive_location": "WOS:000932821700017",
		"DOI": "10.1145/3447928.3456653",
		"event-title": "HSCC2021: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON HYBRID SYSTEMS: COMPUTATION AND CONTROL (PART OF CPS-IOT WEEK)",
		"ISBN": "978-1-4503-8339-4",
		"title": "Verifiably Safe Exploration for End-to-End Reinforcement Learning",
		"author": [
			{
				"family": "Hunt",
				"given": "N"
			},
			{
				"family": "Fulton",
				"given": "N"
			},
			{
				"family": "Magliacane",
				"given": "S"
			},
			{
				"family": "Hoang",
				"given": "TN"
			},
			{
				"family": "Das",
				"given": "S"
			},
			{
				"family": "Solar-Lezama",
				"given": "A"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "rileyAssuredDeepMultiAgent2022",
		"type": "paper-conference",
		"abstract": "Using multi-agent reinforcement learning to find solutions to complex decision-making problems in shared environments has become standard practice in many scenarios. However, this is not the case in safety-critical scenarios, where the reinforcement learning process, which uses stochastic mechanisms, could lead to highly unsafe outcomes. We proposed a novel, safe multi-agent reinforcement learning approach named Assured Multi-Agent Reinforcement Learning (AMARL) to address this issue. Distinct from other safe multi-agent reinforcement learning approaches, AMARL utilises quantitative verification, a model checking technique that guarantees agent compliance of safety, performance, and non-functional requirements, both during and after the learning process. We have previously evaluated AMARL in patrolling domains with various multi-agent reinforcement learning algorithms for both homogeneous and heterogeneous systems. In this work we extend AMARL through the use of deep multi-agent reinforcement learning. This approach is particularly appropriate for systems in which the rewards are sparse and hence extends the applicability of AMARL. We evaluate our approach within a new search and collection domain which demonstrates promising results in safety standards and performance compared to algorithms not using AMARL.",
		"archive_location": "WOS:000876376200008",
		"DOI": "10.1007/978-3-031-10161-8_8",
		"event-title": "AGENTS AND ARTIFICIAL INTELLIGENCE, ICAART 2021",
		"ISBN": "0302-9743",
		"page": "158-180",
		"title": "Assured Deep Multi-Agent Reinforcement Learning for Safe Robotic Systems",
		"volume": "13251",
		"author": [
			{
				"family": "Riley",
				"given": "J"
			},
			{
				"family": "Calinescu",
				"given": "R"
			},
			{
				"family": "Paterson",
				"given": "C"
			},
			{
				"family": "Kudenko",
				"given": "D"
			},
			{
				"family": "Banks",
				"given": "A"
			}
		],
		"editor": [
			{
				"family": "Rocha",
				"given": "AP"
			},
			{
				"family": "Steels",
				"given": "L"
			},
			{
				"family": "VanDenHerik",
				"given": "J"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schumegProposedVModelVerification2023",
		"type": "paper-conference",
		"abstract": "The Department of Defense strives to continuously develop and acquire systems that utilize novel technologies and methods for implementing new and complex mission requirements. One of the identified technologies with high impact and benefit to the Warfighter is the integration of Artificial Intelligence (AI) and Machine Learning (ML). Current AI models and methods have added layers of complexity to achieving a satisfactory level of verification and validation (V&V), possibly resulting in elevated risks with fewer mitigations. Regardless of the type of applications for AI technology within the DoD, the technology implementation must be verified, validated, and ultimately any residual risks accepted. This paper looks to introduce a V-model concept for Artificial Intelligence and Machine Learning, to include an outline of proposed activities that the development, assurance, and evaluation communities can follow. By following this proposed assessment, these organizations can increase their understanding and knowledge of the system, mitigating risk and helping to achieve justified confidence.",
		"archive_location": "WOS:001050787900009",
		"DOI": "10.1109/ICAA58325.2023.00017",
		"event-title": "2023 IEEE INTERNATIONAL CONFERENCE ON ASSURED AUTONOMY, ICAA",
		"ISBN": "979-8-3503-2601-7",
		"page": "61-66",
		"title": "Proposed V-Model for Verification, Validation, and Safety Activities for Artificial Intelligence",
		"author": [
			{
				"family": "Schumeg",
				"given": "B"
			},
			{
				"family": "Marotta",
				"given": "F"
			},
			{
				"family": "Werner",
				"given": "B"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "deyMultilayeredReviewSafety2021",
		"type": "article-journal",
		"abstract": "The unprecedented advancement of artificial intelligence (AI) in recent years has altered our perspectives on software engineering and systems engineering as a whole. Nowadays, software-intensive intelligent systems rely more on a learning model than thousands of lines of codes. Such alteration has led to new research challenges in the engineering process that can ensure the safe and beneficial behavior of AI systems. This paper presents a literature survey of the significant efforts made in the last fifteen years to foster safety in complex intelligent systems. This survey covers relevant aspects of AI safety research including safety requirements engineering, safety-driven design at both system and machine learning (ML) component level, validation and verification from the perspective of software and system engineers. We categorize these research efforts based on a three-layered conceptual framework for developing and maintaining AI systems. We also perform a gap analysis to emphasize the open research challenges in ensuring safe AI. Finally, we conclude the paper by providing future research directions and a road map for AI safety. (C) 2021 Elsevier Inc. All rights reserved.",
		"archive_location": "WOS:000636371400004",
		"container-title": "JOURNAL OF SYSTEMS AND SOFTWARE",
		"DOI": "10.1016/j.jss.2021.110941",
		"ISSN": "0164-1212",
		"title": "Multilayered review of safety approaches for machine learning-based systems in the days of AI",
		"volume": "176",
		"author": [
			{
				"family": "Dey",
				"given": "S"
			},
			{
				"family": "Lee",
				"given": "SW"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					6
				]
			]
		}
	},
	{
		"id": "gheraibiaSafetyAINovel2019",
		"type": "article-journal",
		"abstract": "Safety-critical systems are becoming larger and more complex to obtain a higher level of functionality. Hence, modeling and evaluation of these systems can be a difficult and error-prone task. Among existing safety models, Fault Tree Analysis (FTA) is one of the well-known methods in terms of easily understandable graphical structure. This study proposes a novel approach by using Machine Learning (ML) and real-time operational data to learn about the normal behavior of the system. Afterwards, if any abnormal situation arises with reference to the normal behavior model, the approach tries to find the explanation of the abnormality on the fault tree and then share the knowledge with the operator. If the fault tree fails to explain the situation, a number of different recommendations, including the potential repair of the fault tree, are provided based on the nature of the situation. A decision tree is utilized for this purpose. The effectiveness of the proposed approach is shown through a hypothetical example of an Aircraft Fuel Distribution System (AFDS).",
		"archive_location": "WOS:000563954900139",
		"container-title": "IEEE ACCESS",
		"DOI": "10.1109/ACCESS.2019.2941566",
		"ISSN": "2169-3536",
		"page": "135855-135869",
		"title": "Safety AI: A Novel Approach to Update Safety Models Using Artificial Intelligence",
		"volume": "7",
		"author": [
			{
				"family": "Gheraibia",
				"given": "Y"
			},
			{
				"family": "Kabir",
				"given": "S"
			},
			{
				"family": "Aslansefat",
				"given": "K"
			},
			{
				"family": "Sorokos",
				"given": "I"
			},
			{
				"family": "Papadopoulos",
				"given": "Y"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "johnsonMetacognitionArtificialIntelligence2022",
		"type": "article-journal",
		"abstract": "Advances in computational thinking and data science have led to a new era of artificial intelligence systems being engineered to adapt to complex situations and develop actionable knowledge. These learning systems are meant to reliably understand the essence of a situation and construct critical decision recommendations to support autonomous and human-machine teaming operations. In parallel, the increasing volume, velocity, variety, ve-racity, value, and variability of data is confounding the complexity of these new systems - creating challenges in terms of their development and implementation. For artificial systems supporting critical decisions with higher consequences, safety has become an important concern. Methods are needed to avoid failure modes and ensure that only desired behavior is permitted. This paper discusses an approach that promotes self-awareness, or metacognition, within the artificial intelligence systems to understand their external and internal operational environments and use this knowledge to identify potential failures and enable self-healing and self-management for safe and desired behavior.",
		"archive_location": "WOS:000792913700003",
		"container-title": "SAFETY SCIENCE",
		"DOI": "10.1016/j.ssci.2022.105743",
		"ISSN": "0925-7535",
		"title": "Metacognition for artificial intelligence system safety-An approach to safe and desired behavior",
		"volume": "151",
		"author": [
			{
				"family": "Johnson",
				"given": "B"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					7
				]
			]
		}
	},
	{
		"id": "ganRiskDegreebasedSafe2016",
		"type": "article-journal",
		"abstract": "Semi-supervised learning has attracted much attention in machine learning field over the past decades and a number of algorithms are proposed to improve the performance by exploiting unlabeled data. However, unlabeled data may hurt performance of semi-supervised learning in some cases. It is instinctively expected to design a reasonable strategy to safety exploit unlabeled data. To address the problem, we introduce a safe semi-supervised learning by analyzing the different characteristics of unlabeled data in supervised and semi-supervised learning. Our intuition is that unlabeled data may be often risky in semi-supervised setting and the risk degree are different. Hence, we assign different risk degree to unlabeled data and the risk degree serve as a sieve to determine the exploiting way of unlabeled data. The unlabeled data with high risk should be exploited by supervised learning and the other should be used for semi-supervised learning. In particular, we utilize kernel minimum squared error (KMSE) and Laplacian regularized KMSE for supervised and semi-supervised learning, respectively. Experimental results on several benchmark datasets illustrate the performance of our algorithm is never inferior to that of KMSE and indicate the effectiveness and efficiency of our algorithm.",
		"archive_location": "WOS:000368167400006",
		"container-title": "INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS",
		"DOI": "10.1007/s13042-015-0416-8",
		"ISSN": "1868-8071",
		"issue": "1",
		"page": "85-94",
		"title": "A risk degree-based safe semi-supervised learning algorithm",
		"volume": "7",
		"author": [
			{
				"family": "Gan",
				"given": "HT"
			},
			{
				"family": "Luo",
				"given": "ZZ"
			},
			{
				"family": "Meng",
				"given": "M"
			},
			{
				"family": "Ma",
				"given": "YL"
			},
			{
				"family": "She",
				"given": "QS"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2016",
					2
				]
			]
		}
	},
	{
		"id": "carlsonProvablySafeArtificial2021",
		"type": "article-journal",
		"abstract": "Methods are currently lacking to prove artificial general intelligence (AGI) safety. An AGI 'hard takeoff' is possible, in which first generation AGI(1) rapidly triggers a succession of more powerful AGI(n) that differ dramatically in their computational capabilities (AGI(n) << AGI(n+1)). No proof exists that AGI will benefit humans or of a sound value-alignment method. Numerous paths toward human extinction or subjugation have been identified. We suggest that probabilistic proof methods are the fundamental paradigm for proving safety and value-alignment between disparately powerful autonomous agents. Interactive proof systems (IPS) describe mathematical communication protocols wherein a Verifier queries a computationally more powerful Prover and reduces the probability of the Prover deceiving the Verifier to any specified low probability (e.g., 2(-100)). IPS procedures can test AGI behavior control systems that incorporate hard-coded ethics or value-learning methods. Mapping the axioms and transformation rules of a behavior control system to a finite set of prime numbers allows validation of 'safe' behavior via IPS number-theoretic methods. Many other representations are needed for proving various AGI properties. Multi-prover IPS, program-checking IPS, and probabilistically checkable proofs further extend the paradigm. In toto, IPS provides a way to reduce AGI(n) <-> AGI(n+1) interaction hazards to an acceptably low level.",
		"archive_location": "WOS:000738051700001",
		"container-title": "PHILOSOPHIES",
		"DOI": "10.3390/philosophies6040083",
		"ISSN": "2409-9287",
		"issue": "4",
		"title": "Provably Safe Artificial General Intelligence via Interactive Proofs",
		"volume": "6",
		"author": [
			{
				"family": "Carlson",
				"given": "K"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					12
				]
			]
		}
	},
	{
		"id": "zhaoSafeSemisupervisedClassification2018",
		"type": "article-journal",
		"abstract": "In order to improve the performance of semi-supervised learning, a kind of safe semi-supervised classification algorithm based active learning sampling strategy is proposed. First, an active learning sampling method based on uncertainty and representativenes is designed. The weighted algorithm combining the uncertainty and representativenesss is used to select the unlabeled samples with rich information and representation, providing for semi-supervised learning. Second, a method of label prediction based on grouping verification is designed. Prelabeling is executed on unlabeled sample selected by active learning. The sample with pseudo-label is added into the labeled sample set to carry out grouping, training and testing. The corresponding errors of various pseudo-labels are calculated and the pseudo-label making the accuracy least is selected as the candidate label of the unlabeled sample. Third, a method of security verification is designed. Only the label making the accuracy lower than before is selected as the final label of the unlabeled sample to expand the number of labeled samples. Iterations are repeatedly executed until a certain precision is met. Finally, the classifier is trained using the final labeled set. The experiments are carried out on semi-supervised datasets and UCI datasets, and the results show that the proposed algorithms are effective.",
		"archive_location": "WOS:000451338400007",
		"container-title": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS",
		"DOI": "10.3233/JIFS-169722",
		"ISSN": "1064-1246",
		"issue": "4",
		"page": "4001-4010",
		"title": "Safe semi-supervised classification algorithm combined with active learning sampling strategy",
		"volume": "35",
		"author": [
			{
				"family": "Zhao",
				"given": "JH"
			},
			{
				"family": "Liu",
				"given": "N"
			},
			{
				"family": "Malov",
				"given": "A"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "barzaminiMultilevelSemanticWeb2022",
		"type": "article-journal",
		"abstract": "Machine Learning (ML) algorithms are widely used in building software-intensive systems, including safety-critical ones. Unlike traditional software components, Machine-Learned Components (MLC)s, software components built using ML algorithms, learn their specifications through generalizing the common features that they find in a limited set of collected examples. While this inductive nature overcomes the limitations of programming hard-to-specify concepts, the same feature becomes problematic for verifying safety in ML-based software systems. One reason is that, due to MLCs data-driven nature, there is often no set of explicitly written and pre-defined specifications, against which the MLC can be verified. In this regard, we propose to partially specify hard-to-specify domain concepts, which MLCs tend to classify, instead of fully relying on their inductive learning ability from arbitrarily-collected datasets. In this paper, we propose a semi-automated approach to construct a multi-level semantic web to partially outline the hard-to-specify, yet crucial, domain concept \"pedestrian\" in automotive domain. We evaluate the applicability of the generated semantic web in two ways: first, with a reference to the web, we augment a pedestrian dataset for a missing feature, wheelchair, to show training a state-of-the-art ML-based object detector on the augmented dataset improves its accuracy in detecting pedestrians; second, we evaluate the coverage of the generated semantic web based on multiple state-of-the-art pedestrian and human datasets.",
		"archive_location": "WOS:000740138800001",
		"container-title": "REQUIREMENTS ENGINEERING",
		"DOI": "10.1007/s00766-021-00366-0",
		"ISSN": "0947-3602",
		"issue": "2",
		"page": "161-182",
		"title": "A multi-level semantic web for hard-to-specify domain concept, Pedestrian, in ML-based software",
		"volume": "27",
		"author": [
			{
				"family": "Barzamini",
				"given": "H"
			},
			{
				"family": "Shahzad",
				"given": "M"
			},
			{
				"family": "Alhoori",
				"given": "H"
			},
			{
				"family": "Rahimi",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					6
				]
			]
		}
	},
	{
		"id": "phanNeuralSimplexArchitecture2020",
		"type": "paper-conference",
		"abstract": "We present the Neural Simplex Architecture (NSA), a new approach to runtime assurance that provides safety guarantees for neural controllers (obtained e.g. using reinforcement learning) of autonomous and other complex systems without unduly sacrificing performance. NSA is inspired by the Simplex control architecture of Sha et al., but with some significant differences. In the traditional approach, the advanced controller (AC) is treated as a black box; when the decision module switches control to the baseline controller (BC), the BC remains in control forever. There is relatively little work on switching control back to the AC, and there are no techniques for correcting the AC's behavior after it generates a potentially unsafe control input that causes a failover to the BC. Our NSA addresses both of these limitations. NSA not only provides safety assurances in the presence of a possibly unsafe neural controller, but can also improve the safety of such a controller in an online setting via retraining, without overly degrading its performance. To demonstrate NSA's benefits, we have conducted several significant case studies in the continuous control domain. These include a target-seeking ground rover navigating an obstacle field, and a neural controller for an artificial pancreas system.",
		"archive_location": "WOS:000890074700006",
		"DOI": "10.1007/978-3-030-55754-6_6",
		"event-title": "NASA FORMAL METHODS (NFM 2020)",
		"ISBN": "0302-9743",
		"page": "97-114",
		"title": "Neural Simplex Architecture",
		"volume": "12229",
		"author": [
			{
				"family": "Phan",
				"given": "DT"
			},
			{
				"family": "Grosu",
				"given": "R"
			},
			{
				"family": "Jansen",
				"given": "N"
			},
			{
				"family": "Paoletti",
				"given": "N"
			},
			{
				"family": "Smolka",
				"given": "SA"
			},
			{
				"family": "Stoller",
				"given": "SD"
			}
		],
		"editor": [
			{
				"family": "Lee",
				"given": "R"
			},
			{
				"family": "Jha",
				"given": "S"
			},
			{
				"family": "Mavridou",
				"given": "A"
			},
			{
				"family": "Giannakopoulou",
				"given": "D"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wenConstrainedCrossEntropyMethod2021",
		"type": "article-journal",
		"abstract": "We study a safe reinforcement learning problem, in which the constraints are defined as the expected cost over finite-length trajectories. We propose a constrained cross-entropy-based method to solve this problem. The key idea is to transform the original constrained optimization problem into an unconstrained one with a surrogate objective. The method explicitly tracks its performance with respect to constraint satisfaction and thus is well suited for safety-critical applications. We show that the asymptotic behavior of the proposed algorithm can be almost-surely described by that of an ordinary differential equation. Then, we give sufficient conditions on the properties of this differential equation for the convergence of the proposed algorithm. At last, we show the performance of the proposed algorithm in two simulation examples. In a constrained linear-quadratic regulator example, we observe that the algorithm converges to the global optimum with high probability. In a 2-D navigation example, we find that the algorithm effectively learns feasible policies without assumptions on the feasibility of initial policies, even with non-Markovian objective functions and constraint functions.",
		"archive_location": "WOS:000668858300014",
		"container-title": "IEEE TRANSACTIONS ON AUTOMATIC CONTROL",
		"DOI": "10.1109/TAC.2020.3015931",
		"ISSN": "0018-9286",
		"issue": "7",
		"page": "3123-3137",
		"title": "Constrained Cross-Entropy Method for Safe Reinforcement Learning",
		"volume": "66",
		"author": [
			{
				"family": "Wen",
				"given": "M"
			},
			{
				"family": "Topcu",
				"given": "U"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					7
				]
			]
		}
	},
	{
		"id": "fultonVerifiablySafeOffModel2019",
		"type": "paper-conference",
		"abstract": "The desire to use reinforcement learning in safety-critical settings has inspired a recent interest in formal methods for learning algorithms. Existing formal methods for learning and optimization primarily consider the problem of constrained learning or constrained optimization. Given a single correct model and associated safety constraint, these approaches guarantee efficient learning while provably avoiding behaviors outside the safety constraint. Acting well given an accurate environmental model is an important pre-requisite for safe learning, but is ultimately insufficient for systems that operate in complex heterogeneous environments. This paper introduces verification-preserving model updates, the first approach toward obtaining formal safety guarantees for reinforcement learning in settings where multiple possible environmental models must be taken into account. Through a combination of inductive data and deductive proving with design-time model updates and runtime model falsification, we provide a first approach toward obtaining formal safety proofs for autonomous systems acting in heterogeneous environments.",
		"archive_location": "WOS:000681166500028",
		"DOI": "10.1007/978-3-030-17462-0_28",
		"event-title": "TOOLS AND ALGORITHMS FOR THE CONSTRUCTION AND ANALYSIS OF SYSTEMS, PT I",
		"ISBN": "0302-9743",
		"page": "413-430",
		"title": "Verifiably Safe Off-Model Reinforcement Learning",
		"volume": "11427",
		"author": [
			{
				"family": "Fulton",
				"given": "N"
			},
			{
				"family": "Platzer",
				"given": "A"
			}
		],
		"editor": [
			{
				"family": "Vojnar",
				"given": "T"
			},
			{
				"family": "Zhang",
				"given": "L"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "yavasRealWorldReinforcementLearning2023",
		"type": "article-journal",
		"abstract": "Lane-change decision-making for vehicles is a challenging task for many reasons, including traffic rules, safety, and the stochastic nature of driving. Because of its success in solving complex problems, deep reinforcement learning (DRL) has been suggested for addressing these issues. However, the studies on DRL to date have gone no further than validation in simulation and failed to address what are arguably the most critical issues, namely, the mismatch between simulation and reality, human-likeness, and safety. This paper introduces a real-world DRL framework for decision-making to design safe and human-like agents that can operate in the real world without extra tuning. We propose a new learning paradigm for DRL integrated with Real2Sim transfer, which comprises training, validation, and testing phases. The approach involves two simulator environments with different levels of fidelity, which are parameterized via real-world data. Within the framework, a large amount of randomized experience is generated with a low-fidelity simulator, whereupon the learned skills are validated regularly in a high-fidelity simulator to avoid overfitting. Finally, in the testing phase, the agent is examined concerning safety and human-like decision-making. Extensive simulation and real-world evaluations show the superiority of the proposed approach. To the best of the authors' knowledge, this is the first application of DRL lane-changing policy in the real world.",
		"archive_location": "WOS:001040652400001",
		"container-title": "IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS",
		"DOI": "10.1109/TITS.2023.3292981",
		"ISSN": "1524-9050",
		"title": "A Real-World Reinforcement Learning Framework for Safe and Human-Like Tactical Decision-Making",
		"author": [
			{
				"family": "Yavas",
				"given": "MU"
			},
			{
				"family": "Kumbasar",
				"given": "T"
			},
			{
				"family": "Ure",
				"given": "NK"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					25
				]
			]
		}
	},
	{
		"id": "lecerfAutomaticallyLearningFallback2022",
		"type": "paper-conference",
		"abstract": "When learning to behave in a stochastic environment where safety is critical, such as driving a vehicle in traffic, it is natural for human drivers to plan fallback strategies as a backup to use if ever there is an unexpected change in the environment. Knowing to expect the unexpected, and planning for such outcomes, increases our capability for being robust to unseen scenarios and may help prevent catastrophic failures. Control of Autonomous Vehicles (AVs) has a particular interest in knowing when and how to use fallback strategies in the interest of safety. Due to imperfect information available to an AV about its environment, it is important to have alternate strategies at the ready which might not have been deduced from the original training data distribution.\nIn this paper we present a principled approach for a model-free Reinforcement Learning (RL) agent to capture multiple modes of behaviour in an environment. We introduce an extra pseudo-reward term to the reward model, to encourage exploration to areas of state-space different from areas privileged by the optimal policy. We base this reward term on a distance metric between the trajectories of agents, in order to force policies to focus on different areas of state-space than the initial exploring agent. Throughout the paper, we refer to this particular training paradigm as learning fallback strategies.\nWe apply this method to an autonomous driving scenario and show that we are able to learn useful policies that would have otherwise been missed out on during training, and would have been unavailable to the agent when executing the control algorithm.",
		"archive_location": "WOS:001053939400033",
		"DOI": "10.1145/3529399.3529432",
		"event-title": "PROCEEDINGS OF 2022 7TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING TECHNOLOGIES, ICMLT 2022",
		"ISBN": "978-1-4503-9574-8",
		"page": "209-215",
		"title": "Automatically Learning Fallback Strategies with Model-Free Reinforcement Learning in Safety-Critical Driving Scenarios",
		"author": [
			{
				"family": "Lecerf",
				"given": "UUL"
			},
			{
				"family": "Yemdji-Tchassi",
				"given": "CCY"
			},
			{
				"family": "Aubert",
				"given": "SSA"
			},
			{
				"family": "Michiardi",
				"given": "PPM"
			},
			{
				"literal": "ACM"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kimEvaluatingCorrectnessReinforcement2022",
		"type": "paper-conference",
		"abstract": "Deep learning is used for decision making and functional control in various fields, such as autonomous systems. However, rather than being developed by logical design, deep learning models are trained by itself through learning data. Moreover, only reward values are used to evaluate its performance, which does not provide enough information that the model learned properly. This paper proposes a new method to assess the correctness of reinforcement learning, considering other properties of the learning algorithm. The proposed method is applied for the evaluation of ActorCritic Algorithms, and correctness-related insights of the algorithm are confirmed through experiments.",
		"archive_location": "WOS:000855059600065",
		"DOI": "10.1109/ICUFN55119.2022.9829571",
		"event-title": "2022 THIRTEENTH INTERNATIONAL CONFERENCE ON UBIQUITOUS AND FUTURE NETWORKS (ICUFN)",
		"ISBN": "2165-8528",
		"page": "320-325",
		"title": "Evaluating Correctness of Reinforcement Learning based on Actor-Critic Algorithm",
		"author": [
			{
				"family": "Kim",
				"given": "Y"
			},
			{
				"family": "Hussain",
				"given": "M"
			},
			{
				"family": "Suh",
				"given": "JW"
			},
			{
				"family": "Hong",
				"given": "JE"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "fischerSamplingbasedInverseReinforcement2021",
		"type": "paper-conference",
		"abstract": "Planning for robotic systems is frequently formulated as an optimization problem. Instead of manually tweaking the parameters of the cost function, they can be learned from human demonstrations by Inverse Reinforcement Learning (IRL). Common IRL approaches employ a maximum entropy trajectory distribution that can be learned with soft reinforcement learning, where the reward maximization is regularized with an entropy objective. The consideration of safety constraints is of paramount importance for human-robot collaboration. For this reason, our work addresses maximum entropy IRL in constrained environments. Our contribution to this research area is threefold: (1) We propose Constrained Soft Reinforcement Learning (CSRL), an extension of soft reinforcement learning to Constrained Markov Decision Processes (CMDPs). (2) We transfer maximum entropy IRL to CMDPs based on CSRL. (3) We show that using importance sampling in maximum entropy IRL in constrained environments introduces a bias and fails to achieve feature matching. In our evaluation we consider the tactical lane change decision of an autonomous vehicle in a highway scenario modeled in the SUMO traffic simulation.",
		"archive_location": "WOS:000755125500079",
		"DOI": "10.1109/IROS51168.2021.9636672",
		"event-title": "2021 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)",
		"ISBN": "2153-0858",
		"page": "791-798",
		"title": "Sampling-based Inverse Reinforcement Learning Algorithms with Safety Constraints",
		"author": [
			{
				"family": "Fischer",
				"given": "J"
			},
			{
				"family": "Eyberg",
				"given": "C"
			},
			{
				"family": "Werling",
				"given": "M"
			},
			{
				"family": "Lauer",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "liBridgingModelbasedSafety2022",
		"type": "paper-conference",
		"abstract": "Bridging model-based safety and model-free reinforcement learning (RL) for dynamic robots is appealing since model-based methods are able to provide formal safety guarantees, while RL-based methods are able to exploit the robot agility by learning from the full-order system dynamics. However, current approaches to tackle this problem are mostly restricted to simple systems. In this paper, we propose a new method to combine model-based safety with model-free reinforcement learning by explicitly finding a low-dimensional model of the system controlled by a RL policy and applying stability and safety guarantees on that simple model. We use a complex bipedal robot Cassie, which is a high dimensional nonlinear system with hybrid dynamics and underactuation, and its RL-based walking controller as an example. We show that a low-dimensional dynamical model is sufficient to capture the dynamics of the closed-loop system. We demonstrate that this model is linear, asymptotically stable, and is decoupled across control input in all dimensions. We further exemplify that such linearity exists even when using different RL control policies. Such results point out an interesting direction to understand the relationship between RL and optimal control: whether RL tends to linearize the nonlinear system during training in some cases. Furthermore, we illustrate that the found linear model is able to provide guarantees by safety-critical optimal control framework, e.g., Model Predictive Control with Control Barrier Functions, on an example of autonomous navigation using Cassie while taking advantage of the agility provided by the RL-based controller.",
		"archive_location": "WOS:000827625700033",
		"event-title": "ROBOTICS: SCIENCE AND SYSTEM XVIII",
		"ISBN": "2330-7668",
		"title": "Bridging Model-based Safety and Model-free Reinforcement Learning through System Identification of Low Dimensional Linear Models",
		"URL": "https://arxiv.org/abs/2205.05787",
		"author": [
			{
				"family": "Li",
				"given": "ZY"
			},
			{
				"family": "Zeng",
				"given": "J"
			},
			{
				"family": "Thirugnanam",
				"given": "A"
			},
			{
				"family": "Sreenath",
				"given": "K"
			}
		],
		"editor": [
			{
				"family": "Hauser",
				"given": "K"
			},
			{
				"family": "Shell",
				"given": "D"
			},
			{
				"family": "Huang",
				"given": "S"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rauschAutoencoderBasedSemanticNovelty2021",
		"type": "article-journal",
		"abstract": "Many autonomous systems, such as driverless taxis, perform safety-critical functions. Autonomous systems employ artificial intelligence (AI) techniques, specifically for environmental perception. Engineers cannot completely test or formally verify AI-based autonomous systems. The accuracy of AI-based systems depends on the quality of training data. Thus, novelty detection, that is, identifying data that differ in some respect from the data used for training, becomes a safety measure for system development and operation. In this study, we propose a new architecture for autoencoder-based semantic novelty detection with two innovations: architectural guidelines for a semantic autoencoder topology and a semantic error calculation as novelty criteria. We demonstrate that such a semantic novelty detection outperforms autoencoder-based novelty detection approaches known from the literature by minimizing false negatives.",
		"archive_location": "WOS:000719108200001",
		"container-title": "APPLIED SCIENCES-BASEL",
		"DOI": "10.3390/app11219881",
		"ISSN": "2076-3417",
		"issue": "21",
		"title": "Autoencoder-Based Semantic Novelty Detection: Towards Dependable AI-Based Systems",
		"volume": "11",
		"author": [
			{
				"family": "Rausch",
				"given": "A"
			},
			{
				"family": "Sedeh",
				"given": "AM"
			},
			{
				"family": "Zhang",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					11
				]
			]
		}
	},
	{
		"id": "chenBinFIEfficientFault2019",
		"type": "paper-conference",
		"abstract": "As machine learning (ML) becomes pervasive in high performance computing, ML has found its way into safety-critical domains (e.g., autonomous vehicles). Thus the reliability of ML has grown in importance. Specifically, failures of ML systems can have catastrophic consequences, and can occur due to soft errors, which are increasing in frequency due to system scaling. Therefore, we need to evaluate ML systems in the presence of soft errors.\nIn this work, we propose Biel, an efficient fault injector (FI) for finding the safety-critical bits in ML applications. We find the widely-used ML computations are often monotonic. Thus we can approximate the error propagation behavior of a ML application as a monotonic function. BinFI uses a binary-search like FI technique to pinpoint the safety-critical bits (also measure the overall resilience). BinFI identifies 99.56% of safety-critical bits (with 99.63% precision) in the systems, which significantly outperforms random FI, with much lower costs.",
		"archive_location": "WOS:000545976800069",
		"DOI": "10.1145/3295500.3356177",
		"event-title": "PROCEEDINGS OF SC19: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS",
		"ISBN": "978-1-4503-6229-0",
		"title": "BinFI: An Efficient Fault Injector for Safety-Critical Machine Learning Systems",
		"author": [
			{
				"family": "Chen",
				"given": "ZT"
			},
			{
				"family": "Li",
				"given": "GP"
			},
			{
				"family": "Pattabiraman",
				"given": "K"
			},
			{
				"family": "DeBardeleben",
				"given": "N"
			},
			{
				"literal": "Assoc Comp Machinery"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "polymenakosSafePolicySearch2019",
		"type": "paper-conference",
		"abstract": "We propose a method to optimise the parameters of a policy which will be used to safely perform a given task in a data-efficient manner. We train a Gaussian process model to capture the system dynamics, based on the PILCO framework. The model has useful analytic properties, which allow closed form computation of error gradients and the probability of violating given state space constraints. Even during training, only policies that are deemed safe are implemented on the real system, minimising the risk of catastrophic failure.",
		"archive_location": "WOS:000474345000180",
		"event-title": "AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS",
		"ISBN": "978-1-4503-6309-9",
		"page": "1565-1573",
		"title": "Safe Policy Search Using Gaussian Process Models",
		"URL": "https://arxiv.org/abs/1712.05556",
		"author": [
			{
				"family": "Polymenakos",
				"given": "K"
			},
			{
				"family": "Abate",
				"given": "A"
			},
			{
				"family": "Roberts",
				"given": "S"
			},
			{
				"literal": "Assoc Comp Machinery"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "treacyMechanismsConstraintsUnderpinning2021",
		"type": "paper-conference",
		"abstract": "The unpredictability of artificial intelligence (AI) services and products pose major ethical concerns for multinational companies as evidenced by the prevalence of unfair, biased, and discriminate AI systems. Examples including Amazon's recruiting tool, Facebook's biased ads, and racially biased healthcare risk algorithms have raised fundamental questions about what these systems should be used for, the inherent risks they possess, and how they can be mitigated. Unfortunately, these failures not only serve to highlight the lack of regulation in AI development, but it also reveals how organisations are struggling to alleviate the dangers associated with this technology. We argue that to successfully implement ethical AI applications, developers need a deeper understanding of not only the implications of misuse, but also a grounded approach in their conception. Judgement studies were therefore conducted with experts from data science backgrounds who identified six performance areas, resulting in a theoretical framework for the development of ethically aligned AI systems. This framework also reveals that these performance areas require specific mechanisms which must be acted upon to ensure that an AI system implements and meets ethical requirements throughout its lifecycle. The findings also outline several constraints which present challenges in the manifestation of these elements. By implementing this framework, organisations can contribute to an elevated trust between technology and people resulting in significant implications for both IS research and practice. This framework will further allow organisations to take a positive and proactive approach in ensuring they are best prepared for the ethical implications associated with the development, deployment and use of AI systems.",
		"archive_location": "WOS:000838033200024",
		"DOI": "10.34190/EAIR.21.005",
		"event-title": "PROCEEDINGS OF THE 3RD EUROPEAN CONFERENCE ON THE IMPACT OF ARTIFICIAL INTELLIGENCE AND ROBOTICS (ECIAIR 2021)",
		"ISBN": "978-1-914587-23-8",
		"page": "183-191",
		"title": "Mechanisms and Constraints Underpinning Ethically Aligned Artificial Intelligence Systems: An Exploration of key Performance Areas",
		"author": [
			{
				"family": "Treacy",
				"given": "S"
			}
		],
		"editor": [
			{
				"family": "Matos",
				"given": "F"
			},
			{
				"family": "Salavisa",
				"given": "I"
			},
			{
				"family": "Serrao",
				"given": "C"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "lvDeepSafeReinforcement2021",
		"type": "paper-conference",
		"abstract": "Reinforcement learning (RL) is used more and more in robot navigation, however the safety of RL is usually not guaranteed. To improve the safety in the end-to-end mapless navigation using deep reinforcement learning (DRL), we propose a deep safe RL approach which uses a safe RL algorithm called Constrained Policy Optimization (CPO) and design the Actor-Critic-Safety (ACS) architecture to apply CPO. We use the Social Force Pedestrian Simulator based on social force model to simulate the dynamic environment with pedestrians in Gazebo. Experiment results show that the proposed approach can obviously increase the success rate and reduce the collision rate, which means the safety in navigation is improved. The planned path is almost as good as by ROS move base which needs to build a map of environment first. What's more, the model trained in static environment is able to generalize to unseen dynamic environment with pedestrians without any fine tuning and behaves well.",
		"archive_location": "WOS:000812286900248",
		"DOI": "10.1109/ROBIO54168.2021.9739251",
		"event-title": "2021 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (IEEE-ROBIO 2021)",
		"ISBN": "978-1-66540-535-5",
		"page": "1520-1525",
		"title": "A Deep Safe Reinforcement Learning Approach for Mapless Navigation",
		"author": [
			{
				"family": "Lv",
				"given": "SH"
			},
			{
				"family": "Li",
				"given": "YJ"
			},
			{
				"family": "Liu",
				"given": "Q"
			},
			{
				"family": "Gao",
				"given": "JQ"
			},
			{
				"family": "Pang",
				"given": "XZ"
			},
			{
				"family": "Chen",
				"given": "ML"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "beckerAuditingTestingAI2022",
		"type": "paper-conference",
		"abstract": "This paper describes a framework that can be used to assess and analyze AI systems in terms of risk. The framework addresses the structure and components of AI systems at five layers and allows taking a holistic view of AI systems while focusing on specific aspects, such as discrimination or data.",
		"archive_location": "WOS:000870272800020",
		"DOI": "10.1007/978-3-031-06018-2_20",
		"event-title": "DIGITAL HUMAN MODELING AND APPLICATIONS IN HEALTH, SAFETY, ERGONOMICS AND RISK MANAGEMENT: HEALTH, OPERATIONS MANAGEMENT, AND DESIGN, PT II",
		"ISBN": "0302-9743",
		"page": "283-292",
		"title": "Auditing and Testing AI - A Holistic Framework",
		"volume": "13320",
		"author": [
			{
				"family": "Becker",
				"given": "N"
			},
			{
				"family": "Waltl",
				"given": "B"
			}
		],
		"editor": [
			{
				"family": "Duffy",
				"given": "VG"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cowen-riversSAMBASafeModelbased2022",
		"type": "article-journal",
		"abstract": "In this paper, we propose SAMBA, a novel framework for safe reinforcement learning that combines aspects from probabilistic modelling, information theory, and statistics. Our method builds upon PILCO to enable active exploration using novel acquisition functions for out-of-sample Gaussian process evaluation optimised through a multi-objective problem that supports conditional-value-at-risk constraints. We evaluate our algorithm on a variety of safe dynamical system benchmarks involving both low and high-dimensional state representations. Our results show orders of magnitude reductions in samples and violations compared to state-of-the-art methods. Lastly, we provide intuition as to the effectiveness of the framework by a detailed analysis of our acquisition functions and safety constraints.",
		"archive_location": "WOS:000738430100001",
		"container-title": "MACHINE LEARNING",
		"DOI": "10.1007/s10994-021-06103-6",
		"ISSN": "0885-6125",
		"issue": "1",
		"page": "173-203",
		"title": "SAMBA: safe model-based & active reinforcement learning",
		"volume": "111",
		"author": [
			{
				"family": "Cowen-Rivers",
				"given": "AI"
			},
			{
				"family": "Palenicek",
				"given": "D"
			},
			{
				"family": "Moens",
				"given": "V"
			},
			{
				"family": "Abdullah",
				"given": "MA"
			},
			{
				"family": "Sootla",
				"given": "A"
			},
			{
				"family": "Wang",
				"given": "J"
			},
			{
				"family": "Bou-Ammar",
				"given": "H"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					1
				]
			]
		}
	},
	{
		"id": "dobbeHardChoicesArtificial2021",
		"type": "article-journal",
		"abstract": "As AI systems are integrated into high stakes social domains, researchers now examine how to design and operate them in a safe and ethical manner. However, the criteria for identifying and diagnosing safety risks in complex social contexts remain unclear and contested. In this paper, we examine the vagueness in debates about the safety and ethical behavior of AI systems. We show how this vagueness cannot be resolved through mathematical formalism alone, instead requiring deliberation about the politics of development as well as the context of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness in terms of distinct design challenges at key stages in AI system development. The resulting framework of Hard Choices in Artificial Intelligence (HCAI) empowers developers by 1) identifying points of overlap between design decisions and major sociotechnical challenges; 2) motivating the creation of stakeholder feedback channels so that safety issues can be exhaustively addressed. As such, HCAI contributes to a timely debate about the status of AI development in democratic societies, arguing that deliberation should be the goal of AI Safety, not just the procedure by which it is ensured. (C) 2021 The Authors. Published by Elsevier B.V.",
		"archive_location": "WOS:000697026000010",
		"container-title": "ARTIFICIAL INTELLIGENCE",
		"DOI": "10.1016/j.artint.2021.103555",
		"ISSN": "0004-3702",
		"title": "Hard choices in artificial intelligence",
		"volume": "300",
		"author": [
			{
				"family": "Dobbe",
				"given": "R"
			},
			{
				"family": "Gilbert",
				"given": "TK"
			},
			{
				"family": "Mintz",
				"given": "Y"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					11
				]
			]
		}
	},
	{
		"id": "okawaAutomaticExplorationProcess2020",
		"type": "paper-conference",
		"abstract": "In reinforcement learning (RL) algorithms, exploratory control inputs are used during learning to acquire knowledge for decision making and control, while the true dynamics of a controlled object is unknown. However, this exploring property sometimes causes undesired situations by violating constraints regarding the state of the controlled object. In this paper, we propose an automatic exploration process adjustment method for safe RL in continuous state and action spaces utilizing a linear nominal model of the controlled object. Specifically, our proposed method automatically selects whether the exploratory input is used or not at each time depending on the state and its predicted value as well as adjusts the variance-covariance matrix used in the Gaussian policy for exploration. We also show that our exploration process adjustment method theoretically guarantees the satisfaction of the constraints with the pre-specified probability, that is, the satisfaction of a joint chance constraint at every time. Finally, we illustrate the validity and the effectiveness of our method through numerical simulation. Copyright (C) 2020 The Authors.",
		"archive_location": "WOS:000652592500257",
		"DOI": "10.1016/j.ifacol.2020.12.2198",
		"event-title": "IFAC PAPERSONLINE",
		"ISBN": "2405-8963",
		"note": "issue: 2",
		"page": "1588-1595",
		"title": "Automatic Exploration Process Adjustment for Safe Reinforcement Learning with Joint Chance Constraint Satisfaction",
		"volume": "53",
		"author": [
			{
				"family": "Okawa",
				"given": "Y"
			},
			{
				"family": "Sasaki",
				"given": "T"
			},
			{
				"family": "Iwane",
				"given": "H"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "poenaru-olaruRetrainAISystems2023",
		"type": "paper-conference",
		"abstract": "Deployed machine learning systems often suffer from accuracy degradation over time generated by constant data shifts, also known as concept drift. Therefore, these systems require regular maintenance, in which the machine learning model needs to be adapted to concept drift. The literature presents plenty of model adaptation techniques. The most common technique is periodically executing the whole training pipeline with all the data gathered until a particular point in time, yielding a massive energy footprint. In this paper, we propose a research path that uses concept drift detection and adaptation to enable sustainable AI systems.",
		"archive_location": "WOS:001041741400003",
		"DOI": "10.1109/GREENS59328.2023.00009",
		"event-title": "2023 IEEE/ACM 7TH INTERNATIONAL WORKSHOP ON GREEN AND SUSTAINABLE SOFTWARE, GREENS",
		"ISBN": "979-8-3503-1238-6",
		"page": "17-18",
		"title": "Retrain AI Systems Responsibly! Use Sustainable Concept Drift Adaptation Techniques",
		"author": [
			{
				"family": "Poenaru-Olaru",
				"given": "L"
			},
			{
				"family": "Sallou",
				"given": "J"
			},
			{
				"family": "Cruz",
				"given": "L"
			},
			{
				"family": "Rellermeyer",
				"given": "JS"
			},
			{
				"family": "Deursen",
				"given": "A",
				"non-dropping-particle": "van"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "pfrommerReduceHandicapPerformance2023",
		"type": "paper-conference",
		"abstract": "The safety validation of AI and ML-based systems is challenging, as (i) analytical validation needs to include the interaction with a complex and stochastic physical environment and (ii) empirical validation needs to observe very long time-horizons to get enough \"statistical signal\" for the typically very low safety-related incident rate. This paper proposes an approach that amplifies the empirical evidence by introducing a handicap that reduces the system performance-making safety-related failures empirically more visible in a controlled environment-and gradually removing the handicap so that the convergence to the final incident rate can be estimated. Two numerical case studies are used to support and exemplify the approach.",
		"archive_location": "WOS:001066089800056",
		"DOI": "10.1109/INDIN51400.2023.10218017",
		"event-title": "2023 IEEE 21ST INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS, INDIN",
		"ISBN": "1935-4576",
		"title": "Reduce the Handicap: Performance Estimation for AI Systems Safety Certification",
		"author": [
			{
				"family": "Pfrommer",
				"given": "J"
			},
			{
				"family": "Poyer",
				"given": "M"
			},
			{
				"family": "Kiroriwal",
				"given": "S"
			}
		],
		"editor": [
			{
				"family": "Dorksen",
				"given": "H"
			},
			{
				"family": "Scanzio",
				"given": "S"
			},
			{
				"family": "Jasperneite",
				"given": "J"
			},
			{
				"family": "Wisniewski",
				"given": "L"
			},
			{
				"family": "Man",
				"given": "KF"
			},
			{
				"family": "Sauter",
				"given": "T"
			},
			{
				"family": "Seno",
				"given": "L"
			},
			{
				"family": "Trsek",
				"given": "H"
			},
			{
				"family": "Vyatkin",
				"given": "V"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "abellaSAFEXPLAINSafeExplainable2023",
		"type": "paper-conference",
		"abstract": "Deep Learning (DL) techniques are at the heart of most future advanced software functions in Critical Autonomous AI-based Systems (CAIS), where they also represent a major competitive factor. Hence, the economic success of CAIS industries (e.g., automotive, space, railway) depends on their ability to design, implement, qualify, and certify DL-based software products under bounded effort/cost. However, there is a fundamental gap between Functional Safety (FUSA) requirements on CAIS and the nature of DL solutions. This gap stems from the development process of DL libraries and affects high-level safety concepts such as (1) explainability and traceability, (2) suitability for varying safety requirements, (3) FUSA-compliant implementations, and (4) real-time constraints. As a matter of fact, the data-dependent and stochastic nature of DL algorithms clashes with current FUSA practice, which instead builds on deterministic, verifiable, and pass/fail test-based software. The SAFEXPLAIN project tackles these challenges and targets by providing a flexible approach to allow the certification - hence adoption - of DL-based solutions in CAIS building on: (1) DL solutions that provide end-to-end traceability, with specific approaches to explain whether predictions can be trusted and strategies to reach (and prove) correct operation, in accordance to certification standards; (2) alternative and increasingly sophisticated design safety patterns for DL with varying criticality and fault tolerance requirements; (3) DL library implementations that adhere to safety requirements; and (4) computing platform configurations, to regain determinism, and probabilistic timing analyses, to handle the remaining non-determinism.",
		"archive_location": "WOS:001027444200173",
		"event-title": "2023 DESIGN, AUTOMATION & TEST IN EUROPE CONFERENCE & EXHIBITION, DATE",
		"ISBN": "1530-1591",
		"title": "SAFEXPLAIN: Safe and Explainable Critical Embedded Systems Based on AI",
		"DOI": "10.23919/DATE56975.2023.10137128",
		"author": [
			{
				"family": "Abella",
				"given": "J"
			},
			{
				"family": "Perez",
				"given": "J"
			},
			{
				"family": "Englund",
				"given": "C"
			},
			{
				"family": "Zonooz",
				"given": "B"
			},
			{
				"family": "Giordana",
				"given": "G"
			},
			{
				"family": "Donzella",
				"given": "C"
			},
			{
				"family": "Cazorla",
				"given": "FJ"
			},
			{
				"family": "Mezzetti",
				"given": "E"
			},
			{
				"family": "Serra",
				"given": "I"
			},
			{
				"family": "Brando",
				"given": "A"
			},
			{
				"family": "Agirre",
				"given": "I"
			},
			{
				"family": "Eizaguirre",
				"given": "F"
			},
			{
				"family": "Bui",
				"given": "TH"
			},
			{
				"family": "Arani",
				"given": "E"
			},
			{
				"family": "Sarfraz",
				"given": "F"
			},
			{
				"family": "Balasubramaniam",
				"given": "A"
			},
			{
				"family": "Badar",
				"given": "A"
			},
			{
				"family": "Bloise",
				"given": "I"
			},
			{
				"family": "Feruglio",
				"given": "L"
			},
			{
				"family": "Cinelli",
				"given": "I"
			},
			{
				"family": "Brighenti",
				"given": "D"
			},
			{
				"family": "Cunial",
				"given": "D"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "dzambicArchitecturalPatternsIntegrating2021",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence (AI) is widely acknowledged as one of the most disruptive technologies driving the digital transformation of industries, enterprises, and societies in the 21st century. Advances in computing speed, algorithmic improvements, and access to a vast amount of data contributed to the adaption of AI in many different domains. Due to the outstanding performance, AI technology is increasingly integrated into safety-critical applications. However, the established safety engineering processes and practices have been only successfully applied in conventional model-based system development and no commonly agreed approaches for integrating AI technology are available yet. This work presents two architectural patterns that can support designers and engineers in the conception of safety-critical AI-enhanced cyber-physical system (CPS) applications. The first pattern addresses the problem of integrating AI capabilities into safety-critical functions. The second pattern deals with architectural approaches to integrate AI technologies for monitoring and learning system-specific behavior at runtime.",
		"archive_location": "WOS:000931946300036",
		"DOI": "10.1145/3489449.3490014",
		"event-title": "PROCEEDINGS OF THE EUROPEAN CONFERENCE ON PATTERN LANGUAGES OF PROGRAMS 2021, EUROPLOP 2021",
		"ISBN": "978-1-4503-8997-6",
		"title": "Architectural Patterns for Integrating AI Technology into Safety-Critical Systems",
		"author": [
			{
				"family": "Dzambic",
				"given": "M"
			},
			{
				"family": "Dobaj",
				"given": "J"
			},
			{
				"family": "Seidl",
				"given": "M"
			},
			{
				"family": "Macher",
				"given": "G"
			},
			{
				"literal": "ACM"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "grossArchitecturalPatternsHandling2022",
		"type": "paper-conference",
		"abstract": "Data-driven models (DDM) based on machine learning and other AI techniques play an important role in the perception of increasingly autonomous systems. Due to the merely implicit definition of their behavior mainly based on the data used for training, DDM outputs are subject to uncertainty. This poses a challenge with respect to the realization of safety-critical perception tasks by means of DDMs. A promising approach to tackling this challenge is to estimate the uncertainty in the current situation during operation and adapt the system behavior accordingly. In previous work, we focused on runtime estimation of uncertainty and discussed approaches for handling uncertainty estimations. In this paper, we present additional architectural patterns for handling uncertainty. Furthermore, we evaluate the four patterns qualitatively and quantitatively with respect to safety and performance gains. For the quantitative evaluation, we consider a distance controller for vehicle platooning where performance gains are measured by considering how much the distance can be reduced in different operational situations. We conclude that the consideration of context information concerning the driving situation makes it possible to accept more or less uncertainty depending on the inherent risk of the situation, which results in performance gains.",
		"archive_location": "WOS:000871734000019",
		"DOI": "10.1007/978-3-031-14835-4_19",
		"event-title": "COMPUTER SAFETY, RELIABILITY, AND SECURITY, SAFECOMP 2022",
		"ISBN": "0302-9743",
		"page": "284-297",
		"title": "Architectural Patterns for Handling Runtime Uncertainty of Data-Driven Models in Safety-Critical Perception",
		"volume": "13414",
		"author": [
			{
				"family": "Gross",
				"given": "J"
			},
			{
				"family": "Adler",
				"given": "R"
			},
			{
				"family": "Kläs",
				"given": "M"
			},
			{
				"family": "Reich",
				"given": "J"
			},
			{
				"family": "Jöckel",
				"given": "L"
			},
			{
				"family": "Gansch",
				"given": "R"
			}
		],
		"editor": [
			{
				"family": "Trapp",
				"given": "M"
			},
			{
				"family": "Saglietti",
				"given": "F"
			},
			{
				"family": "Spislander",
				"given": "M"
			},
			{
				"family": "Bitsch",
				"given": "F"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "badeaHaveBreakMaking2022",
		"type": "paper-conference",
		"abstract": "The Multi-valued Action Reasoning System (MARS) is an automated value-based ethical decision-making model for agents in Artificial Intelligence (AI). Given a set of available actions and an underlying moral paradigm, by employing MARS one can identify the ethically preferred action. It can be used to implement and model different ethical theories, different moral paradigms, as well as combinations of such, in the context of automated practical reasoning and normative decision analysis. It can also be used to model moral dilemmas and discover the moral paradigms that result in the desired outcomes therein. In this paper we give a condensed description of MARS, explain its uses, and comparatively place it in the existing literature.",
		"archive_location": "WOS:000922637500031",
		"DOI": "10.1007/978-3-031-21441-7_31",
		"event-title": "ARTIFICIAL INTELLIGENCE XXXIX, AI 2022",
		"ISBN": "0302-9743",
		"page": "359-366",
		"title": "Have a Break from Making Decisions, Have a MARS: The Multi-valued Action Reasoning System",
		"volume": "13652",
		"author": [
			{
				"family": "Badea",
				"given": "C"
			}
		],
		"editor": [
			{
				"family": "Bramer",
				"given": "M"
			},
			{
				"family": "Stahl",
				"given": "F"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhuoDeepUnsupervisedConvolutional2017",
		"type": "paper-conference",
		"abstract": "In multimedia analysis, the task of domain adaptation is to adapt the feature representation learned in the source domain with rich label information to the target domain with less or even no label information. Significant research endeavors have been devoted to aligning the feature distributions between the source and the target domains in the top fully connected layers based on unsupervised DNN-based models. However, the domain adaptation has been arbitrarily constrained near the output ends of the DNN models, which thus brings about inadequate knowledge transfer in DNN-based domain adaptation process, especially near the input end. We develop an attention transfer process for convolutional domain adaptation. The domain discrepancy, measured in correlation alignment loss, is minimized on the second order correlation statistics of the attention maps for both source and target domains. Then we propose Deep Unsupervised Convolutional Domain Adaptation (DUCDA) method, which jointly minimizes the supervised classification loss of labeled source data and the unsupervised correlation alignment loss measured on both convolutional layers and fully connected layers. The multi-layer domain adaptation process collaborately reinforces each individual domain adaptation component, and significantly enhances the generalization ability of the CNN models. Extensive cross-domain object classification experiments show DUCDA outperforms other state-of-the-art approaches, and validate the promising power of DUCDA towards large scale real world application.",
		"archive_location": "WOS:000482109500031",
		"DOI": "10.1145/3123266.3123292",
		"event-title": "PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17)",
		"ISBN": "978-1-4503-4906-2",
		"page": "261-269",
		"title": "Deep Unsupervised Convolutional Domain Adaptation",
		"author": [
			{
				"family": "Zhuo",
				"given": "JB"
			},
			{
				"family": "Wang",
				"given": "SH"
			},
			{
				"family": "Zhang",
				"given": "WG"
			},
			{
				"family": "Huang",
				"given": "QM"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "jinStabilityCertifiedReinforcementLearning2020",
		"type": "article-journal",
		"abstract": "We investigate the important problem of certifying stability of reinforcement learning policies when interconnected with nonlinear dynamical systems. We show that by regulating the partial gradients of policies, strong guarantees of robust stability can be obtained based on a proposed semidefinite programming feasibility problem. The method is able to certify a large set of stabilizing controllers by exploiting problem-specific structures; furthermore, we analyze and establish its (non)conservatism. Empirical evaluations on two decentralized control tasks, namely multi-flight formation and power system frequency regulation, demonstrate that the reinforcement learning agents can have high performance within the stability-certified parameter space and also exhibit stable learning behaviors in the long run.",
		"archive_location": "WOS:000616290100001",
		"container-title": "IEEE ACCESS",
		"DOI": "10.1109/ACCESS.2020.3045114",
		"ISSN": "2169-3536",
		"page": "229086-229100",
		"title": "Stability-Certified Reinforcement Learning: A Control-Theoretic Perspective",
		"volume": "8",
		"author": [
			{
				"family": "Jin",
				"given": "M"
			},
			{
				"family": "Lavaei",
				"given": "J"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wabersichProbabilisticModelPredictive2022",
		"type": "article-journal",
		"abstract": "Reinforcement learning (RL) methods have demonstrated their efficiency in simulation. However, many of the applications for which RL offers great potential, such as autonomous driving, are also safety critical and require a certified closed-loop behavior in order to meet the safety specifications in the presence of physical constraints. This article introduces a concept called probabilistic model predictive safety certification (PMPSC), which can be combined with any RL algorithm and provides provable safety certificates in terms of state and input chance constraints for potentially large-scale systems. The certificate is realized through a stochastic tube that safely connects the current system state with a terminal set of states that is known to be safe. A novel formulation allows a recursively feasible real-time computation of such probabilistic tubes, despite the presence of possibly unbounded disturbances. A design procedure for PMPSC relying on Bayesian inference and recent advances in probabilistic set invariance is presented. Using a numerical car simulation, the method and its design procedure are illustrated by enhancing an RL algorithm with safety certificates.",
		"archive_location": "WOS:000735567400016",
		"container-title": "IEEE TRANSACTIONS ON AUTOMATIC CONTROL",
		"DOI": "10.1109/TAC.2021.3049335",
		"ISSN": "0018-9286",
		"issue": "1",
		"page": "176-188",
		"title": "Probabilistic Model Predictive Safety Certification for Learning-Based Control",
		"volume": "67",
		"author": [
			{
				"family": "Wabersich",
				"given": "KJ"
			},
			{
				"family": "Hewing",
				"given": "L"
			},
			{
				"family": "Carron",
				"given": "A"
			},
			{
				"family": "Zeilinger",
				"given": "MN"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					1
				]
			]
		}
	},
	{
		"id": "vakkuriECCOLAMethodImplementing2021",
		"type": "article-journal",
		"abstract": "Artificial Intelligence (AI) systems are becoming increasingly widespread and exert a growing influence on society at large. The growing impact of these systems has also highlighted potential issues that may arise from their utilization, such as data privacy issues, resulting in calls for ethical AI systems. Yet, how to develop ethical AI systems remains an important question in the area. How should the principles and values be converted into requirements for these systems, and what should developers and the organizations developing these systems do? To further bridge this gap in the area, in this paper, we present a method for implementing AI ethics: ECCOLA. Following a cyclical action research approach, ECCOLA has been iteratively developed over the course of multiple years, in collaboration with both researchers and practitioners. (C) 2021 The Author(s). Published by Elsevier Inc.",
		"archive_location": "WOS:000704056400002",
		"container-title": "JOURNAL OF SYSTEMS AND SOFTWARE",
		"DOI": "10.1016/j.jss.2021.111067",
		"ISSN": "0164-1212",
		"title": "ECCOLA - A method for implementing ethically aligned AI systems",
		"volume": "182",
		"author": [
			{
				"family": "Vakkuri",
				"given": "V"
			},
			{
				"family": "Kemell",
				"given": "KK"
			},
			{
				"family": "Jantunen",
				"given": "M"
			},
			{
				"family": "Halme",
				"given": "E"
			},
			{
				"family": "Abrahamsson",
				"given": "P"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					12
				]
			]
		}
	},
	{
		"id": "vaccariEXplainableReliableAdversarial2022",
		"type": "article-journal",
		"abstract": "Machine learning (ML) algorithms are nowadays widely adopted in different contexts to perform autonomous decisions and predictions. Due to the high volume of data shared in the recent years, ML algorithms are more accurate and reliable since training and testing phases are more precise. An important concept to analyze when defining ML algorithms concerns adversarial machine learning attacks. These attacks aim to create manipulated datasets to mislead ML algorithm decisions. In this work, we propose new approaches able to detect and mitigate malicious adversarial machine learning attacks against a ML system. In particular, we investigate the Carlini-Wagner (CW), the fast gradient sign method (FGSM) and the Jacobian based saliency map (JSMA) attacks. The aim of this work is to exploit detection algorithms as countermeasures to these attacks. Initially, we performed some tests by using canonical ML algorithms with a hyperparameters optimization to improve metrics. Then, we adopt original reliable AI algorithms, either based on eXplainable AI (Logic Learning Machine) or Support Vector Data Description (SVDD). The obtained results show how the classical algorithms may fail to identify an adversarial attack, while the reliable AI methodologies are more prone to correctly detect a possible adversarial machine learning attack. The evaluation of the proposed methodology was carried out in terms of good balance between FPR and FNR on real world application datasets: Domain Name System (DNS) tunneling, Vehicle Platooning and Remaining Useful Life (RUL). In addition, a statistical analysis was performed to improve the robustness of the trained models, including evaluating their performance in terms of runtime and memory consumption.",
		"archive_location": "WOS:000842742900001",
		"container-title": "IEEE ACCESS",
		"DOI": "10.1109/ACCESS.2022.3197299",
		"ISSN": "2169-3536",
		"page": "83949-83970",
		"title": "eXplainable and Reliable Against Adversarial Machine Learning in Data Analytics",
		"volume": "10",
		"author": [
			{
				"family": "Vaccari",
				"given": "I"
			},
			{
				"family": "Carlevaro",
				"given": "A"
			},
			{
				"family": "Narteni",
				"given": "S"
			},
			{
				"family": "Cambiaso",
				"given": "E"
			},
			{
				"family": "Mongelli",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mindomAssessingSafetyReinforcement2021",
		"type": "paper-conference",
		"abstract": "The increasing adoption of Reinforcement Learning in safety-critical systems domains such as autonomous vehicles, health, and aviation raises the need for ensuring their safety. Existing safety mechanisms such as adversarial training, adversarial detection, and robust learning are not always adapted to all disturbances in which the agent is deployed. Those disturbances include moving adversaries whose behavior can be unpredictable by the agent, and as a matter of fact harmful to its learning. Ensuring the safety of critical systems also requires methods that give formal guarantees on the behaviour of the agent evolving in a perturbed environment. It is therefore necessary to propose new solutions adapted to the learning challenges faced by the agent. In this paper, first we generate adversarial agents that exhibit flaws in the agent's policy by presenting moving adversaries. Secondly, We use reward shaping and a modified Q-learning algorithm as defense mechanisms to improve the agent's policy when facing adversarial perturbations. Finally, probabilistic model checking is employed to evaluate the effectiveness of both mechanisms. We have conducted experiments on a discrete grid world with a single agent facing non-learning and learning adversaries. Our results show a diminution in the number of collisions between the agent and the adversaries. Probabilistic model checking provides lower and upper probabilistic bounds regarding the agent's safety in the adversarial environment.",
		"archive_location": "WOS:000814747000027",
		"DOI": "10.1109/QRS54544.2021.00037",
		"event-title": "2021 IEEE 21ST INTERNATIONAL CONFERENCE ON SOFTWARE QUALITY, RELIABILITY AND SECURITY (QRS 2021)",
		"ISBN": "2693-9185",
		"page": "260-269",
		"title": "On Assessing The Safety of Reinforcement Learning algorithms Using Formal Methods",
		"author": [
			{
				"family": "Mindom",
				"given": "PSN"
			},
			{
				"family": "Nikanjam",
				"given": "A"
			},
			{
				"family": "Khomh",
				"given": "F"
			},
			{
				"family": "Mullins",
				"given": "J"
			},
			{
				"literal": "IEEE COMP SOC"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "umbrelloBeneficialArtificialIntelligence2019",
		"type": "article-journal",
		"abstract": "This paper argues that the Value Sensitive Design (VSD) methodology provides a principled approach to embedding common values into AI systems both early and throughout the design process. To do so, it draws on an important case study: the evidence and final report of the UK Select Committee on Artificial Intelligence. This empirical investigation shows that the different and often disparate stakeholder groups that are implicated in AI design and use share some common values that can be used to further strengthen design coordination efforts. VSD is shown to be both able to distill these common values as well as provide a framework for stakeholder coordination.",
		"archive_location": "WOS:000697668400005",
		"container-title": "BIG DATA AND COGNITIVE COMPUTING",
		"DOI": "10.3390/bdcc3010005",
		"ISSN": "2504-2289",
		"issue": "1",
		"title": "Beneficial Artificial Intelligence Coordination by Means of a Value Sensitive Design Approach",
		"volume": "3",
		"author": [
			{
				"family": "Umbrello",
				"given": "S"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019",
					3
				]
			]
		}
	},
	{
		"id": "fernandezFunctionalSafetyCompliance2021",
		"type": "article-journal",
		"abstract": "Autonomous systems execute complex tasks to perceive the environment and take self-aware decisions with limited human interaction. This autonomy is commonly achieved with the support of machine learning algorithms. The nature of these algorithms, that need to process large data volumes, poses high-performance demands on the underlying hardware. As a result, the embedded critical real-time domain is adopting increasingly powerful processors that combine multi-core processors with accelerators such as GPUs. The resulting hardware and software complexity makes it difficult to demonstrate that the system will run safely and reliably. This is the main objective of functional safety standards, such as IEC 61508 or ISO 26262, that deal with the avoidance, detection and control of hardware or software errors. In this paper, we adopt those measures for the safe inference of machine learning libraries on multi-core devices, two topics that are not explicitly covered in the current version of standards. To this end, we adapt the matrix-matrix multiplication function, a central element of existing machine learning libraries, according to the recommendations of functional safety standards. The paper makes the following contributions: (i) adoption of recommended programming practices for the avoidance of programming errors in the matrix-matrix multiplication, (ii) inclusion of diagnostic mechanisms based on widely used checksums to control runtime errors, and (iii) evaluation of the impact of previous measures in terms of performance and a quantification of the achieved diagnostic coverage. For this purpose, we implement the diagnostic mechanisms on one of the ARM R5 cores of a Zynq UltraScale+ multi-processor system-on-chip and we then adapt them to an Intel i7 processor with native code employing vectorization for the sake of performance.",
		"archive_location": "WOS:000712050800002",
		"container-title": "JOURNAL OF SYSTEMS ARCHITECTURE",
		"DOI": "10.1016/j.sysarc.2021.102298",
		"ISSN": "1383-7621",
		"title": "Towards functional safety compliance of matrix-matrix multiplication for machine learning-based autonomous systems",
		"volume": "121",
		"author": [
			{
				"family": "Fernández",
				"given": "J"
			},
			{
				"family": "Perez",
				"given": "J"
			},
			{
				"family": "Agirre",
				"given": "I"
			},
			{
				"family": "Allende",
				"given": "I"
			},
			{
				"family": "Abella",
				"given": "J"
			},
			{
				"family": "Cazorla",
				"given": "FJ"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					12
				]
			]
		}
	},
	{
		"id": "massianiSafeValueFunctions2023",
		"type": "article-journal",
		"abstract": "Safety constraints and optimality are important but sometimes conflicting criteria for controllers. Although these criteria are often solved separately with different tools to maintain formal guarantees, it is also common practice in reinforcement learning (RL) to simply modify reward functions by penalizing failures, with the penalty treated as a mere heuristic. We rigorously examine the relationship of both safety and optimality to penalties, and formalize sufficient conditions for safe value functions (SVFs): value functions that are both optimal for a given task, and enforce safety constraints. We reveal this structure by examining when rewards preserve viability under optimal control, and show that there always exists a finite penalty that induces an SVF. This penalty is not unique, but upper-unbounded: larger penalties do not harm optimality. Although it is often not possible to compute the minimum required penalty, we reveal clear structure of how the penalty, rewards, discount factor, and dynamics interact. This insight suggests practical, theory-guided heuristics to design reward functions for control problems where safety is important.",
		"archive_location": "WOS:000979661300009",
		"container-title": "IEEE TRANSACTIONS ON AUTOMATIC CONTROL",
		"DOI": "10.1109/TAC.2022.3200948",
		"ISSN": "0018-9286",
		"issue": "5",
		"page": "2743-2757",
		"title": "Safe Value Functions",
		"volume": "68",
		"author": [
			{
				"family": "Massiani",
				"given": "PF"
			},
			{
				"family": "Heim",
				"given": "S"
			},
			{
				"family": "Solowjow",
				"given": "F"
			},
			{
				"family": "Trimpe",
				"given": "S"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					5
				]
			]
		}
	},
	{
		"id": "maConservativeAdaptivePenalty2022",
		"type": "paper-conference",
		"abstract": "Reinforcement Learning (RL) agents in the real world must satisfy safety constraints in addition to maximizing a reward objective. Model-based RL algorithms hold promise for reducing unsafe real-world actions: they may synthesize policies that obey all constraints using simulated samples from a learned model. However, imperfect models can result in real-world constraint violations even for actions that are predicted to satisfy all constraints. We propose Conservative and Adaptive Penalty (CAP), a model-based safe RL framework that accounts for potential modeling errors by capturing model uncertainty and adaptively exploiting it to balance the reward and the cost objectives. First, CAP inflates predicted costs using an uncertainty-based penalty. Theoretically, we show that policies that satisfy this conservative cost constraint are guaranteed to also be feasible in the true environment. We further show that this guarantees the safety of all intermediate solutions during RL training. Further, CAP adaptively tunes this penalty during training using true cost feedback from the environment. We evaluate this conservative and adaptive penalty-based approach for model-based safe RL extensively on state and image-based environments. Our results demonstrate substantial gains in sample-efficiency while incurring fewer violations than prior safe RL algorithms.",
		"archive_location": "WOS:000893636205058",
		"event-title": "THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE",
		"ISBN": "2159-5399",
		"page": "5404-5412",
		"title": "Conservative and Adaptive Penalty for Model-Based Safe Reinforcement Learning",
		"URL": "https://arxiv.org/abs/2112.07701",
		"author": [
			{
				"family": "Ma",
				"given": "YJ"
			},
			{
				"family": "Shen",
				"given": "A"
			},
			{
				"family": "Bastani",
				"given": "O"
			},
			{
				"family": "Jayaraman",
				"given": "D"
			},
			{
				"literal": "Assoc Advancement Artificial Intelligence"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bazzanAligningIndividualCollective2019",
		"type": "article-journal",
		"abstract": "In complex socio-technical systems it is not easy to find a balance between the welfare state (i.e., a state where the overall performance of a system is optimal) and a situation in which individual components act selfishly to optimize their own utilities. This is even harder when individuals compete for scarce resources. In order to deal with this, some forms of biasing the optimization process have been proposed. However, mostly, such approaches only work for cooperative scenarios. When resources are scarce, the components of the system compete for them, thus approaches designed for cooperative systems are not necessarily appropriate. In the present paper an approach is proposed, which is based on a synergy between: (i) a global optimization process in which the system authority employs metaheuristics, and (ii) reinforcement learning processes that run at each component or agent. Both the agents and the system authority exchange solutions that are incorporated by the other party. The contributions of the proposed approach are twofold: a general scheme for such synergy is given and its benefits are shown in scenarios related to selfish routing, a typical load balance problem in a complex socio-technical system.",
		"archive_location": "WOS:000459524300003",
		"container-title": "ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE",
		"DOI": "10.1016/j.engappai.2018.12.003",
		"ISSN": "0952-1976",
		"page": "23-33",
		"title": "Aligning individual and collective welfare in complex socio-technical systems by combining metaheuristics and reinforcement learning",
		"volume": "79",
		"author": [
			{
				"family": "Bazzan",
				"given": "ALC"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019",
					3
				]
			]
		}
	},
	{
		"id": "douthwaiteEstablishingVerificationValidation2017",
		"type": "paper-conference",
		"abstract": "The assurance of autonomous systems and the technologies that drive them is a major research challenge in the safety-critical systems engineering domain. The nature of many of these Machine Learning (ML) and Artificial Intelligence (AI) approaches raises a number of additional, technology-specific assurance concerns. One such approach is the Bayesian Network (BN) probabilistic modelling framework. Bayesian Networks and the family of modelling techniques they belong to form the basis of many AI applications. However, little research has been conducted into the assurance of BN-based systems for use in safety-critical applications. This paper explores some of the key distinctions between BN-based software-intensive systems and conventional software systems. It introduces a modelling framework that explicitly captures BN-based system-specific considerations and facilitates both the communication of assurance concerns between safety practitioners and system stakeholders, and the subsequent safety analysis of the system itself. It demonstrates how this approach can be used to develop specific verification and validation objectives for a BN-based system in a medical application.",
		"archive_location": "WOS:000418465000065",
		"DOI": "10.1109/ISSREW.2017.60",
		"event-title": "2017 IEEE 28TH INTERNATIONAL SYMPOSIUM ON SOFTWARE RELIABILITY ENGINEERING WORKSHOPS (ISSREW 2017)",
		"ISBN": "2375-821X",
		"page": "302-309",
		"title": "Establishing Verification and Validation Objectives for Safety-Critical Bayesian Networks",
		"author": [
			{
				"family": "Douthwaite",
				"given": "M"
			},
			{
				"family": "Kelly",
				"given": "T"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "tommasinoReinforcementLearningArchitecture2019",
		"type": "article-journal",
		"abstract": "When humans learn several skills to solve multiple tasks, they exhibit an extraordinary capacity to transfer knowledge between them. We present here the last enhanced version of a bio-inspired reinforcement-learning (RL) modular architecture able to perform skill-to-skill knowledge transfer and called transfer expert RL (TERL) model. TERL architecture is based on a RL actor-critic model where both actor and critic have a hierarchical structure, inspired by the mixture-of-experts model, formed by a gating network that selects experts specializing in learning the policies or value functions of different tasks. A key feature of TERL is the capacity of its gating networks to accumulate, in parallel, evidence on the capacity of experts to solve the new tasks so as to increase the responsibility for action of the best ones. A second key feature is the use of two different responsibility signals for the experts' functioning and learning: this allows the training of multiple experts for each task so that some of them can be later recruited to solve new tasks and avoid catastrophic interference. The utility of TERL mechanisms is shown with tests involving two simulated dynamic robot arms engaged in solving reaching tasks, in particular a planar 2-DoF arm, and a 3-D 4-DoF arm.",
		"archive_location": "WOS:000471119200014",
		"container-title": "IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS",
		"DOI": "10.1109/TCDS.2016.2607018",
		"ISSN": "2379-8920",
		"issue": "2",
		"page": "292-317",
		"title": "A Reinforcement Learning Architecture That Transfers Knowledge Between Skills When Solving Multiple Tasks",
		"volume": "11",
		"author": [
			{
				"family": "Tommasino",
				"given": "P"
			},
			{
				"family": "Caligiore",
				"given": "D"
			},
			{
				"family": "Mirolli",
				"given": "M"
			},
			{
				"family": "Baldassarre",
				"given": "G"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019",
					6
				]
			]
		}
	},
	{
		"id": "burtonAddressingUncertaintySafety2023",
		"type": "article-journal",
		"abstract": "There is increasing interest in the application of machine learning (ML) technologies to safety-critical cyber-physical systems, with the promise of increased levels of autonomy due to their potential for solving complex perception and planning tasks. However, demonstrating the safety of ML is seen as one of the most challenging hurdles to their widespread deployment for such applications. In this paper we explore the factors which make the safety assurance of ML such a challenging task. In particular we address the impact of uncertainty on the confidence in ML safety assurance arguments. We show how this uncertainty is related to complexity in the ML models as well as the inherent complexity of the tasks that they are designed to implement. Based on definitions of uncertainty as well as an exemplary assurance argument structure, we examine typical weaknesses in the argument and how these can be addressed. The analysis combines an understanding of causes of insufficiencies in ML models with a systematic analysis of the types of asserted context, asserted evidence and asserted inference within the assurance argument. This leads to a systematic identification of requirements on the assurance argument structure as well as supporting evidence. We conclude that a combination of qualitative arguments combined with quantitative evidence are required to build a robust argument for safety-related properties of ML functions that is continuously refined to reduce residual and emerging uncertainties in the arguments after the function has been deployed into the target environment.",
		"archive_location": "WOS:000971580900001",
		"container-title": "FRONTIERS IN COMPUTER SCIENCE",
		"DOI": "10.3389/fcomp.2023.1132580",
		"ISSN": "2624-9898",
		"title": "Addressing uncertainty in the safety assurance of machine-learning",
		"volume": "5",
		"author": [
			{
				"family": "Burton",
				"given": "S"
			},
			{
				"family": "Herd",
				"given": "B"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					6
				]
			]
		}
	},
	{
		"id": "carlucciMultiDIALDomainAlignment2021",
		"type": "article-journal",
		"abstract": "One of the main challenges for developing visual recognition systems working in the wild is to devise computational models immune from the domain shift problem, i.e., accurate when test data are drawn from a (slightly) different data distribution than training samples. In the last decade, several research efforts have been devoted to devise algorithmic solutions for this issue. Recent attempts to mitigate domain shift have resulted into deep learning models for domain adaptation which learn domain-invariant representations by introducing appropriate loss terms, by casting the problem within an adversarial learning framework or by embedding into deep network specific domain normalization layers. This paper describes a novel approach for unsupervised domain adaptation. Similarly to previous works we propose to align the learned representations by embedding them into appropriate network feature normalization layers. Opposite to previous works, our Domain Alignment Layers are designed not only to match the source and target feature distributions but also to automatically learn the degree of feature alignment required at different levels of the deep network. Differently from most previous deep domain adaptation methods, our approach is able to operate in a multi-source setting. Thorough experiments on four publicly available benchmarks confirm the effectiveness of our approach.",
		"archive_location": "WOS:000714203900021",
		"container-title": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE",
		"DOI": "10.1109/TPAMI.2020.3001338",
		"ISSN": "0162-8828",
		"issue": "12",
		"page": "4441-4452",
		"title": "MultiDIAL: Domain Alignment Layers for (Multisource) Unsupervised Domain Adaptation",
		"volume": "43",
		"author": [
			{
				"family": "Carlucci",
				"given": "FM"
			},
			{
				"family": "Porzi",
				"given": "L"
			},
			{
				"family": "Caputo",
				"given": "B"
			},
			{
				"family": "Ricci",
				"given": "E"
			},
			{
				"family": "Bulo",
				"given": "SR"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					1
				]
			]
		}
	},
	{
		"id": "otteInterpretableSemiparametricRegression2014",
		"type": "article-journal",
		"abstract": "Unreliable extrapolation of data-driven models hinders their applicability not only in safety-related domains. The paper discusses how model interpretability and uncertainty estimates can address this problem. A new semi-parametric approach is proposed for providing an interpretable model with improved accuracy by combining a symbolic regression model with a residual Gaussian Process. While the learned symbolic model is highly interpretable the residual model usually is not. However, by limiting the output of the residual model to a defined range a worst-case guarantee can be given in the sense that the maximal deviation from the symbolic model is always below a defined limit. The limitation of the residual model can include the uncertainty estimate of the Gaussian Process, thus giving the residual model more impact in high-confidence regions. When ranking the accuracy and interpretability of several different approaches on the SARCOS data benchmark the proposed combination yields the best result",
		"archive_location": "WOS:000340982800001",
		"container-title": "NEUROCOMPUTING",
		"DOI": "10.1016/j.neucom.2013.11.042",
		"ISSN": "0925-2312",
		"page": "1-6",
		"title": "Interpretable semi-parametric regression models with defined error bounds",
		"volume": "143",
		"author": [
			{
				"family": "Otte",
				"given": "C"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2014",
					11,
					2
				]
			]
		}
	},
	{
		"id": "boudiDeepReinforcementLearning2023",
		"type": "article-journal",
		"abstract": "Artificial Intelligence (AI) and data are reshaping organizations and businesses. Human Resources (HR) management and talent development make no exception, as they tend to involve more automation and growing quantities of data. Because this brings implications on workforce, career transparency, and equal opportunities, overseeing what fuels AI and analytical models, their quality standards, integrity, and correctness becomes an imperative for those aspiring to such systems. Based on an ontology transformation to B-machines, this article presents an approach to constructing a valid and error-free career agent with Deep Reinforcement Learning (DRL). In short, the agent's policy is built on a framework we called Multi State-Actor (MuStAc) using a decentralized training approach. Its purpose is to predict both relevant and valid career steps to employees, based on their profiles and company pathways (observations). Observations can comprise various data elements such as the current occupation, past experiences, performance, skills, qualifications, and so on. The policy takes in all these observations and outputs the next recommended career step, in an environment set as the combination of an HR ontology and an Event-B model, which generates action spaces with respect to formal properties. The Event-B model and formal properties are derived using OWL to B transformation.",
		"archive_location": "WOS:000950940200005",
		"container-title": "FORMAL ASPECTS OF COMPUTING",
		"DOI": "10.1145/3577204",
		"ISSN": "0934-5043",
		"issue": "1",
		"title": "A Deep Reinforcement Learning Framework with Formal Verification",
		"volume": "35",
		"author": [
			{
				"family": "Boudi",
				"given": "Z"
			},
			{
				"family": "Wakrime",
				"given": "AA"
			},
			{
				"family": "Toub",
				"given": "M"
			},
			{
				"family": "Haloua",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					3
				]
			]
		}
	},
	{
		"id": "zhangBarrierLyapunovFunctionBased2022",
		"type": "article-journal",
		"abstract": "Guaranteed safety and performance under various circumstances remain technically critical and practically challenging for the wide deployment of autonomous vehicles. Safety-critical systems in general, require safe performance even during the reinforcement learning (RL) period. To address this issue, a Barrier Lyapunov Function-based safe RL (BLF-SRL) algorithm is proposed here for the formulated nonlinear system in strict-feedback form. This approach appropriately arranges and incorporates the BLF items into the optimized backstepping control method to constrain the state-variables in the designed safety region during learning. Wherein, thus, the optimal virtual/actual control in every backstepping subsystem is decomposed with BLF items and also with an adaptive uncertain item to be learned, which achieves safe exploration during the learning process. Then, the principle of Bellman optimality of continuous-time Hamilton-Jacobi-Bellman equation in every backstepping subsystem is satisfied with independently approximated actor and critic under the framework of actor-critic through the designed iterative updating. Eventually, the overall system control is optimized with the proposed BLF-SRL method. It is furthermore noteworthy that the variance of the attained control performance under uncertainty is also reduced with the proposed method. The effectiveness of the proposed method is verified with two motion control problems for autonomous vehicles through appropriate comparison simulations.",
		"archive_location": "WOS:000826063500001",
		"container-title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS",
		"DOI": "10.1109/TNNLS.2022.3186528",
		"ISSN": "2162-237X",
		"title": "Barrier Lyapunov Function-Based Safe Reinforcement Learning for Autonomous Vehicles With Optimized Backstepping",
		"author": [
			{
				"family": "Zhang",
				"given": "YX"
			},
			{
				"family": "Liang",
				"given": "XL"
			},
			{
				"family": "Li",
				"given": "DY"
			},
			{
				"family": "Ge",
				"given": "SZS"
			},
			{
				"family": "Gao",
				"given": "BZ"
			},
			{
				"family": "Chen",
				"given": "H"
			},
			{
				"family": "Lee",
				"given": "TH"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					12
				]
			]
		}
	},
	{
		"id": "dacquistoConflictsEthicalLogical2020",
		"type": "article-journal",
		"abstract": "Artificial intelligence is nowadays a reality. Setting rules on the potential outcomes of intelligent machines, so that no surprise can be expected by humans from the behavior of those machines, is becoming a priority for policy makers. In its recent Communication \"Artificial Intelligence for Europe\" (EU Commission2018), for instance, the European Commission identifies the distinguishing trait of an intelligent machine in the presence of \"a certain degree of autonomy\" in decision making, in the light of the context. The crucial issue to be addressed is, therefore, whether it is possible to identify a set of rules for data use by intelligent machines so that the decision-making autonomy of machines can allow for humans' traditional informational self-determination (humans provide machines only with the data they decide to), as enshrined in many existing legal frameworks (including, for personal data protection, the EU's General Data Protection Regulation) (EU Parliament and Council2016) and can actually turn out to be further beneficial to individuals. Governing the autonomy of machines can be a very ambitious goal for humans since machines are geared first to the principles of formal logic and then-possibly-to ethical or legal principles. This introduces an unprecedented degree of complexity in how a norm should be engineered, which requires, in turn, an in-depth reflection in order to prevent conflicts between the legal and ethical principles underlying humans' civil coexistence and the rules of formal logic upon which the functioning of machines is based (EU Parliament2017).",
		"archive_location": "WOS:000548491100001",
		"container-title": "AI & SOCIETY",
		"DOI": "10.1007/s00146-019-00927-6",
		"ISSN": "0951-5666",
		"issue": "4",
		"page": "895-900",
		"title": "On conflicts between ethical and logical principles in artificial intelligence",
		"volume": "35",
		"author": [
			{
				"family": "D'Acquisto",
				"given": "G"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020",
					12
				]
			]
		}
	},
	{
		"id": "tayStudyRealtimeArtificial1998",
		"type": "paper-conference",
		"abstract": "This paper highlights some of the important considerations when applying artificial intelligence techniques in real-time applications. There is a specific focus on real-time expert systems. The issues of reliability and safety in real-time control systems are also addressed. Important considerations like scheduling, real-time operating systems are also highlighted in designing real-time systems. Copyright (C) 1998 IFAC.",
		"archive_location": "WOS:000077333600018",
		"event-title": "ARTIFICIAL INTELLIGENCE IN REAL-TIME CONTROL 1997",
		"ISBN": "0962-9505",
		"page": "109-114",
		"title": "A study on real-time artificial intelligence",
		"URL": "https://www.sciencedirect.com/science/article/pii/S1474667017413097",
		"author": [
			{
				"family": "Tay",
				"given": "EB"
			},
			{
				"family": "Gan",
				"given": "OP"
			},
			{
				"family": "Ho",
				"given": "WK"
			}
		],
		"editor": [
			{
				"family": "Rauch",
				"given": "HE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"1998"
				]
			]
		}
	},
	{
		"id": "kocakSafePredictMetaAlgorithmMachine2021",
		"type": "article-journal",
		"abstract": "SafePredict is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, 1 - epsilon, by allowing refusals. Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm so that the error rate on non-refused predictions does not exceed epsilon. The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor. When the base predictor happens not to exceed the target error rate epsilon, SafePredict refuses only a finite number of times. When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee. Empirical results show that (i) SafePredict compares favorably with state-of-the-art confidence-based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals. Our software is included in the supplementary material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2932415.",
		"archive_location": "WOS:000607383300019",
		"container-title": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE",
		"DOI": "10.1109/TPAMI.2019.2932415",
		"ISSN": "0162-8828",
		"issue": "2",
		"page": "663-678",
		"title": "SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness",
		"volume": "43",
		"author": [
			{
				"family": "Kocak",
				"given": "MA"
			},
			{
				"family": "Ramirez",
				"given": "D"
			},
			{
				"family": "Erkip",
				"given": "E"
			},
			{
				"family": "Shasha",
				"given": "DE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					2,
					1
				]
			]
		}
	},
	{
		"id": "badeaMoralityMachinesInterpretation2022",
		"type": "paper-conference",
		"abstract": "We present what we call the Interpretation Problem, whereby any rule in symbolic form is open to infinite interpretation in ways thatwemight disapprove of and argue that any attempt to build morality into machines is subject to it. We show how the Interpretation Problem in Artificial Intelligence is an illustration of Wittgenstein's general claim that no rule can contain the criteria for its own application, and that the risks created by this problem escalates in proportion to the degree to which a machine is causally connected to the world, in what we call the Law of Interpretative Exposure. Using games as an illustration, we attempt to define the structure of normative spaces and argue that any rule-following within a normative space is guided by values that are external to that space and which cannot themselves be represented as rules. In light of this, we categorise the types of mistakes an artificial moral agent could make into Mistakes of Intention and Instrumental Mistakes, and we proposeways of building morality into machines by getting them to interpret the rules we give in accordance with these external values, through explicit moral reasoning, the \"Show, not Tell\" paradigm, the adjustment of causal power and structure of the agent, and relational values, with the ultimate aim that the machine develop a virtuous character and that the impact of the Interpretation Problem is minimised.",
		"archive_location": "WOS:000922637500009",
		"DOI": "10.1007/978-3-031-21441-7_9",
		"event-title": "ARTIFICIAL INTELLIGENCE XXXIX, AI 2022",
		"ISBN": "0302-9743",
		"page": "124-137",
		"title": "Morality, Machines, and the Interpretation Problem: A Value-based, Wittgensteinian Approach to Building Moral Agents",
		"volume": "13652",
		"author": [
			{
				"family": "Badea",
				"given": "C"
			},
			{
				"family": "Artus",
				"given": "G"
			}
		],
		"editor": [
			{
				"family": "Bramer",
				"given": "M"
			},
			{
				"family": "Stahl",
				"given": "F"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "carlanAutomatingSafetyArgument2022",
		"type": "paper-conference",
		"abstract": "The need to make sense of complex input data within a vast variety of unpredictable scenarios has been a key driver for the use of machine learning (ML), for example in Automated Driving Systems (ADS). Such systems are usually safety-critical, and therefore they need to be safety assured. In order to consider the results of the safety assurance activities (scoping uncovering previously unknown hazardous scenarios), a continuous approach to arguing safety is required, whilst iteratively improving ML-specific safety-relevant properties, such as robustness and prediction certainty. Such a continuous safety life cycle will only be practical with an efficient and effective approach to analyzing the impact of system changes on the safety case. In this paper, we propose a semi-automated approach for accurately identifying the impact of changes on safety arguments. We focus on arguments that reason about the sufficiency of the data used for the development of ML components. The approach qualitatively and quantitatively analyses the impact of changes in the input space of the considered ML component on other artifacts created during the execution of the safety life cycle, such as datasets and performance requirements and makes recommendations to safety engineers for handling the identified impact. We implement the proposed approach in a model-based safety engineering environment called FASTEN, and we demonstrate its application for an ML-based pedestrian detection component of an ADS.",
		"archive_location": "WOS:000965064800005",
		"DOI": "10.1109/PRDC55274.2022.00019",
		"event-title": "2022 IEEE 27TH PACIFIC RIM INTERNATIONAL SYMPOSIUM ON DEPENDABLE COMPUTING (PRDC)",
		"ISBN": "1555-094X",
		"page": "43-53",
		"title": "Automating Safety Argument Change Impact Analysis for Machine Learning Components",
		"author": [
			{
				"family": "Carlan",
				"given": "C"
			},
			{
				"family": "Gauerhof",
				"given": "L"
			},
			{
				"family": "Gallina",
				"given": "B"
			},
			{
				"family": "Burton",
				"given": "S"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "elfwingParallelRewardPunishment2017",
		"type": "paper-conference",
		"abstract": "An important issue in reinforcement learning systems for autonomous agents is whether it makes sense to have separate systems for predicting rewards and punishments. In robotics, learning and control are typically achieved by a single controller, with punishments coded as negative rewards. However in biological systems, some evidence suggests that the brain has a separate system for punishment. Although this may in part be due to biological constraints of implementing negative quantities, it raises the question as to whether there is any computational rationale for keeping reward and punishment prediction operationally distinct. Here we outline a basic argument supporting this idea, based on the proposition that learning best-case predictions (as in Q-learning) does not always achieve the safest behaviour. We introduce a modified RL scheme involving a new algorithm which we call 'MaxPain' - which back-ups worst-case predictions in parallel, and then scales the two predictions in a multi-attribute RL policy. i.e. independently learning 'what to do' as well as 'what not to do' and then combining this information. We show how this scheme can improve performance in benchmark RL environments, including a grid-world experiment and a delayed version of the mountain car experiment. In particular, we demonstrate how early exploration and learning are substantially improved, leading to much 'safer' behaviour. In conclusion, the results illustrate the importance of independent punishment prediction in RL, and provide a testable framework for better understanding punishment (such as pain) and avoidance in humans, in both health and disease.",
		"archive_location": "WOS:000491967600019",
		"event-title": "2017 THE SEVENTH JOINT IEEE INTERNATIONAL CONFERENCE ON DEVELOPMENT AND LEARNING AND EPIGENETIC ROBOTICS (ICDL-EPIROB)",
		"ISBN": "2161-9484",
		"page": "140-147",
		"title": "Parallel reward and punishment control in humans and robots: safe reinforcement learning using the MaxPain algorithm",
		"DOI": "10.1109/DEVLRN.2017.8329799",
		"author": [
			{
				"family": "Elfwing",
				"given": "S"
			},
			{
				"family": "Seymour",
				"given": "B"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "khanNoHarmNovel2023",
		"type": "article-journal",
		"abstract": "Given the impact artificial intelligence (AI)-based medical technologies (hardware devices, software programs, and mobile apps) can have on society, debates regarding the principles behind their development and deployment are emerging. Using the biopsychosocial model applied in psychiatry and other fields of medicine as our foundation, we propose a novel 3-step framework to guide industry developers of AI-based medical tools as well as health care regulatory agencies on how to decide if a product should be launched-a \"Go or No-Go\" approach. More specifically, our novel framework places stakeholders' (patients, health care professionals, industry, and government institutions) safety at its core by asking developers to demonstrate the biological-psychological (impact on physical and mental health), economic, and social value of their AI tool before it is launched. We also introduce a novel cost-effective, time-sensitive, and safety-oriented mixed quantitative and qualitative clinical phased trial approach to help industry and government health care regulatory agencies test and deliberate on whether to launch these AI-based medical technologies. To our knowledge, our biological-psychological, economic, and social (BPES) framework and mixed method phased trial approach are the first to place the Hippocratic Oath of \"Do No Harm\" at the center of developers', implementers', regulators', and users' mindsets when determining whether an AI-based medical technology is safe to launch. Moreover, as the welfare of AI users and developers becomes a greater concern, our framework's novel safety feature will allow it to complement existing and future AI reporting guidelines.",
		"archive_location": "WOS:001007075800001",
		"container-title": "JOURNAL OF MEDICAL INTERNET RESEARCH",
		"DOI": "10.2196/43386",
		"ISSN": "1438-8871",
		"title": "A \"Do No Harm\" Novel Safety Checklist and Research Approach to Determine Whether to Launch an Artificial Intelligence-Based Medical Technology: Introducing the Biological-Psychological, Economic, and Social (BPES) Framework",
		"volume": "25",
		"author": [
			{
				"family": "Khan",
				"given": "WU"
			},
			{
				"family": "Seto",
				"given": "E"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					5
				]
			]
		}
	},
	{
		"id": "sallamiSafetyRobustnessDeep2019",
		"type": "paper-conference",
		"abstract": "Embedding machine or deep learning software into safety-critical systems such as autonomous vehicles requires software verification and validation. Such software adds non traceable hazards to traditional hardware and sensors failures, not to mention attacks that fool the prediction of a DNN and hampers its robustness. Formal methods from computer science are now applied to deep neural networks to assess the local and global robustness of a given DNN. Typically static analysis with Abstract Interpretation or SAT solvers approaches are applied to neural networks and leverages the important progress of formal methods over the last decades. Such approaches estimate bounds on the perturbation of the inputs and formally guarantee the same DNN prediction within these bounds. However formal methods over DNN for image perception system have only been applied to simple image attacks (2D rotation, brightness). In this work, we extend the definition of Lower and Upper Bounds to assess the robustness of a DNN perception system against more generic attacks. We propose a general method to verify object recognition systems using Abstract Interpretation theory. Another major contribution is the adaptation of Upper and Lower Bounds with the abstract intervals to support more complex attacks. We consider the three following classes: convolutional attacks, occlusion attacks and geometrical transformations. For the last one, we generalize the geometrical transformations with displacements in the three-dimensional space.",
		"archive_location": "WOS:000651201400030",
		"DOI": "10.1007/978-3-030-36808-1_30",
		"event-title": "NEURAL INFORMATION PROCESSING (ICONIP 2019), PT IV",
		"ISBN": "1865-0929",
		"page": "274-286",
		"title": "Safety and Robustness of Deep Neural Networks Object Recognition Under Generic Attacks",
		"volume": "1142",
		"author": [
			{
				"family": "Sallami",
				"given": "MM"
			},
			{
				"family": "Ibn Khedher",
				"given": "M"
			},
			{
				"family": "Trabelsi",
				"given": "A"
			},
			{
				"family": "Kerboua-Benlarbi",
				"given": "S"
			},
			{
				"family": "Bettebghor",
				"given": "D"
			}
		],
		"editor": [
			{
				"family": "Gedeon",
				"given": "T"
			},
			{
				"family": "Wong",
				"given": "KW"
			},
			{
				"family": "Lee",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "maskaraDevelopingSaferAIconcepts2023",
		"type": "article-journal",
		"abstract": "With the rapid advancement of AI, there exists a possibility of rogue human actor(s) taking control of a potent AI system or an AI system redefining its objective function such that it presents an existential threat to mankind or severely curtails its freedom. Therefore, some suggest an outright ban on AI development while others profess international agreement on constraining specific types of AI. These approaches are untenable because countries will continue developing AI for national defense, regardless. Some suggest having an all-powerful benevolent one-AI that will act as an AI nanny. However, such an approach relies on the everlasting benevolence of one-AI, an untenable proposition. Furthermore, such an AI is itself subject to capture by a rogue actor. We present an alternative approach that uses existing mechanisms and time-tested economic concepts of competition and marginal analysis to limit centralization and integration of AI, rather than AI itself. Instead of depending on international consensus it relies on countries working in their best interests. We recommend that through regulation and subsidies countries promote independent development of competing AI technologies, especially those with decentralized architecture. The Sherman Antitrust Act can be used to limit the domain of an AI system, training module, or any of its components. This will increase the segmentation of potent AI systems and force technological incompatibility across systems. Finally, cross-border communication between AI-enabled systems should be restricted, something countries like China and the US are already inclined to do to serve their national interests. Our approach can ensure the availability of numerous sufficiently powerful AI systems largely disconnected from each other that can be called upon to identify and neutralize rogue systems when needed. This setup can provide sufficient deterrence to any rational human or AI system from attempting to exert undue control.",
		"archive_location": "WOS:001076383400001",
		"container-title": "AI & SOCIETY",
		"DOI": "10.1007/s00146-023-01778",
		"ISSN": "0951-5666",
		"title": "Developing safer AI-concepts from economics to the rescue",
		"author": [
			{
				"family": "Maskara",
				"given": "PK"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					2
				]
			]
		}
	},
	{
		"id": "terraSafetyVsEfficiency2020",
		"type": "paper-conference",
		"abstract": "The use of AI-based risk mitigation is increasing to provide safety in the areas of smart manufacturing, automated logistics etc, where the human-robot collaboration operations are in use. This paper presents our work on implementation of fuzzy logic system (FLS) and reinforcement learning (RL) to build risk mitigation modules for human-robot collaboration scenarios. Risk mitigation using FLS strategy is developed by manually defining the linguistic values, tuning the membership functions and generating the rules based on ISO/TS15066:2016. RL-based risk mitigation modules are developed using three different Qnetworks to estimate the Q-value function. Our purpose is twofold: to perform a comparative analysis of FLS and RL in terms of safety perspectives and further to evaluate the efficiency to accomplish the task. Our results present that all the proposed risk mitigation strategies improve the safety aspect by up to 26% as compared to a default setup where the robot is just relying on a navigation module without risk mitigation. The efficiency of using FLS model is maintained to the default setup, while the efficiency of using RL model is reduced by 26% from the default setup. We also compare the computation performance of risk mitigation between centralized and edge execution where the edge execution is 27.5 times faster than the centralized one.",
		"archive_location": "WOS:000591176900027",
		"DOI": "10.1109/iccar49639.2020.9108037",
		"event-title": "2020 6TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND ROBOTICS (ICCAR)",
		"ISBN": "2251-2446",
		"page": "151-160",
		"title": "Safety vs. Efficiency: AI-Based Risk Mitigation in Collaborative Robotics",
		"author": [
			{
				"family": "Terra",
				"given": "A"
			},
			{
				"family": "Riaz",
				"given": "H"
			},
			{
				"family": "Raizer",
				"given": "K"
			},
			{
				"family": "Hata",
				"given": "A"
			},
			{
				"family": "Inam",
				"given": "R"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kshetrySafetyFaceUnknown2019",
		"type": "paper-conference",
		"abstract": "Most current machine learning algorithms make highly confident yet incorrect classifications when faced with unexpected test samples from an unknown distribution different from training; such epistemic uncertainty (unknown unknowns) can have catastrophic safety implications. In this conceptual paper, we propose a method to leverage engineering science knowledge to control epistemic uncertainty and maintain decision safety. The basic idea is an algorithm fusion approach that combines data-driven learned models with physical system knowledge, to operate between the extremes of purely data-driven classifiers and purely engineering science rules. This facilitates the safe operation of data-driven engineering systems, such as wastewater treatment plants.",
		"archive_location": "WOS:000482554008080",
		"event-title": "2019 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP)",
		"ISBN": "1520-6149",
		"page": "8162-8166",
		"title": "Safety in the Face of Unknown Unknowns: Algorithm Fusion in Data-driven Engineering Systems",
		"DOI": "10.1109/ICASSP.2019.8683392",
		"author": [
			{
				"family": "Kshetry",
				"given": "N"
			},
			{
				"family": "Varshney",
				"given": "LR"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "boggustSharedInterestMeasuring2022",
		"type": "paper-conference",
		"abstract": "Saliency methods-techniques to identify the importance of input features on a model's output-are a common step in understanding neural network behavior. However, interpreting saliency requires tedious manual inspection to identify and aggregate patterns in model behavior, resulting in ad hoc or cherry-picked analysis. To address these concerns, we present Shared Interest: metrics for comparing model reasoning (via saliency) to human reasoning (via ground truth annotations). By providing quantitative descriptors, Shared Interest enables ranking, sorting, and aggregating inputs, thereby facilitating large-scale systematic analysis of model behavior. We use Shared Interest to identify eight recurring patterns in model behavior, such as cases where contextual features or a subset of ground truth features are most important to the model. Working with representative real-world users, we show how Shared Interest can be used to decide if a model is trustworthy, uncover issues missed in manual analyses, and enable interactive probing.",
		"archive_location": "WOS:000890212502034",
		"DOI": "10.1145/3491102.3501965",
		"event-title": "PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22)",
		"ISBN": "978-1-4503-9157-3",
		"title": "Shared Interest: Measuring Human-AI Alignment to Identify Recurring Patterns in Model Behavior",
		"author": [
			{
				"family": "Boggust",
				"given": "A"
			},
			{
				"family": "Hoover",
				"given": "B"
			},
			{
				"family": "Satyanarayan",
				"given": "A"
			},
			{
				"family": "Strobelt",
				"given": "H"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "choMaturityModelTrustworthy2023",
		"type": "article-journal",
		"abstract": "Recently, AI software has been rapidly growing and is widely used in various industrial domains, such as finance, medicine, robotics, and autonomous driving. Unlike traditional software, in which developers need to define and implement specific functions and rules according to requirements, AI software learns these requirements by collecting and training relevant data. For this reason, if unintended biases exist in the training data, AI software can create fairness and safety issues. To address this challenge, we propose a maturity model for ensuring trustworthy and reliable AI software, known as AI-MM, by considering common AI processes and fairness-specific processes within a traditional maturity model, SPICE (ISO/IEC 15504). To verify the effectiveness of AI-MM, we applied this model to 13 real-world AI projects and provide a statistical assessment on them. The results show that AI-MM not only effectively measures the maturity levels of AI projects but also provides practical guidelines for enhancing maturity levels.",
		"archive_location": "WOS:000977808400001",
		"container-title": "APPLIED SCIENCES-BASEL",
		"DOI": "10.3390/app13084771",
		"ISSN": "2076-3417",
		"issue": "8",
		"title": "A Maturity Model for Trustworthy AI Software Development",
		"volume": "13",
		"author": [
			{
				"family": "Cho",
				"given": "S"
			},
			{
				"family": "Kim",
				"given": "I"
			},
			{
				"family": "Kim",
				"given": "J"
			},
			{
				"family": "Woo",
				"given": "H"
			},
			{
				"family": "Shin",
				"given": "W"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					4
				]
			]
		}
	},
	{
		"id": "yuanJointDomainAdaptation2021",
		"type": "article-journal",
		"abstract": "Domain adaptation aims to improve the performance of the classifier in the target domain by reducing the difference between the two domains. Domain shifts usually exist in both marginal distribution and conditional distribution, and their relative importance varies with datasets. Moreover, there is an influence between marginal distribution distance and conditional distribution distance. However, joint domain adaptation approaches rarely consider those. Existing dynamic distribution alignment methods require a feature discriminator, and they need to train a subdomain discriminator for each class. Besides, they don't think about the interaction between the two distribution distances. In this article, we propose a dynamic joint domain adaptation approach, namely Joint Domain Adaptation Based on Adversarial Dynamic Parameter Learning (ADPL), to deal with the above problems. Both marginal distribution alignment and conditional distribution alignment can be implemented by adversarial learning. The dynamic algorithm can keep a balance between marginal and conditional distribution alignment with only two domain discriminators. In addition, the dynamic algorithm takes the influence between the two distribution distances into consideration. Compared with several advanced domain adaptation methods on both text and image datasets, all classification experiments and extensive comparison experiments demonstrate that ADPL has higher learning performance of classification and less running time. This reveals that ADPL outperforms the state-of-the-art domain adaptation approaches.",
		"archive_location": "WOS:000677536500017",
		"container-title": "IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE",
		"DOI": "10.1109/TETCI.2021.3055873",
		"ISSN": "2471-285X",
		"issue": "4",
		"page": "714-723",
		"title": "Joint Domain Adaptation Based on Adversarial Dynamic Parameter Learning",
		"volume": "5",
		"author": [
			{
				"family": "Yuan",
				"given": "YM"
			},
			{
				"family": "Li",
				"given": "YH"
			},
			{
				"family": "Zhu",
				"given": "ZL"
			},
			{
				"family": "Li",
				"given": "RX"
			},
			{
				"family": "Gu",
				"given": "XW"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021",
					8
				]
			]
		}
	},
	{
		"id": "antikainenDeploymentModelExtend2021",
		"type": "paper-conference",
		"abstract": "There is a struggle in Artificial intelligence (AI) ethics to gain ground in actionable methods and models to be utilized by practitioners while developing and implementing ethically sound AI systems. AI ethics is a vague concept without a consensus of definition or theoretical grounding and bearing little connection to practice. Practice involving primarily technical tasks like software development is not aptly equipped to process and decide upon ethical considerations. Efforts to create tools and guidelines to help people working with AI development have been concentrating almost solely on the technical aspects of AI. A few exceptions do apply, such as the ECCOIA method for creating ethically aligned AI -systems. ECCOIA has proven results in terms of increased ethical considerations in AI systems development. Yet, it is a novel innovation, and room for development still exists. This study aims to extend ECCOIA with a deployment model to drive the adoption of ECCOIA, as any method - no matter how good -is of no value without adoption and use. The model includes simple metrics to facilitate the communication of ethical gaps or outcomes of ethical AI development. It offers the opportunity to assess any AI system at any given life-cycle phase, e.g., opening possibilities like analyzing the ethicality of an AI system under acquisition",
		"archive_location": "WOS:000788547300034",
		"DOI": "10.1109/REW53955.2021.00043",
		"event-title": "29TH IEEE INTERNATIONAL REQUIREMENTS ENGINEERING CONFERENCE WORKSHOPS (REW 2021)",
		"ISBN": "978-1-66541-898-0",
		"page": "230-235",
		"title": "A Deployment Model to Extend Ethically Aligned AI Implementation Method ECCOLA",
		"author": [
			{
				"family": "Antikainen",
				"given": "J"
			},
			{
				"family": "Agbese",
				"given": "M"
			},
			{
				"family": "Alanen",
				"given": "HK"
			},
			{
				"family": "Halme",
				"given": "E"
			},
			{
				"family": "Isomäki",
				"given": "H"
			},
			{
				"family": "Jantunen",
				"given": "M"
			},
			{
				"family": "Kemell",
				"given": "KK"
			},
			{
				"family": "Rousi",
				"given": "R"
			},
			{
				"family": "Vainio-Pekka",
				"given": "H"
			},
			{
				"family": "Vakkuri",
				"given": "V"
			}
		],
		"editor": [
			{
				"family": "Yue",
				"given": "T"
			},
			{
				"family": "Mirakhorli",
				"given": "M"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "costonValidityPerspectiveEvaluating2023",
		"type": "paper-conference",
		"abstract": "Recent research increasingly brings to question the appropriateness of using predictive tools in complex, real-world tasks. While a growing body of work has explored ways to improve value alignment in these tools, comparatively less work has centered concerns around the fundamental justifiability of using these tools. This work seeks to center validity considerations in deliberations around whether and how to build data-driven algorithms in high-stakes domains. Toward this end, we translate key concepts from validity theory to predictive algorithms. We apply the lens of validity to re-examine common challenges in problem formulation and data issues that jeopardize the justifiability of using predictive algorithms and connect these challenges to the social science discourse around validity. Our interdisciplinary exposition clarifies how these concepts apply to algorithmic decision making contexts. We demonstrate how these validity considerations could distill into a series of high-level questions intended to promote and document reflections on the legitimacy of the predictive task and the suitability of the data.",
		"archive_location": "WOS:001012311500040",
		"DOI": "10.1109/SaTML54575.2023.00050",
		"event-title": "2023 IEEE CONFERENCE ON SECURE AND TRUSTWORTHY MACHINE LEARNING, SATML",
		"ISBN": "978-1-66546-299-0",
		"page": "690-704",
		"title": "A Validity Perspective on Evaluating the Justified Use of Data-driven Decision-making Algorithms",
		"author": [
			{
				"family": "Coston",
				"given": "A"
			},
			{
				"family": "Kawakami",
				"given": "A"
			},
			{
				"family": "Zhu",
				"given": "HY"
			},
			{
				"family": "Holstein",
				"given": "K"
			},
			{
				"family": "Heidari",
				"given": "H"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "nilsenRewardTamperingEvolutionary2023",
		"type": "article-journal",
		"abstract": "Reward tampering is a problem that will impact the trustworthiness of the powerful AI systems of the future. Reward Tampering describes the problem where AI agents bypass their intended objective, enabling unintended and potentially harmful behaviours. This paper investigates whether the creative potential of evolutionary algorithms could help ensure trustworthy solutions when facing this problem. The reason why evolutionary algorithms may help combat reward tampering is that they are able to find a diverse collection of different solutions to a problem within a single run, aiding the search for desirable solutions. Four different evolutionary algorithms were deployed in tasks illustrating the problem of reward tampering. The algorithms were designed with varying degrees of human expertise, measuring how human guidance influences the ability to discover trustworthy solutions. The results indicate that the algorithms' ability to find and preserve trustworthy solutions is very dependent on preserving diversity during the search. Algorithms searching for behavioural diversity showed to be the most effective against reward tampering. Human expertise also showed to improve the certainty and quality of safe solutions, but even with only a minimal degree of human expertise, domain-independent diversity management was found to discover safe solutions.",
		"archive_location": "WOS:001068097800001",
		"container-title": "GENETIC PROGRAMMING AND EVOLVABLE MACHINES",
		"DOI": "10.1007/s10710-023-09456-0",
		"ISSN": "1389-2576",
		"issue": "2",
		"title": "Reward tampering and evolutionary computation: a study of concrete AI-safety problems using evolutionary algorithms",
		"volume": "24",
		"author": [
			{
				"family": "Nilsen",
				"given": "MK"
			},
			{
				"family": "Nygaard",
				"given": "TF"
			},
			{
				"family": "Ellefsen",
				"given": "KO"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					12
				]
			]
		}
	},
	{
		"id": "freieslebenGeneralizationTheoryRobustness2023",
		"type": "article-journal",
		"abstract": "The term robustness is ubiquitous in modern Machine Learning (ML). However, its meaning varies depending on context and community. Researchers either focus on narrow technical definitions, such as adversarial robustness, natural distribution shifts, and performativity, or they simply leave open what exactly they mean by robustness. In this paper, we provide a conceptual analysis of the term robustness, with the aim to develop a common language, that allows us to weave together different strands of robustness research. We define robustness as the relative stability of a robustness target with respect to specific interventions on a modifier. Our account captures the various sub-types of robustness that are discussed in the research literature, including robustness to distribution shifts, prediction robustness, or the robustness of algorithmic explanations. Finally, we delineate robustness from adjacent key concepts in ML, such as extrapolation, generalization, and uncertainty, and establish it as an independent epistemic concept. © 2023, The Author(s).",
		"archive": "Scopus",
		"container-title": "Synthese",
		"DOI": "10.1007/s11229-023-04334-9",
		"issue": "4",
		"title": "Beyond generalization: a theory of robustness in machine learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172812290&doi=10.1007%2fs11229-023-04334-9&partnerID=40&md5=7e9f1df24e78804b758b8df25867fe87",
		"volume": "202",
		"author": [
			{
				"family": "Freiesleben",
				"given": "T."
			},
			{
				"family": "Grote",
				"given": "T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "soremekunBackdoorAttacksDefense2023",
		"type": "article-journal",
		"abstract": "The introduction of robust optimisation has pushed the state-of-the-art in defending against adversarial attacks. Notably, the state-of-the-art projected gradient descent (PGD) -based training method has been shown to be universally and reliably effective in defending against adversarial inputs. This robustness approach uses PGD as a reliable and universal “first-order adversary”. However, the behaviour of such optimisation has not been studied in the light of a fundamentally different class of attacks called backdoors. In this paper, we study how to inject and defend against backdoor attacks for robust models trained using PGD-based robust optimisation. We demonstrate that these models are susceptible to backdoor attacks. Subsequently, we observe that backdoors are reflected in the feature representation of such models. Then, this observation is leveraged to detect such backdoor-infected models via a detection technique called AEGIS. Specifically, given a robust Deep Neural Network (DNN) that is trained using PGD-based first-order adversarial training approach, AEGIS uses feature clustering to effectively detect whether such DNNs are backdoor-infected or clean. In our evaluation of several visible and hidden backdoor triggers on major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS effectively detects PGD-trained robust DNNs infected with backdoors. AEGIS detects such backdoor-infected models with 91.6% accuracy (11 out of 12 tested models), without any false positives. Furthermore, AEGIS detects the targeted class in the backdoor-infected model with a reasonably low (11.1%) false positive rate. Our investigation reveals that salient features of adversarially robust DNNs could be promising to break the stealthy nature of backdoor attacks. © 2023 Elsevier Ltd",
		"archive": "Scopus",
		"container-title": "Computers and Security",
		"DOI": "10.1016/j.cose.2023.103101",
		"title": "Towards Backdoor Attacks and Defense in Robust Machine Learning Models",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147089205&doi=10.1016%2fj.cose.2023.103101&partnerID=40&md5=082a38fe3e54aee9a4c98eb655378cfa",
		"volume": "127",
		"author": [
			{
				"family": "Soremekun",
				"given": "E."
			},
			{
				"family": "Udeshi",
				"given": "S."
			},
			{
				"family": "Chattopadhyay",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gholampourAdversarialRobustnessPhishing2023",
		"type": "paper-conference",
		"abstract": "Developing robust detection models against phishing emails has long been the main concern of the cyber defense community. Currently, public phishing/legitimate datasets lack adversarial email examples which keeps the detection models vulnerable. To address this problem, we developed an augmented phishing/legitimate email dataset, utilizing different adversarial text attack techniques. Next, the models were retrained with the adversarial dataset. Results showed that accuracy and F1 score of the models improved under subsequent attacks. In another experiment, synthetic phishing emails were generated using a fine-tuned GPT-2 model. The detection model was retrained with a newly formed synthetic dataset. Subsequently, we observed that the accuracy and robustness of the model did not improve significantly under black box attack methods. In the last experiment, we proposed a defensive technique to classify adversarial examples to their true labels using a K-Nearest Neighbor approach with 94% accuracy in our prediction.  © 2023 ACM.",
		"archive": "Scopus",
		"DOI": "10.1145/3579987.3586567",
		"event-title": "IWSPA 2023 - Proceedings of the 9th ACM International Workshop on Security and Privacy Analytics",
		"page": "67-76",
		"title": "Adversarial Robustness of Phishing Email Detection Models",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159123101&doi=10.1145%2f3579987.3586567&partnerID=40&md5=a1c834530dbd2c55d36ba67001fdbe55",
		"author": [
			{
				"family": "Gholampour",
				"given": "P.M."
			},
			{
				"family": "Verma",
				"given": "R.M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "feldkampExplainableAIExplainable2023",
		"type": "paper-conference",
		"abstract": "Evaluating robustness is an important goal in simulation-based analysis. Robustness is achieved when the controllable factors of a system are adjusted in such a way that any possible variance in uncontrollable factors (noise) has minimal impact on the variance of the desired output. The optimization of system robustness using simulation is a dedicated and well-established research direction. However, once a simulation model is available, there is a lot of potential to learn more about the inherent relationships in the system, especially regarding its robustness. Data farming offers the possibility to explore large design spaces using smart experiment design, high performance computing, automated analysis, and interactive visualization. Sophisticated machine learning methods excel at recognizing and modelling the relation between large amounts of simulation input and output data. However, investigating and analyzing this modelled relationship can be very difficult, since most modern machine learning methods like neural networks or random forests are opaque black boxes. Explainable Artificial Intelligence (XAI) can help to peak into this black box, helping us to explore and learn about relations between simulation input and output. In this paper, we introduce a concept for using Data Farming, machine learning and XAI to investigate and understand system robustness of a given simulation model. © 2023 Owner/Author.",
		"archive": "Scopus",
		"DOI": "10.1145/3573900.3591114",
		"event-title": "ACM International Conference Proceeding Series",
		"page": "96-106",
		"title": "From Explainable AI to Explainable Simulation: Using Machine Learning and XAI to understand System Robustness",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163894026&doi=10.1145%2f3573900.3591114&partnerID=40&md5=1d1bd8540275e7e02e822b4891aad1dc",
		"author": [
			{
				"family": "Feldkamp",
				"given": "N."
			},
			{
				"family": "Strassburger",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "robinetteSelfPreservingGeneticAlgorithms2023",
		"type": "paper-conference",
		"abstract": "Self-Preserving Genetic Algorithms (SPGA) combine the evolutionary strategy of a genetic algorithm with safety assurance methods commonly implemented in safe reinforcement learning (SRL), a branch of reinforcement learning (RL) that accounts for safety in the exploration and decision-making process of the agent. Safe learning approaches are especially important in safety-critical environments, where failure to account for the safety of the controlled system could result in the loss of millions of dollars in hardware or bodily harm to people working nearby, as is true of many cyber-physical systems. While SRL is a viable approach to safe learning, there are many challenges that must be taken into consideration when training agents, such as sample efficiency, stability, and exploration—an issue that is easily addressed by the evolutionary strategy of a genetic algorithm. By combining GAs with the safety mechanisms used with SRL, SPGA offers a safe learning alternative that is able to explore large areas of the solution space, addressing SRL’s challenge of exploration. This work implements SPGA with both action masking and run time assurance safety strategies to evolve safe controllers for three types of discrete action space environments applicable to cyber physical systems (control, routing, and operations) and under various safety conditions. Training and testing evaluation metrics are compared with results from SRL trained controllers to validate results. SPGA and SRL controllers are trained across 5 random seeds and evaluated on 500 episodes to calculate average wall time to train, average expected return, and percentage of safe action evaluation metrics. SPGA achieves comparable reward and safety performance results with significantly improved training efficiency (55x faster on average), demonstrating the effectiveness of this safe learning approach. © ICCPS 2023. All rights reserved.",
		"archive": "Scopus",
		"DOI": "10.1145/3576841.3585936",
		"event-title": "ICCPS 2023 - Proceedings of the 2023 ACM/IEEE 14th International Conference on Cyber-Physical Systems with CPS-IoT Week 2023",
		"page": "110-119",
		"title": "Self-Preserving Genetic Algorithms for Safe Learning in Discrete Action Spaces",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167866179&doi=10.1145%2f3576841.3585936&partnerID=40&md5=01b2dc767bb29f9262c5ca95bfd80333",
		"author": [
			{
				"family": "Robinette",
				"given": "P.K."
			},
			{
				"family": "Hamilton",
				"given": "N.P."
			},
			{
				"family": "Johnson",
				"given": "T.T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "deyMultilayeredCollaborativeFramework2023",
		"type": "paper-conference",
		"abstract": "In the days of AI, data-centric machine learning (ML) models are increasingly used in various complex systems. While many researchers are focusing on specifying ML-specific performance requirements, not enough guideline is provided to engineer the data requirements systematically involving diverse stakeholders. Lack of written agreement about the training data, collaboration bottlenecks, lack of data validation framework, etc. are posing new challenges to ensuring training data fitness for safety-critical ML components. To reduce these gaps, we propose a multi-layered framework that helps to perceive and elicit data requirements. We provide a template for verifiable data requirements specifications. Moreover, we show how such requirements can facilitate an evidence-driven assessment of the training data quality based on the experts' judgments about the satisfaction of the requirements. We use Dempster Shafer's theory to combine experts' subjective opinions in the process. A preliminary case study on the CityPersons dataset for the pedestrian detection feature of autonomous cars shows the usefulness of the proposed framework for data requirements understanding and the confidence assessment of the dataset.  © 2023 ACM.",
		"archive": "Scopus",
		"DOI": "10.1145/3555776.3577647",
		"event-title": "Proceedings of the ACM Symposium on Applied Computing",
		"page": "1404-1413",
		"title": "A Multi-layered Collaborative Framework for Evidence-driven Data Requirements Engineering for Machine Learning-based Safety-critical Systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162869276&doi=10.1145%2f3555776.3577647&partnerID=40&md5=f371778982c09e76b7bdfe7aeb0c546f",
		"author": [
			{
				"family": "Dey",
				"given": "S."
			},
			{
				"family": "Lee",
				"given": "S.-W."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "tunaTENETNewHybrid2023",
		"type": "article-journal",
		"abstract": "Deep neural network (DNN) models are widely renowned for their resistance to random perturbations. However, researchers have found out that these models are indeed extremely vulnerable to deliberately crafted and seemingly imperceptible perturbations of the input, referred to as adversarial examples. Adversarial attacks have the potential to substantially compromise the security of DNN-powered systems and posing high risks especially in the areas where security is a top priority. Numerous studies have been conducted in recent years to defend against these attacks and to develop more robust architectures resistant to adversarial threats. In this study, we propose a new architecture and enhance a recently proposed technique by which we can restore adversarial samples back to their original class manifold. We leverage the use of several uncertainty metrics obtained from Monte Carlo dropout (MC Dropout) estimates of the model together with the model’s own loss function and combine them with the use of defensive distillation technique to defend against these attacks. We have experimentally evaluated and verified the efficacy of our approach on MNIST (Digit), MNIST (Fashion) and CIFAR10 datasets. In our experiments, we showed that our proposed method reduces the attack’s success rate lower than 5% without compromising clean accuracy. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE.",
		"archive": "Scopus",
		"container-title": "International Journal of Information Security",
		"DOI": "10.1007/s10207-023-00675-1",
		"issue": "4",
		"page": "987-1004",
		"title": "TENET: a new hybrid network architecture for adversarial defense",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150178910&doi=10.1007%2fs10207-023-00675-1&partnerID=40&md5=5f8f82fcebf7de7f98e293b8a6974614",
		"volume": "22",
		"author": [
			{
				"family": "Tuna",
				"given": "O.F."
			},
			{
				"family": "Catak",
				"given": "F.O."
			},
			{
				"family": "Eskil",
				"given": "M.T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kasirzadehUserTamperingReinforcement2023",
		"type": "paper-conference",
		"abstract": "In this paper, we introduce new formal methods and provide empirical evidence to highlight a unique safety concern prevalent in reinforcement learning (RL)-based recommendation algorithms - 'user tampering.' User tampering is a situation where an RL-based recommender system may manipulate a media user's opinions through its suggestions as part of a policy to maximize long-term user engagement. We use formal techniques from causal modeling to critically analyze prevailing solutions proposed in the literature for implementing scalable RL-based recommendation systems, and we observe that these methods do not adequately prevent user tampering. Moreover, we evaluate existing mitigation strategies for reward tampering issues, and show that these methods are insufficient in addressing the distinct phenomenon of user tampering within the context of recommendations. We further reinforce our findings with a simulation study of an RL-based recommendation system focused on the dissemination of political content. Our study shows that a Q-learning algorithm consistently learns to exploit its opportunities to polarize simulated users with its early recommendations in order to have more consistent success with subsequent recommendations that align with this induced polarization. Our findings emphasize the necessity for developing safer RL-based recommendation systems and suggest that achieving such safety would require a fundamental shift in the design away from the approaches we have seen in the recent literature.  © 2023 Owner/Author.",
		"archive": "Scopus",
		"DOI": "10.1145/3600211.3604669",
		"event-title": "AIES 2023 - Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",
		"page": "58-69",
		"title": "User Tampering in Reinforcement Learning Recommender Systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172357979&doi=10.1145%2f3600211.3604669&partnerID=40&md5=799ac343804d2feac22eb741385ca0bc",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "A."
			},
			{
				"family": "Evans",
				"given": "C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chenCurriculumLearningbasedFuzzy2023",
		"type": "article-journal",
		"abstract": "To improve the robustness of SVM models to noise and outliers, fuzzy support vector machine (FSVM) has been proposed. However, many existing FSVM models have limitations such as their dependence on assumptions, limited optimization, and unreasonable handling of noise. To address these problems, we propose a novel approach called curriculum learning-based FSVM. Our approach employs a curriculum-learning strategy, where model initially learns easy samples to avoid noise interference and obtain a good initial solution, before proceeding to learn all samples, including hard ones. To distinguish between easy and hard samples, we introduce an adaptive density-based clustering model and it is extended to kernel feature space. Moreover, we propose a slack variable-based fuzzy membership function to evaluate the importance of samples. Additionally, our model adaptively adapts the importance of samples based on feedback during the learning process. Finally, our experimental results on popular benchmarks demonstrate that our proposed model outperforms existing competitors in terms of accuracy and robustness. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Fuzzy Systems",
		"DOI": "10.1109/TFUZZ.2023.3319170",
		"page": "1-15",
		"title": "Curriculum learning-based fuzzy support vector machine",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173338514&doi=10.1109%2fTFUZZ.2023.3319170&partnerID=40&md5=228b8e7997aa7b33157da00f29d0e3a6",
		"author": [
			{
				"family": "Chen",
				"given": "B."
			},
			{
				"family": "Gao",
				"given": "Y."
			},
			{
				"family": "Liu",
				"given": "J."
			},
			{
				"family": "Weng",
				"given": "W."
			},
			{
				"family": "Huang",
				"given": "J."
			},
			{
				"family": "Fan",
				"given": "Y."
			},
			{
				"family": "Lan",
				"given": "W."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "shiNearOptimalAlgorithmSafe2023",
		"type": "paper-conference",
		"abstract": "In many applications of Reinforcement Learning (RL), it is critically important that the algorithm performs safely, such that instantaneous hard constraints are satisfied at each step, and unsafe states and actions are avoided. However, existing algorithms for “safe” RL are often designed under constraints that either require expected cumulative costs to be bounded or assume all states are safe. Thus, such algorithms could violate instantaneous hard constraints and traverse unsafe states (and actions) in practice. Hence, in this paper, we develop the first near-optimal safe RL algorithm for episodic Markov Decision Processes with unsafe states and actions under instantaneous hard constraints and the linear mixture model. It achieves a regret (Equation presented) that nearly matches the state-of-the-art regret in the setting with only unsafe actions and that in the unconstrained setting, and is safe at each step, where d is the feature-mapping dimension, K is the number of episodes, H is the episode length, and ∆c is a safety-related parameter. We also provide a lower bound (Equation presented), which indicates that the dependency on ∆c is necessary. Further, both our algorithm design and regret analysis involve several novel ideas, which may be of independent interest. © 2023 Proceedings of Machine Learning Research. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "31243-31268",
		"title": "A Near-Optimal Algorithm for Safe Reinforcement Learning Under Instantaneous Hard Constraints",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174407935&partnerID=40&md5=e136ba097637fe4a30217c735b53fb7b",
		"volume": "202",
		"author": [
			{
				"family": "Shi",
				"given": "M."
			},
			{
				"family": "Liang",
				"given": "Y."
			},
			{
				"family": "Shroff",
				"given": "N."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kirchheimDeepAnomalyDetection2023",
		"type": "paper-conference",
		"abstract": "Machine learning models tend to only make reliable predictions for inputs that are similar to the training data. Consequentially, anomaly detection, which can be used to detect unusual inputs, is critical for ensuring the safety of machine learning agents operating in open environments. In this work, we identify and discuss several limitations of current anomaly detection methods, such as their weak performance on tasks that require abstract reasoning, the inability to integrate background knowledge, and the opaqueness that undermines their trustworthiness in critical applications. Furthermore, we propose an architecture for anomaly detection models that aims to integrate structured knowledge representations to address these limitations. Our hypothesis is that this approach can improve performance and robustness, reduce the required resources (such as data and computation), and provide a higher degree of transparency. As a result, our work contributes to the increased safety of machine learning systems. Our code is publicly available. (https://github.com/kkirchheim/sumnist )",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-031-40953-0_32",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "382-389",
		"title": "Towards Deep Anomaly Detection with Structured Knowledge Representations",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172421520&doi=10.1007%2f978-3-031-40953-0_32&partnerID=40&md5=e50642e6d72d42491a1f6b39cdb01a4a",
		"volume": "14182 LNCS",
		"author": [
			{
				"family": "Kirchheim",
				"given": "K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "moRobustDataSampling2023",
		"type": "article-journal",
		"abstract": "How to sample training/validation data is an important question for machine learning models, especially when the dataset is heterogeneous and skewed. In this paper, we propose a data sampling method that robustly selects training/validation data. We formulate the training/validation data sampling process as a two-player game: a trainer aims to sample training data so as to minimize the test error, while a validator adversarially samples validation data that can increase the test error. Robust sampling is achieved at the game equilibrium. To accelerate the searching process, we adopt reinforcement learning aided Monte Carlo trees search (MCTS). We apply our method to a car-following modeling problem, a complicated scenario with heterogeneous and random human driving behavior. Real-world data, the Next Generation SIMulation (NGSIM), is used to validate this method, and experiment results demonstrate the sampling robustness and thereby the model out-of-sample performance. © 2023 by the authors.",
		"archive": "Scopus",
		"container-title": "Games",
		"DOI": "10.3390/g14010013",
		"issue": "1",
		"title": "Robust Data Sampling in Machine Learning: A Game-Theoretic Framework for Training and Validation Data Selection",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148599130&doi=10.3390%2fg14010013&partnerID=40&md5=1befbf74e2c83028ea4123204f7abeea",
		"volume": "14",
		"author": [
			{
				"family": "Mo",
				"given": "Z."
			},
			{
				"family": "Di",
				"given": "X."
			},
			{
				"family": "Shi",
				"given": "R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kammConceptDynamicRobust2023",
		"type": "paper-conference",
		"abstract": "With the increasing amount of available and connected data sources, industrial automation applications such as condition monitoring of a production machine can be improved by considering various data. To gain insights from this data and make it useable, heterogeneous data has to be analyzed intensively. Limited machine learning approaches exist in industrial automation and manufacturing for analyzing data acquired from multiple sources. In this paper, first, a suitable concept for handling heterogeneous data from integration to analysis is presented as well as a multi-layer architecture for the concept's realization. The architecture encapsulates functionalities into the different layers and allows easy extendability and modifiability. Afterwards, a context modeling approach for managing heterogeneous data and existing approaches and algorithms for analyzing this data robustly and dynamically analyzing it are presented. © 2023 Elsevier B.V.. All rights reserved.",
		"archive": "Scopus",
		"DOI": "10.1016/j.procir.2023.06.061",
		"event-title": "Procedia CIRP",
		"page": "354-359",
		"title": "A Concept for Dynamic and Robust Machine Learning with Context Modeling for Heterogeneous Manufacturing Data",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173582100&doi=10.1016%2fj.procir.2023.06.061&partnerID=40&md5=93caaf62c655e13f5089c9dd4ee26c65",
		"volume": "118",
		"author": [
			{
				"family": "Kamm",
				"given": "S."
			},
			{
				"family": "Sahlab",
				"given": "N."
			},
			{
				"family": "Jazdi",
				"given": "N."
			},
			{
				"family": "Weyrich",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zwaneSafeTrajectorySampling2023",
		"type": "paper-conference",
		"abstract": "Model-based reinforcement learning aims to learn a policy to solve a target task by leveraging a learned dynamics model. This approach, paired with principled handling of uncertainty allows for data-efficient policy learning in robotics. However, the physical environment has feasibility and safety constraints that need to be incorporated into the policy before it is safe to execute on a real robot. In this work, we study how to enforce the aforementioned constraints in the context of model-based reinforcement learning with probabilistic dynamics models. In particular, we investigate how trajectories sampled from the learned dynamics model can be used on a real robot, while fulfilling user-specified safety requirements. We present a model-based reinforcement learning approach using Gaussian processes where safety constraints are taken into account without simplifying Gaussian assumptions on the predictive state distributions. We evaluate the proposed approach on different continuous control tasks with varying complexity and demonstrate how our safe trajectory-sampling approach can be directly used on a real robot without violating safety constraints.  © 2023 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/CASE56687.2023.10260496",
		"event-title": "IEEE International Conference on Automation Science and Engineering",
		"title": "Safe Trajectory Sampling in Model-Based Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174425795&doi=10.1109%2fCASE56687.2023.10260496&partnerID=40&md5=edfccd70835d83e66fac61314c7dd926",
		"volume": "2023-August",
		"author": [
			{
				"family": "Zwane",
				"given": "S."
			},
			{
				"family": "Hadjivelichkov",
				"given": "D."
			},
			{
				"family": "Luo",
				"given": "Y."
			},
			{
				"family": "Bekiroglu",
				"given": "Y."
			},
			{
				"family": "Kanoulas",
				"given": "D."
			},
			{
				"family": "Deisenroth",
				"given": "M.P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "diemertSafetyIntegrityLevels2023",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence (AI) and Machine Learning (ML) technologies are rapidly being adopted to perform safety-related tasks in critical systems. These AI-based systems pose significant challenges, particularly regarding their assurance. Existing safety approaches defined in internationally recognized standards such as ISO 26262, DO-178C, UL 4600, EN 50126, and IEC 61508 do not provide detailed guidance on how to assure AI-based systems. For conventional (non-AI) systems, these standards adopt a ‘Level of Rigor’ (LoR) approach, where increasingly demanding engineering activities are required as risk associated with the system increases. This paper proposes an extension to existing LoR approaches, which considers the complexity of the task(s) being performed by an AI-based component. Complexity is assessed in terms of input entropy and output non-determinism, and then combined with the allocated Safety Integrity Level (SIL) to produce an AI-SIL. That AI-SIL may be used to identify appropriate measures and techniques for the development and verification of the system. The proposed extension is illustrated by examples from the automotive, aviation, and medical industries. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-031-40953-0_34",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "397-409",
		"title": "Safety Integrity Levels for Artificial Intelligence",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172417895&doi=10.1007%2f978-3-031-40953-0_34&partnerID=40&md5=5d73b325a3baac32df8f86740108eda6",
		"volume": "14182 LNCS",
		"author": [
			{
				"family": "Diemert",
				"given": "S."
			},
			{
				"family": "Millet",
				"given": "L."
			},
			{
				"family": "Groves",
				"given": "J."
			},
			{
				"family": "Joyce",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "deyImperativeActionMasking2023",
		"type": "paper-conference",
		"abstract": "Reinforcement Learning (RL) needs sufficient exploration to learn an optimal policy. However, exploratory actions could lead the learning agent to safety hazards, not necessarily in the next state but in the future. Therefore, it is essential to evaluate each action beforehand to ensure safety. The exploratory actions and the actions proposed by the RL agent could also be unsafe during training and in the deployment phase. In this work, we have proposed the Imperative Action Masking Framework, a Graph-Plan-based method considering a finite and small look ahead to assess the safety of actions from the current state. This information is used to construct action masks on the run, filtering out the unsafe actions proposed by the RL agent (including the exploitative ones). The Graph-Plan-based method makes our framework interpretable, while the finite and small look ahead makes the proposed method scalable for larger environments. However, considering the finite and small look ahead comes with a cost of overlooking safety beyond the look ahead. We have done a comparative study against the probabilistic safety shield in Pacman and Warehouse environments approach. Our framework has produced better results in terms of both safety and reward. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-031-40878-6_8",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "130-142",
		"title": "Imperative Action Masking for Safe Exploration in Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172194779&doi=10.1007%2f978-3-031-40878-6_8&partnerID=40&md5=d76cfee6e9ec5905292cddffea4cd7cb",
		"volume": "14127 LNAI",
		"author": [
			{
				"family": "Dey",
				"given": "S."
			},
			{
				"family": "Bhat",
				"given": "S."
			},
			{
				"family": "Dasgupta",
				"given": "P."
			},
			{
				"family": "Dey",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "liCurricularRobustReinforcement2023",
		"type": "article-journal",
		"abstract": "Reinforcement learning (RL), one of three branches of machine learning, aims for autonomous learning and is now greatly driving the artificial intelligence development, especially in autonomous distributed systems, such as cooperative Boston Dynamics robots. However, robust RL has been a challenging problem of reliable aspects due to the gap between laboratory simulation and real world. Existing efforts have been made to approach this problem, such as performing random environmental perturbations in the learning process. However, one cannot guarantee to train with a positive perturbation as bad ones might bring failures to RL. In this work, we treat robust RL as a multi-task RL problem, and propose a curricular robust RL approach. We first present a generative adversarial network (GAN) based task generation model to iteratively output new tasks at the appropriate level of difficulty for the current policy. Furthermore, with these progressive tasks, we can realize curricular learning and finally obtain a robust policy. Extensive experiments in multiple environments demonstrate that our method improves the training stability and is robust to differences in training/test conditions.  © 1996-2012 Tsinghua University Press.",
		"archive": "Scopus",
		"container-title": "Tsinghua Science and Technology",
		"DOI": "10.26599/TST.2021.9010076",
		"issue": "1",
		"page": "27-38",
		"title": "Curricular Robust Reinforcement Learning via GAN-Based Perturbation Through Continuously Scheduled Task Sequence",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135316788&doi=10.26599%2fTST.2021.9010076&partnerID=40&md5=394a312cdebd40ed1f5ed1d35cf53987",
		"volume": "28",
		"author": [
			{
				"family": "Li",
				"given": "Y."
			},
			{
				"family": "Tian",
				"given": "Y."
			},
			{
				"family": "Tong",
				"given": "E."
			},
			{
				"family": "Niu",
				"given": "W."
			},
			{
				"family": "Xiang",
				"given": "Y."
			},
			{
				"family": "Chen",
				"given": "T."
			},
			{
				"family": "Wu",
				"given": "Y."
			},
			{
				"family": "Liu",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhangEvaluatingModelFreeReinforcement2023",
		"type": "paper-conference",
		"abstract": "Safety comes first in many real-world applications involving autonomous agents. Despite a large number of reinforcement learning (RL) methods focusing on safety-critical tasks, there is still a lack of high-quality evaluation of those algorithms that adheres to safety constraints at each decision step under complex and unknown dynamics. In this paper, we revisit prior work in this scope from the perspective of state-wise safe RL and categorize them as projection-based, recovery-based, and optimization-based approaches, respectively. Furthermore, we propose Unrolling Safety Layer (USL), a joint method that combines safety optimization and safety projection. This novel technique explicitly enforces hard constraints via the deep unrolling architecture and enjoys structural advantages in navigating the trade-off between reward improvement and constraint satisfaction. To facilitate further research in this area, we reproduce related algorithms in a unified pipeline and incorporate them into SafeRL-Kit, a toolkit that provides off-the-shelf interfaces and evaluation utilities for safety-critical tasks. We then perform a comparative study of the involved algorithms on six benchmarks ranging from robotic control to autonomous driving. The empirical results provide an insight into their applicability and robustness in learning zero-cost-return policies without task-dependent handcrafting. The project page is available at https://sites.google.com/view/saferlkit. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023",
		"page": "15313-15321",
		"title": "Evaluating Model-Free Reinforcement Learning toward Safety-Critical Tasks",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167964284&partnerID=40&md5=792e1228a61ef0eed74f6cb077dba25d",
		"volume": "37",
		"author": [
			{
				"family": "Zhang",
				"given": "L."
			},
			{
				"family": "Zhang",
				"given": "Q."
			},
			{
				"family": "Shen",
				"given": "L."
			},
			{
				"family": "Yuan",
				"given": "B."
			},
			{
				"family": "Wang",
				"given": "X."
			},
			{
				"family": "Tao",
				"given": "D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "mauriRobustMLModel2023",
		"type": "article-journal",
		"abstract": "In this paper, we improve the robustness of Machine Learning (ML) classifiers against training-time attacks by linking the risk of training data being tampered with to the redundancy in the ML model's design needed to prevent it. Our defense mechanism is directly applicable to classifiers' training data, without any knowledge of the specific ML model to be hardened. First, we compute the training data proximity to class separation surfaces, identified via a reference linear model. Each data point is associated with a risk index, which is used to partition the training set by an unsupervised technique. Then, we train a learner for each partition and combine the learners' output in an ensemble. Our method treats the protected ML classifier as a black box and is inherently robust to transfer attacks. Experiments show that, for data poisoning rates between 6 and 25 percent of the training set, our method is more robust compared to benchmarks and to a monolithic version of the model trained on the whole training set. Our results make a convincing case for adopting training set partitioning and ensemble generation as a stage of ML models' development and deployment lifecycle. © 2023 The Author(s)",
		"archive": "Scopus",
		"container-title": "Information Sciences",
		"DOI": "10.1016/j.ins.2023.03.085",
		"page": "122-140",
		"title": "Robust ML model ensembles via risk-driven anti-clustering of training data",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150025882&doi=10.1016%2fj.ins.2023.03.085&partnerID=40&md5=85e7c63d37ea275829e2bc1b82c9fb9f",
		"volume": "633",
		"author": [
			{
				"family": "Mauri",
				"given": "L."
			},
			{
				"family": "Apolloni",
				"given": "B."
			},
			{
				"family": "Damiani",
				"given": "E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "jeddiMemoryaugmentedLyapunovbasedSafe2023",
		"type": "article-journal",
		"abstract": "Despite recent advances in safe reinforcement learning (RL), safety constraints are often violated at deployment; especially under extreme uncertainty in memory-based partially observable environments. To address these limitations, we propose a memory-augmented Lyapunov-based safe RL model. The primary contributions of our method include (i) an explicit memory module based on Transformers to process long time horizons of information feedback from the environment; (ii) a memory-augmented Lyapunov function to determine a safe set of policies, and (iii) an exploration module that identifies highly rewarding safe actions by characterizing the uncertainty in the environment. We evaluate the proposed model in reactive OpenAI Safety Gym and memory-based partially observable DMLab-30 environments. The results of these experiments show that the proposed method significantly outperforms state-of-the-art baselines. Specifically, our proposed method achieves the lowest constraint costs among the tested benchmarks, while delivering high returns. Moreover, we perform ablation studies that show significant contributions of the introduced Transformer-based encoder, memory-augmented Lyapunov functions, and the uncertainty-aware exploration module. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Artificial Intelligence",
		"DOI": "10.1109/TAI.2023.3238700",
		"page": "1-10",
		"title": "Memory-augmented Lyapunov-based safe reinforcement learning: end-to-end safety under uncertainty",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147271121&doi=10.1109%2fTAI.2023.3238700&partnerID=40&md5=66191103fffb274acb1a9974b97f7d3a",
		"author": [
			{
				"family": "Jeddi",
				"given": "A.B."
			},
			{
				"family": "Dehghani",
				"given": "N.L."
			},
			{
				"family": "Shafieezadeh",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhouRobustMeanFieldActorCritic2023",
		"type": "article-journal",
		"abstract": "Multiagent deep reinforcement learning (DRL) makes optimal decisions dependent on system states observed by agents, but any uncertainty on the observations may mislead agents to take wrong actions. The mean-field actor-critic (MFAC) reinforcement learning is well-known in the multiagent field since it can effectively handle a scalability problem. However, it is sensitive to state perturbations that can significantly degrade the team rewards. This work proposes a Robust MFAC (RoMFAC) reinforcement learning that has two innovations: 1) a new objective function of training actors, composed of a policy gradient function that is related to the expected cumulative discount reward on sampled clean states and an action loss function that represents the difference between actions taken on clean and adversarial states and 2) a repetitive regularization of the action loss, ensuring the trained actors to obtain excellent performance. Furthermore, this work proposes a game model named a state-adversarial stochastic game (SASG). Despite the Nash equilibrium of SASG may not exist, adversarial perturbations to states in the RoMFAC are proven to be defensible based on SASG. Experimental results show that RoMFAC is robust against adversarial perturbations while maintaining its competitive performance in environments without perturbations. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Neural Networks and Learning Systems",
		"DOI": "10.1109/TNNLS.2023.3278715",
		"page": "1-12",
		"title": "A Robust Mean-Field Actor-Critic Reinforcement Learning Against Adversarial Perturbations on Agent States",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161516711&doi=10.1109%2fTNNLS.2023.3278715&partnerID=40&md5=a842a020dd2078511d253e34c9126787",
		"author": [
			{
				"family": "Zhou",
				"given": "Z."
			},
			{
				"family": "Liu",
				"given": "G."
			},
			{
				"family": "Zhou",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "costaRobustLearningMethodology2023",
		"type": "article-journal",
		"abstract": "Robust learning is an important issue in Scientific Machine Learning (SciML). There are several works in the literature addressing this topic. However, there is an increasing demand for methods that can simultaneously consider all the different uncertainty components involved in SciML model identification. Hence, this work proposes a comprehensive methodology for uncertainty evaluation of the SciML that also considers several possible sources of uncertainties involved in the identification process. The uncertainties considered in the proposed method are the absence of a theory, causal models, sensitivity to data corruption or imperfection, and computational effort. Therefore, it is possible to provide an overall strategy for uncertainty-aware models in the SciML field. The methodology is validated through a case study developing a soft sensor for a polymerization reactor. The first step is to build the nonlinear model parameter probability distribution (PDF) by Bayesian inference. The second step is to obtain the machine learning model uncertainty by Monte Carlo simulations. In the first step, a PDF with 30,000 samples is built. In the second step, the uncertainty of the machine learning model is evaluated by sampling 10,000 values through Monte Carlo simulation. The results demonstrate that the identified soft sensors are robust to uncertainties, corroborating the consistency of the proposed approach. © 2022 by the authors.",
		"archive": "Scopus",
		"container-title": "Mathematics",
		"DOI": "10.3390/math11010074",
		"issue": "1",
		"title": "A Robust Learning Methodology for Uncertainty-Aware Scientific Machine Learning Models",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145902502&doi=10.3390%2fmath11010074&partnerID=40&md5=f0ff60278dc34ab43dc423f9f5cb51d3",
		"volume": "11",
		"author": [
			{
				"family": "Costa",
				"given": "E.A."
			},
			{
				"family": "Rebello",
				"given": "C.D.M."
			},
			{
				"family": "Fontana",
				"given": "M."
			},
			{
				"family": "Schnitman",
				"given": "L."
			},
			{
				"family": "Nogueira",
				"given": "I.B.D.R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "henrikssonOutofDistributionDetectionSupport2023",
		"type": "paper-conference",
		"abstract": "[Context and Motivation] The automotive industry is moving towards increased automation, where features such as automated driving systems typically include machine learning (ML), e.g. in the perception system. [Question/Problem] Ensuring safety for systems partly relying on ML is challenging. Different approaches and frameworks have been proposed, typically where the developer must define quantitative and/or qualitative acceptance criteria, and ensure the criteria are fulfilled using different methods to improve e.g., design, robustness and error detection. However, there is still a knowledge gap between quality methods and metrics employed in the ML domain and how such methods can contribute to satisfying the vehicle level safety requirements. [Principal Ideas/Results] In this paper, we argue the need for connecting available ML quality methods and metrics to the safety lifecycle and explicitly show their contribution to safety. In particular, we analyse Out-of-Distribution (OoD) detection, e.g., the frequency of novelty detection, and show its potential for multiple safety-related purposes. I.e., as (a) an acceptance criterion contributing to the decision if the software fulfills the safety requirements and hence is ready-for-release, (b) in operational design domain selection and expansion by including novelty samples into the training/development loop, and (c) as a run-time measure, e.g., if there is a sequence of novel samples, the vehicle should consider reaching a minimal risk condition. [Contribution] This paper describes the possibility to use OoD detection as a safety measure, and the potential contributions in different stages of the safety lifecycle. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-031-29786-1_16",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "233-242",
		"title": "Out-of-Distribution Detection as Support for Autonomous Driving Safety Lifecycle",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152531710&doi=10.1007%2f978-3-031-29786-1_16&partnerID=40&md5=a7d664a89d78ed62ff983c2f646834a8",
		"volume": "13975 LNCS",
		"author": [
			{
				"family": "Henriksson",
				"given": "J."
			},
			{
				"family": "Ursing",
				"given": "S."
			},
			{
				"family": "Erdogan",
				"given": "M."
			},
			{
				"family": "Warg",
				"given": "F."
			},
			{
				"family": "Thorsén",
				"given": "A."
			},
			{
				"family": "Jaxing",
				"given": "J."
			},
			{
				"family": "Örsmark",
				"given": "O."
			},
			{
				"family": "Toftås",
				"given": "M.Ö."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wangDataBanzhafRobust2023",
		"type": "paper-conference",
		"abstract": "Data valuation has wide use cases in machine learning, including improving data quality and creating economic incentives for data sharing. This paper studies the robustness of data valuation to noisy model performance scores. Particularly, we find that the inherent randomness of the widely used stochastic gradient descent can cause existing data value notions (e.g., the Shapley value and the Leave-one-out error) to produce inconsistent data value rankings across different runs. To address this challenge, we introduce the concept of safety margin, which measures the robustness of a data value notion. We show that the Banzhaf value, a famous value notion that originated from cooperative game theory literature, achieves the largest safety margin among all semivalues (a class of value notions that satisfy crucial properties entailed by ML applications and include the famous Shapley value and Leave-one-out error). We propose an algorithm to efficiently estimate the Banzhaf value based on the Maximum Sample Reuse (MSR) principle. Our evaluation demonstrates that the Banzhaf value outperforms the existing semivalue-based data value notions on several ML tasks such as learning with weighted samples and noisy label detection. Overall, our study suggests that when the underlying ML algorithm is stochastic, the Banzhaf value is a promising alternative to the other semivalue-based data value schemes given its computational advantage and ability to robustly differentiate data quality. Copyright © 2023 by the author(s)",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "6388-6421",
		"title": "Data Banzhaf: A Robust Data Valuation Framework for Machine Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165146592&partnerID=40&md5=a821d12295cd52facd652e9c94031f07",
		"volume": "206",
		"author": [
			{
				"family": "Wang",
				"given": "J.T."
			},
			{
				"family": "Jia",
				"given": "R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "luRobustVerifiablePrivacy2023",
		"type": "article-journal",
		"abstract": "Federated Learning (FL) safeguards user privacy by uploading gradients instead of raw data. However, inference attacks can reconstruct raw data using gradients uploaded by users in FL. To mitigate this issue, researchers have combined privacy computing techniques with FL. However, these tech-niques may not ensure the Byzantine robustness of aggregation or the integrity of the aggregated outcomes. Most current robust privacy FL methods assess differences between gradients and benchmarks in the direction, allowing adversaries to poison the aggregation against the magnitude. Furthermore, these methods cannot ensure the integrity of the aggregation results. To over-come these challenges, this study proposes a novel algorithm, Robust and Verifiable Privacy Federated Learning(RVPFL), which can more effectively eliminate the poisoning attack of the opponent by measuring the direction and magnitude of the gradient in the ciphertext state. The proposed algorithm guarantees the integrity of server aggregation results while safe-guarding user privacy. In this study, comprehensive theoretical analysis and experimental validation of RVPFL are conducted to demonstrate its superiority. The proposed RVPFL algorithm solves the Byzantine robustness problem of aggregation and the integrity problem of aggregation results, which helps to research and develop more robust and effective privacy-preserving federal learning techniques. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Artificial Intelligence",
		"DOI": "10.1109/TAI.2023.3309273",
		"page": "1-14",
		"title": "Robust and verifiable privacy federated learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169701947&doi=10.1109%2fTAI.2023.3309273&partnerID=40&md5=8dc0334ff83737a2d178d275de67ec8a",
		"author": [
			{
				"family": "Lu",
				"given": "Z."
			},
			{
				"family": "Lu",
				"given": "S."
			},
			{
				"family": "Tang",
				"given": "X."
			},
			{
				"family": "Wu",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wangJointAdversarialDomain2023",
		"type": "article-journal",
		"abstract": "Generative adversarial networks as a powerful technique is also used in domain adaptation (DA) problem. Existing adversarial DA methods mainly conduct domain-wise alignment to alleviate marginal distribution shift between the two domains, while it may damage latent discriminative structure hidden in data feature space and cause negative transfer accordingly. To handle this problem, we propose a joint adversarial domain adaptation method with structural graph alignment to minimize joint distribution bias by further realizing class-wise matching (conditional distribution shift) based on a simple sampling strategy except for the domain-wise alignment, and validate that simultaneously considering these two types of shift can approximately reduce the joint distribution bias. To explore transferable structural information and realize more sufficient transfer for DA problem, we propose to align structural graphs between the two domains which is also based on a simple sampling strategy. Notably, the structural graph describes the relationship between each two samples and it is computed on two domains. As such, we can learn new feature representation of the two domains which are more discriminative and transferable to benefit a cross-domain classification task desirably. Finally, we design a number of experiments to evaluate our approach on four public cross-domain benchmark datasets including standard and large-scale ones, and empirical results show that the proposed model can outperform compared state-of-the-art methods. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Network Science and Engineering",
		"DOI": "10.1109/TNSE.2023.3302574",
		"page": "1-10",
		"title": "Joint Adversarial Domain Adaptation With Structural Graph Alignment",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167778383&doi=10.1109%2fTNSE.2023.3302574&partnerID=40&md5=b1d6a78515a4f98172a249841a67ea69",
		"author": [
			{
				"family": "Wang",
				"given": "M."
			},
			{
				"family": "Chen",
				"given": "J."
			},
			{
				"family": "Wang",
				"given": "Y."
			},
			{
				"family": "Wang",
				"given": "S."
			},
			{
				"family": "li",
				"given": "L."
			},
			{
				"family": "Su",
				"given": "H."
			},
			{
				"family": "Gong",
				"given": "Z."
			},
			{
				"family": "Wu",
				"given": "K."
			},
			{
				"family": "Chen",
				"given": "Z."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "samarasingheCounterfactualLearningEnhancing2023",
		"type": "article-journal",
		"abstract": "Resilience in autonomous agent systems is about having the capacity to anticipate, respond to, adapt to, and recover from adverse and dynamic conditions in complex environments. It is associated with the intelligence possessed by the agents to preserve the functionality or to minimize the impact on functionality through a transformation, reconfiguration, or expansion performed across the system. Enhancing the resilience of systems could pave way toward higher autonomy allowing them to tackle intricate dynamic problems. The state-of-the-art systems have mostly focussed on improving the redundancy of the system, adopting decentralized control architectures, and utilizing distributed sensing capabilities. While machine learning approaches for efficient distribution and allocation of skills and tasks have enhanced the potential of these systems, they are still limited when presented with dynamic environments. To move beyond the current limitations, this paper advocates incorporating counterfactual learning models for agents to enable them with the ability to predict possible future conditions and adjust their behavior. Counterfactual learning is a topic that has recently been gaining attention as a model-agnostic and post-hoc technique to improve explainability in machine learning models. Using counterfactual causality can also help gain insights into unforeseen circumstances and make inferences about the probability of desired outcomes. We propose that this can be used in agent systems as a means to guide and prepare them to cope with unanticipated environmental conditions. This supplementary support for adaptation can enable the design of more intelligent and complex autonomous agent systems to address the multifaceted characteristics of real-world problem domains. Copyright © 2023 Samarasinghe.",
		"archive": "Scopus",
		"container-title": "Frontiers in Artificial Intelligence",
		"DOI": "10.3389/frai.2023.1212336",
		"title": "Counterfactual learning in enhancing resilience in autonomous agent systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167886649&doi=10.3389%2ffrai.2023.1212336&partnerID=40&md5=b8c838d98f3d1ffd5dc99ec1d8cba075",
		"volume": "6",
		"author": [
			{
				"family": "Samarasinghe",
				"given": "D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "liuNovelCompositeGraph2023",
		"type": "article-journal",
		"abstract": "Graph neural networks (GNNs) have achieved great success in many fields due to their powerful capabilities of processing graph-structured data. However, most GNNs can only be applied to scenarios where graphs are known, but real-world data are often noisy or even do not have available graph structures. Recently, graph learning has attracted increasing attention in dealing with these problems. In this article, we develop a novel approach to improving the robustness of the GNNs, called composite GNN. Different from existing methods, our method uses composite graphs (C-graphs) to characterize both sample and feature relations. The C-graph is a unified graph that unifies these two kinds of relations, where edges between samples represent sample similarities, and each sample has a tree-based feature graph to model feature importance and combination preference. By jointly learning multiaspect C-graphs and neural network parameters, our method improves the performance of semisupervised node classification and ensures robustness. We conduct a series of experiments to evaluate the performance of our method and the variants of our method that only learn sample relations or feature relations. Extensive experimental results on nine benchmark datasets demonstrate that our proposed method achieves the best performance on almost all the datasets and is robust to feature noises. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Neural Networks and Learning Systems",
		"DOI": "10.1109/TNNLS.2023.3268766",
		"page": "1-15",
		"title": "A Novel Composite Graph Neural Network",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160246263&doi=10.1109%2fTNNLS.2023.3268766&partnerID=40&md5=a81f79fc707d9e9221429e3840d4e9b2",
		"author": [
			{
				"family": "Liu",
				"given": "Z."
			},
			{
				"family": "Yang",
				"given": "J."
			},
			{
				"family": "Zhong",
				"given": "X."
			},
			{
				"family": "Wang",
				"given": "W."
			},
			{
				"family": "Chen",
				"given": "H."
			},
			{
				"family": "Chang",
				"given": "Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gittensAdversarialPerspectiveAccuracy2022",
		"type": "article-journal",
		"abstract": "Model accuracy is the traditional metric employed in machine learning (ML) applications. However, privacy, fairness, and robustness guarantees are crucial as ML algorithms increasingly pervade our lives and play central roles in socially important systems. These four desiderata constitute the pillars of Trustworthy ML (TML) and may mutually inhibit or reinforce each other. It is necessary to understand and clearly delineate the trade-offs among these desiderata in the presence of adversarial attacks. However, threat models for the desiderata are different and the defenses introduced for each leads to further trade-offs in a multilateral adversarial setting (i.e., a setting attacking several pillars simultaneously). The first half of the paper reviews the state of the art in TML research, articulates known multilateral trade-offs, and identifies open problems and challenges in the presence of an adversary that may take advantage of such multilateral trade-offs. The fundamental shortcomings of statistical association-based TML are discussed, to motivate the use of causal methods to achieve TML. The second half of the paper, in turn, advocates the use of causal modeling in TML. Evidence is collected from across the literature that causal ML is well-suited to provide a unified approach to TML. Causal discovery and causal representation learning are introduced as essential stages of causal modeling, and a new threat model for causal ML is introduced to quantify the vulnerabilities introduced through the use of causal methods. The paper concludes with pointers to possible next steps in the development of a causal TML pipeline. © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2022.3218715",
		"page": "120850-120865",
		"title": "An Adversarial Perspective on Accuracy, Robustness, Fairness, and Privacy: Multilateral-Tradeoffs in Trustworthy ML",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141622799&doi=10.1109%2fACCESS.2022.3218715&partnerID=40&md5=9035238f9011623885d10467067c26b9",
		"volume": "10",
		"author": [
			{
				"family": "Gittens",
				"given": "A."
			},
			{
				"family": "Yener",
				"given": "B."
			},
			{
				"family": "Yung",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "peralesgomezMethodologyEvaluatingRobustness2022",
		"type": "article-journal",
		"abstract": "Anomaly Detection systems based on Machine and Deep learning are the most promising solutions to detect cyberattacks in the industry. However, these techniques are vulnerable to adversarial attacks that downgrade prediction performance. Several techniques have been proposed to measure the robustness of Anomaly Detection in the literature. However, they do not consider that, although a small perturbation in an anomalous sample belonging to an attack, i.e., Denial of Service, could cause it to be misclassified as normal while retaining its ability to damage, an excessive perturbation might also transform it into a truly normal sample, with no real impact on the industrial system. This paper presents a methodology to calculate the robustness of Anomaly Detection models in industrial scenarios. The methodology comprises four steps and uses a set of additional models called support models to determine if an adversarial sample remains anomalous. We carried out the validation using the Tennessee Eastman process, a simulated testbed of a chemical process. In such a scenario, we applied the methodology to both a Long-Short Term Memory (LSTM) neural network and 1-dimensional Convolutional Neural Network (1D-CNN) focused on detecting anomalies produced by different cyberattacks. The experiments showed that 1D-CNN is significantly more robust than LSTM for our testbed. Specifically, a perturbation of 60% (empirical robustness of 0.6) of the original sample is needed to generate adversarial samples for LSTM, whereas in 1D-CNN the perturbation required increases up to 111% (empirical robustness of 1.11).  © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2022.3224930",
		"page": "124582-124594",
		"title": "A Methodology for Evaluating the Robustness of Anomaly Detectors to Adversarial Attacks in Industrial Scenarios",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144075247&doi=10.1109%2fACCESS.2022.3224930&partnerID=40&md5=6d19be1d6aa20ba9d211e1995bbbca85",
		"volume": "10",
		"author": [
			{
				"family": "Perales Gomez",
				"given": "A.L."
			},
			{
				"family": "Maimo",
				"given": "L.F."
			},
			{
				"family": "Clemente",
				"given": "F.J.G."
			},
			{
				"family": "Morales",
				"given": "J.A.M."
			},
			{
				"family": "Celdran",
				"given": "A.H."
			},
			{
				"family": "Bovet",
				"given": "G."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hiettMeaningfulMachineLearning2022",
		"type": "paper-conference",
		"abstract": "Applied research presented in this paper describes an approach to provide meaningful evaluation of the Machine Learning (ML) components in a Full Motion Video (FMV) Machine Learning Enabled System (MLES). The MLES itself is not discussed in the paper. We focus on the experimental activity that has been designed to provide confidence that the MLES, when fielded under dynamic and uncertain conditions, performance will not be undermined by a lack of ML robustness. For example, to real-world changes of the same scene under differing light conditions. The paper details the technical approach and how it is applied to data, across the overall experimental pipeline, consisting of a perturbation engine, test pipeline and metric production. Data is from a small imagery dataset and the results are shown and discussed as part of a proof of concept study. © 2022 SPIE.",
		"archive": "Scopus",
		"DOI": "10.1117/12.2638492",
		"event-title": "Proceedings of SPIE - The International Society for Optical Engineering",
		"title": "Meaningful Machine Learning Robustness Evaluation in Real-World Machine Learning Enabled System Contexts",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145198838&doi=10.1117%2f12.2638492&partnerID=40&md5=0fe9bf39ca17fb32f5989749b1568d7c",
		"volume": "12276",
		"author": [
			{
				"family": "Hiett",
				"given": "B."
			},
			{
				"family": "Boyd",
				"given": "P."
			},
			{
				"family": "Fletcher",
				"given": "C."
			},
			{
				"family": "Gowland",
				"given": "S."
			},
			{
				"family": "Sharp",
				"given": "J.H."
			},
			{
				"family": "Sloggett",
				"given": "D."
			},
			{
				"family": "Banks",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "qiHierarchicalHAZOPLikeSafety2022",
		"type": "paper-conference",
		"abstract": "Hazard and Operability Analysis (HAZOP) is a powerful safety analysis technique with a long history in industrial process control domain. With the increasing use of Machine Learning (ML) components in cyber physical systems—so called Learning-Enabled Systems (LESs), there is a recent trend of applying HAZOP-like analysis to LESs. While it shows a great potential to reserve the capability of doing sufficient and systematic safety analysis, there are new technical challenges raised by the novel characteristics of ML that require retrofit of the conventional HAZOP technique. In this regard, we present a new Hierarchical HAZOP-Like method for LESs (HILLS). To deal with the complexity of LESs, HILLS first does “divide and conquer” by stratifying the whole system into three levels, and then proceeds HAZOP on each level to identify (latent-)hazards, causes, security threats and mitigation (with new nodes and guide words). Finally, HILLS attempts at linking and propagating the causal relationship among those identified elements within and across the three levels via both qualitative and quantitative methods. We examine and illustrate the utility of HILLS by a case study on Autonomous Underwater Vehicles, with discussions on assumptions and extensions to real-world applications. HILLS, as a first HAZOP-like attempt on LESs that explicitly considers ML internal behaviours and its interactions with other components, not only uncovers the inherent difficulties of doing safety analysis for LESs, but also demonstrates a good potential to tackle them. © 2022 Copyright for this paper by its authors.",
		"archive": "Scopus",
		"event-title": "CEUR Workshop Proceedings",
		"title": "A Hierarchical HAZOP-Like Safety Analysis for Learning-Enabled Systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139381061&partnerID=40&md5=80164f44c1c28764025cd0045f7ff701",
		"volume": "3215",
		"author": [
			{
				"family": "Qi",
				"given": "Y."
			},
			{
				"family": "Conmy",
				"given": "P.R."
			},
			{
				"family": "Huang",
				"given": "W."
			},
			{
				"family": "Zhao",
				"given": "X."
			},
			{
				"family": "Huang",
				"given": "X."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rodriguez-sotoBuildingMultiAgentEnvironments2022",
		"type": "paper-conference",
		"abstract": "This paper tackles the open problem of value alignment in multi-agent systems. In particular, we propose an approach to build an ethical environment that guarantees that all agents in the system learn to behave ethically while pursuing their individual objectives. Our contributions are founded in the framework of Multi-Objective Multi-Agent Reinforcement Learning. Firstly, we characterise a family of Multi-Objective Markov Games (MOMGs), the so-called ethical MOMGs, for which we can formally guarantee the learning of ethical behaviours. From these, we specify the process for building single-objective ethical environments that simplify the learning in the multi-agent system. Interestingly, our theoretical results for multi-agent environments generalise recent state-of-the-art results for single-agent environments. © 2022 ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022. All rights reserved.",
		"archive": "Scopus",
		"event-title": "ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022",
		"title": "Building Multi-Agent Environments with Theoretical Guarantees on the Learning of Ethical Policies",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173584282&partnerID=40&md5=b592333309eaa9bbf060db65eaa1e353",
		"author": [
			{
				"family": "Rodriguez-Soto",
				"given": "M."
			},
			{
				"family": "Rodriguez-Aguilar",
				"given": "J.A."
			},
			{
				"family": "Lopez-Sanchez",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wangAdversarialRobustnessDeep2022",
		"type": "article-journal",
		"abstract": "Adversarial attacks, e.g., adversarial perturbations of the input and adversarial samples, pose significant challenges to machine learning and deep learning techniques, including interactive recommendation systems. The latent embedding space of those techniques makes adversarial attacks challenging to detect at an early stage. Recent advance in causality shows that counterfactual can also be considered one of the ways to generate the adversarial samples drawn from different distribution as the training samples. We propose to explore adversarial examples and attack agnostic detection on reinforcement learning (RL)-based interactive recommendation systems. We first craft different types of adversarial examples by adding perturbations to the input and intervening on the casual factors. Then, we augment recommendation systems by detecting potential attacks with a deep learning-based classifier based on the crafted data. Finally, we study the attack strength and frequency of adversarial examples and evaluate our model on standard datasets with multiple crafting methods. Our extensive experiments show that most adversarial attacks are effective, and both attack strength and attack frequency impact the attack performance. The strategically-timed attack achieves comparative attack performance with only 1/3 to 1/2 attack frequency. Besides, our white-box detector trained with one crafting method has the generalization ability over several other crafting methods. Copyright © 2022 Wang, Cao, Chen, Yao, Wang and Sheng.",
		"archive": "Scopus",
		"container-title": "Frontiers in Big Data",
		"DOI": "10.3389/fdata.2022.822783",
		"title": "Adversarial Robustness of Deep Reinforcement Learning Based Dynamic Recommender Systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132621553&doi=10.3389%2ffdata.2022.822783&partnerID=40&md5=eaceb56da44a54961dfc1c1089624dea",
		"volume": "5",
		"author": [
			{
				"family": "Wang",
				"given": "S."
			},
			{
				"family": "Cao",
				"given": "Y."
			},
			{
				"family": "Chen",
				"given": "X."
			},
			{
				"family": "Yao",
				"given": "L."
			},
			{
				"family": "Wang",
				"given": "X."
			},
			{
				"family": "Sheng",
				"given": "Q.Z."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ebrahimiAdversarialReinforcementLearning2022",
		"type": "paper-conference",
		"abstract": "Empowered by the recent development in Ma-chine Learning (ML), signatureless ML-based malware detectors present promising performance in identifying unseen mal ware variants and zero days without requiring expensive dynamic malware analysis. However, it has been recently shown that ML-based malware detectors are vulnerable to adversarial malware attacks, in which an attacker modifies a known malware exe-cutable to trick the malware detector into recognizing the modi-fied variant as benign. Adversarial malware example generation has become an emerging area in adversarial ML that studies creating functionality-preserving adversarial malware variants. Advancements in this area have led to an eternal game between the adversary and defender. While the area has attracted much attention in the security community, a large body of these studies merely focuses on attack methods against ML-based malware detectors. There has been little work on understanding how these adversarial variants can be systematically used by the defender to strengthen the robustness of these detectors and stand ahead of the adversary. Latest efforts have led to emergence of adversarial learning. In this work, we propose a simple wargame approach to empirically conduct the adversarial minimax optimization underlying in the adversarial learning for improving the robustness of ML-based malware detectors. Our proposed approach employs adversarial malware variants generated from a reinforcement learning-based adversarial attack policy in a minimax game alternating between strengthening the attack policy and improving the detectors' robustness. We evaluated the effectiveness of our approach on a testbed with 33.2 GB working malware collected from VirusTotal. Despite the sub-optimal nature of our method, it was able to surprisingly enhance the robustness of three known open-source ML-based malware detectors (LGBM, MalConv, and NonNeg) against the adversarial malware variants by 4, 7, and 11 times, respectively.  © 2022 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ICDMW58026.2022.00079",
		"event-title": "IEEE International Conference on Data Mining Workshops, ICDMW",
		"page": "567-576",
		"title": "An Adversarial Reinforcement Learning Framework for Robust Machine Learning-based Malware Detection",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148436341&doi=10.1109%2fICDMW58026.2022.00079&partnerID=40&md5=fa5afef0860ddc17f4b63e0796529524",
		"volume": "2022-November",
		"author": [
			{
				"family": "Ebrahimi",
				"given": "M.R."
			},
			{
				"family": "Li",
				"given": "W."
			},
			{
				"family": "Chai",
				"given": "Y."
			},
			{
				"family": "Pacheco",
				"given": "J."
			},
			{
				"family": "Chen",
				"given": "H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "xiongRobustTrustworthyMachine2022",
		"type": "article-journal",
		"abstract": "While Machine Learning (ML) technologies are widely adopted in many mission critical fields to support intelligent decision-making, concerns remain about system resilience against ML-specific security attacks and privacy breaches as well as the trust that users have in these systems. In this article, we present our recent systematic and comprehensive survey on the state-of-the-art ML robustness and trustworthiness from a security engineering perspective, focusing on the problems in system threat analysis, design and evaluation faced in developing practical machine learning applications, in terms of robustness and user trust. Accordingly, we organize the presentation of this survey intended to facilitate the convey of the body of knowledge from this angle. We then describe a metamodel we created that represents the body of knowledge in a standard and visualized way. We further illustrate how to leverage the metamodel to guide a systematic threat analysis and security design process which extends and scales up the classic process. Finally, we propose the future research directions motivated by our findings. Our work differs itself from the existing surveys by (i) exploring the fundamental principles and best practices to support robust and trustworthy ML system development, and (ii) studying the interplay of robustness and user trust in the context of ML systems. We expect this survey provides a big picture for machine learning security practitioners. © 2022",
		"archive": "Scopus",
		"container-title": "Journal of Information Security and Applications",
		"DOI": "10.1016/j.jisa.2022.103121",
		"title": "Towards a robust and trustworthy machine learning system development: An engineering perspective",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124314035&doi=10.1016%2fj.jisa.2022.103121&partnerID=40&md5=fa8bf24ca832c86ff0ff8108655259bf",
		"volume": "65",
		"author": [
			{
				"family": "Xiong",
				"given": "P."
			},
			{
				"family": "Buffett",
				"given": "S."
			},
			{
				"family": "Iqbal",
				"given": "S."
			},
			{
				"family": "Lamontagne",
				"given": "P."
			},
			{
				"family": "Mamun",
				"given": "M."
			},
			{
				"family": "Molyneaux",
				"given": "H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "weiModelSelectionApproach2022",
		"type": "paper-conference",
		"abstract": "We develop a model selection approach to tackle reinforcement learning with adversarial corruption in both transition and reward. For finite-horizon tabular MDPs, without prior knowledge on the total amount of corruption, our algorithm achieves a regret bound of (Equation presented) where T is the number of episodes, C is the total amount of corruption, and ∆ is the reward gap between the best and the second-best policy. This is the first worst-case optimal bound achieved without knowledge of C, improving previous results of Lykouris et al. (2021); Chen et al. (2021b); Wu et al. (2021). For finite-horizon linear MDPs, we develop a computationally efficient algorithm with a regret bound of Oe(p(1 + C)T), and another computationally inefficient one with Oe(√T + C), improving the result of Lykouris et al. (2021) and answering an open question by Zhang et al. (2021b). Finally, our model selection framework can be easily applied to other settings including linear bandits, linear contextual bandits, and MDPs with general function approximation, leading to several improved or new results. © 2022 C.-Y. Wei, C. Dann & J. Zimmert.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "1043-1096",
		"title": "A Model Selection Approach for Corruption Robust Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163731154&partnerID=40&md5=be608eb3822f8bbe3c87072cb3e0d7fd",
		"volume": "167",
		"author": [
			{
				"family": "Wei",
				"given": "C.-Y."
			},
			{
				"family": "Dann",
				"given": "C."
			},
			{
				"family": "Zimmert",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "padakandlaDataEfficientSafe2022",
		"type": "paper-conference",
		"abstract": "Applying reinforcement learning (RL) methods for real world applications poses multiple challenges - the foremost being safety of the physical system controlled by the learning agent and the learning efficiency. A RL agent learns to control a system by exploring available actions. In some operating states, when the RL agent exercises an exploratory action, the system may enter unsafe operation, which can lead to safety hazards both for the system as well as for humans supervising the system. RL algorithms thus need to respect these safety constraints and must do so with limited available information. In our work, we formulate this problem in the constrained off-policy setting that facilitates safe exploration by the RL agent. Further, we develop a sample efficient algorithm by adapting the cross-entropy method. The proposed algorithm’s safety performance is evaluated numerically on benchmark RL problems. © 2022 ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022. All rights reserved.",
		"archive": "Scopus",
		"event-title": "ALA 2022 - Adaptive and Learning Agents Workshop at AAMAS 2022",
		"title": "Data Efficient Safe Reinforcement Learning Algorithm",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173586002&partnerID=40&md5=360c0b6fe4e7ad5c29d0651567b520d8",
		"author": [
			{
				"family": "Padakandla",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhaRobustDoubleparallelExtreme2022",
		"type": "article-journal",
		"abstract": "To solve the problem of improving the regression accuracy and model stability of the extreme learning machine(ELM), a new approach based on an improved M-estimation optimized double-parallel extreme learning machine is proposed in this study, namely robust double-parallel extreme learning machine(RD-ELM). Firstly, RD-ELM is constructed with a double parallel forward structure, thus the information can be received from both hidden layer neurons and input layer neurons. Secondly, we use an improved M-estimation to calculate output weights of neural network by iteratively reweighted Least-Squares Estimation(LSE), with weights assigned by the least absolute residual estimation of the samples. Finally, we establish a regression prediction model utilized to test the goodness of fit in a SinC function and verify the regression ability in eight benchmark regression problems. Then the proposed method is applied to an actual operational condition of a power plant. Experimental results show that the proposed method can efficiently process the influence of outliers and noise with strong anti-jamming ability. Compared with other methods, RD-ELM has superior performance that is stronger robustness and better generalization performance in many benchmark data and practical experiments. © 2022 Elsevier Ltd",
		"archive": "Scopus",
		"container-title": "Advanced Engineering Informatics",
		"DOI": "10.1016/j.aei.2022.101606",
		"title": "A robust double-parallel extreme learning machine based on an improved M-estimation algorithm",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128187488&doi=10.1016%2fj.aei.2022.101606&partnerID=40&md5=a4f5f40de5633708b4f9407e58292c83",
		"volume": "52",
		"author": [
			{
				"family": "Zha",
				"given": "L."
			},
			{
				"family": "Ma",
				"given": "K."
			},
			{
				"family": "Li",
				"given": "G."
			},
			{
				"family": "Fang",
				"given": "Q."
			},
			{
				"family": "Hu",
				"given": "X."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liAssessingEnhancingAdversarial2022",
		"type": "article-journal",
		"abstract": "As predictive analytics increasingly applies supervised machine learning (SML) models to inform mission-critical decision-making, adversaries become incentivized to exploit the vulnerabilities of these SML models and mislead predictive analytics into erroneous decisions. Due to the limited understanding and awareness of such adversarial attacks, the predictive analytics knowledge and deployment need a principled technique for adversarial robustness assessment and enhancement. In this research, we leverage the technology threat avoidance theory as the kernel theory and propose a research framework for assessing and enhancing the adversarial robustness of predictive analytics applications. We instantiate the proposed framework by developing a robust text classification system, the ARText system. The proposed system is rigorously evaluated in comparison with benchmark methods on two tasks extensively enabled by SML: spam review detection and spam email detection, which then confirmed the utility and effectiveness of our ARText system. Results from numerous experiments revealed that our proposed framework could significantly enhance the adversarial robustness of predictive analytics applications. © 2022 Taylor & Francis Group, LLC.",
		"archive": "Scopus",
		"container-title": "Journal of Management Information Systems",
		"DOI": "10.1080/07421222.2022.2063549",
		"issue": "2",
		"page": "542-572",
		"title": "Assessing and Enhancing Adversarial Robustness of Predictive Analytics: An Empirically Tested Design Framework",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131375989&doi=10.1080%2f07421222.2022.2063549&partnerID=40&md5=03a5b535d2a12af5782faf5ad9d9b1a6",
		"volume": "39",
		"author": [
			{
				"family": "Li",
				"given": "W."
			},
			{
				"family": "Chai",
				"given": "Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liAdaptiveInterleavedReinforcement2022",
		"type": "article-journal",
		"abstract": "This article investigates adaptive robust controller design for discrete-time (DT) affine nonlinear systems using an adaptive dynamic programming. A novel adaptive interleaved reinforcement learning algorithm is developed for finding a robust controller of DT affine nonlinear systems subject to matched or unmatched uncertainties. To this end, the robust control problem is converted into the optimal control problem for nominal systems by selecting an appropriate utility function. The performance evaluation and control policy update combined with neural networks approximation are alternately implemented at each time step for solving a simplified Hamilton-Jacobi-Bellman (HJB) equation such that the uniformly ultimately bounded (UUB) stability of DT affine nonlinear systems can be guaranteed, allowing for all realization of unknown bounded uncertainties. The rigorously theoretical proofs of convergence of the proposed interleaved RL algorithm and UUB stability of uncertain systems are provided. Simulation results are given to verify the effectiveness of the proposed method.  © 2012 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Neural Networks and Learning Systems",
		"DOI": "10.1109/TNNLS.2020.3027653",
		"issue": "1",
		"page": "270-280",
		"title": "Adaptive Interleaved Reinforcement Learning: Robust Stability of Affine Nonlinear Systems with Unknown Uncertainty",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122590061&doi=10.1109%2fTNNLS.2020.3027653&partnerID=40&md5=0519b95993dc7072c4748d7a8416a5be",
		"volume": "33",
		"author": [
			{
				"family": "Li",
				"given": "J."
			},
			{
				"family": "Ding",
				"given": "J."
			},
			{
				"family": "Chai",
				"given": "T."
			},
			{
				"family": "Lewis",
				"given": "F.L."
			},
			{
				"family": "Jagannathan",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wangDirichletProcessMixture2022",
		"type": "article-journal",
		"abstract": "While reinforcement learning (RL) algorithms are achieving state-of-the-art performance in various challenging tasks, they can easily encounter catastrophic forgetting or interference when faced with lifelong streaming information. In this article, we propose a scalable lifelong RL method that dynamically expands the network capacity to accommodate new knowledge while preventing past memories from being perturbed. We use a Dirichlet process mixture to model the nonstationary task distribution, which captures task relatedness by estimating the likelihood of task-to-cluster assignments and clusters the task models in a latent space. We formulate the prior distribution of the mixture as a Chinese restaurant process (CRP) that instantiates new mixture components as needed. The update and expansion of the mixture are governed by the Bayesian nonparametric framework with an expectation maximization (EM) procedure, which dynamically adapts the model complexity without explicit task boundaries or heuristics. Moreover, we use the domain randomization technique to train robust prior parameters for the initialization of each task model in the mixture; thus, the resulting model can better generalize and adapt to unseen tasks. With extensive experiments conducted on robot navigation and locomotion domains, we show that our method successfully facilitates scalable lifelong RL and outperforms relevant existing methods. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Cybernetics",
		"DOI": "10.1109/TCYB.2022.3170485",
		"page": "1-12",
		"title": "A Dirichlet Process Mixture of Robust Task Models for Scalable Lifelong Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130466921&doi=10.1109%2fTCYB.2022.3170485&partnerID=40&md5=db9ee7534b0697b5af43aa33db661075",
		"author": [
			{
				"family": "Wang",
				"given": "Z."
			},
			{
				"family": "Chen",
				"given": "C."
			},
			{
				"family": "Dong",
				"given": "D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "xiongHiSaRLHierarchicalFramework2022",
		"type": "paper-conference",
		"abstract": "We propose a two-level hierarchical framework for safe reinforcement learning in a complex environment. The high-level part is an adaptive planner, which aims at learning and generating safe and efficient paths for tasks with imperfect map information. The lower-level part contains a learning-based controller and its corresponding neural Lyapunov function, which characterizes the controller's stability property. This learned neural Lyapunov function serves two purposes. First, it will be part of the high-level heuristic for our planning algorithm. Second, it acts as a part of a runtime shield to guard the safety of the whole system. We use a robot navigation example to demonstrate that our framework can operate efficiently and safely in complex environments, even under adversarial attacks. Copyright © 2022 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",
		"archive": "Scopus",
		"event-title": "CEUR Workshop Proceedings",
		"title": "HiSaRL: A Hierarchical Framework for Safe Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125382338&partnerID=40&md5=ac7bb4dc501b17e0342bcc2d1c031aca",
		"volume": "3087",
		"author": [
			{
				"family": "Xiong",
				"given": "Z."
			},
			{
				"family": "Agarwal",
				"given": "I."
			},
			{
				"family": "Jagannathan",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sunFairRobustClassification2022",
		"type": "paper-conference",
		"abstract": "Robustness and fairness are two equally important issues for machine learning systems. Despite the active research on robustness and fairness of ML recently, these efforts focus on either fairness or robustness, but not both. To bridge this gap, in this paper, we design Fair and Robust Classification (FRoC) models that equip the classification models with both fairness and robustness. Meeting both fairness and robustness constraints is not trivial due to the tension between them. The trade-off between fairness, robustness, and model accuracy also introduces additional challenge. To address these challenges, we design two FRoC methods, namely FRoC-PRE that modifies the input data before model training, and FRoC-IN that modifies the model with an adversarial objective function to address both fairness and robustness during training. FRoC-IN is suitable to the settings where the users (e.g., ML service providers) only have the access to the model but not the original data, while FRoC-PRE works for the settings where the users (e.g., data owners) have the access to both data and a surrogate model that may have similar architecture as the target model. Our extensive experiments on real-world datasets demonstrate that both FRoC-IN and FRoC-PRE can achieve both fairness and robustness with insignificant accuracy loss of the target model. © 2022 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/EuroSP53844.2022.00030",
		"event-title": "Proceedings - 7th IEEE European Symposium on Security and Privacy, Euro S and P 2022",
		"page": "356-376",
		"title": "Towards Fair and Robust Classification",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134017587&doi=10.1109%2fEuroSP53844.2022.00030&partnerID=40&md5=671eb3c92941e21c487bcff140b27e8d",
		"author": [
			{
				"family": "Sun",
				"given": "H."
			},
			{
				"family": "Wu",
				"given": "K."
			},
			{
				"family": "Wang",
				"given": "T."
			},
			{
				"family": "Wang",
				"given": "W.H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangRobustOfflineReinforcement2022",
		"type": "paper-conference",
		"abstract": "Offline deep reinforcement learning algorithms are still in developing. Some existing algorithms have shown that it is feasible to learn directly without using environmental interaction under the condition of sufficient datasets. In this paper, we combine an offline reinforcement learning method through behavior regularization with a robust offline reinforcement learning algorithm. Moreover, the algorithm is verified and analyzed with a high-quality but limited dataset. The experimental results show that it is feasible to combine the behavior regularization method with the robust offline reinforcement learning algorithm, to gain better performance under the condition of limited data compared with the baseline algorithms.  © 2022 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/IAICT55358.2022.9887435",
		"event-title": "Proceedings of the 2022 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology, IAICT 2022",
		"page": "150-154",
		"title": "A Robust Offline Reinforcement Learning Algorithm Based on Behavior Regularization Methods",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139201012&doi=10.1109%2fIAICT55358.2022.9887435&partnerID=40&md5=21027466550a155936800e0c9aabbafa",
		"author": [
			{
				"family": "Zhang",
				"given": "Y."
			},
			{
				"family": "Gao",
				"given": "T."
			},
			{
				"family": "Mi",
				"given": "Q."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liRobustSupervisedSubspace2021",
		"type": "article-journal",
		"abstract": "This paper proposes a novel robust supervised subspace learning (RSSL) method for output-relevant prediction and detection against outliers. RSSL learns the robust subspaces by optimizing a joint problem over both the prediction of output and the reconstruction of input. To this end, the learned subspaces/data representations are informative, i.e., they are encapsulated with the critic information related to both the input and output, and thus can benefit the following tasks of output-related modeling and detection. Besides, we separate sparse items from the raw measurements to suppress the effects of outliers. An efficient optimization algorithm is designed to solve the optimization problem of RSSL. We further conduct post orthogonal decomposition upon the subspaces provided by RSSL so that the trimmed subspaces are more suitable for output-related detection. The efficacy of the proposed method is extensively verified by synthesis data and benchmark data. © 2021 Elsevier Ltd",
		"archive": "Scopus",
		"container-title": "Journal of Process Control",
		"DOI": "10.1016/j.jprocont.2021.09.007",
		"page": "184-194",
		"title": "A robust supervised subspace learning approach for output-relevant prediction and detection against outliers",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116036069&doi=10.1016%2fj.jprocont.2021.09.007&partnerID=40&md5=0d74c08e6d482e68ae1a0bf1a20ff3cb",
		"volume": "106",
		"author": [
			{
				"family": "Li",
				"given": "W."
			},
			{
				"family": "Wang",
				"given": "Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "xuLookYouLeap2021",
		"type": "paper-conference",
		"abstract": "Safety has become one of the main challenges of applying deep reinforcement learning to real world systems. Currently, the incorporation of external knowledge such as human oversight is the only means to prevent the agent from visiting the catastrophic state. In this paper, we propose MBHI, a novel framework for safe model-based reinforcement learning, which ensures safety in the state-level and can effectively avoid both”local” and”non-local” catastrophes. An ensemble of supervised learners are trained in MBHI to imitate human blocking decisions. Similar to human decision-making process, MBHI will roll out an imagined trajectory in the dynamics model before executing actions to the environment, and estimate its safety. When the imagination encounters a catastrophe, MBHI will block the current action and use an efficient MPC method to output a safety policy. We evaluate our method on several safety tasks, and the results show that MBHI achieved better performance in terms of sample efficiency and number of catastrophes compared to the baselines. © 2021 Proceedings of Machine Learning Research. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "332-341",
		"title": "Look Before You Leap: Safe Model-Based Reinforcement Learning with Human Intervention",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146865238&partnerID=40&md5=b9693007970623e55035eef698de6e42",
		"volume": "164",
		"author": [
			{
				"family": "Xu",
				"given": "Y."
			},
			{
				"family": "Liu",
				"given": "Z."
			},
			{
				"family": "Duan",
				"given": "G."
			},
			{
				"family": "Zhu",
				"given": "J."
			},
			{
				"family": "Bai",
				"given": "X."
			},
			{
				"family": "Tan",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chenFedEqualDefendingModel2021",
		"type": "paper-conference",
		"abstract": "With the upcoming edge AI, federated learning (FL) is a privacy-preserving framework to meet the General Data Protection Regulation (GDPR). Unfortunately, FL is vulnerable to an up-to-date security threat, model poisoning attacks. By successfully replacing the global model with the targeted poisoned model, malicious end devices can trigger backdoor attacks and manipulate the whole learning process. The traditional researches under a homogeneous environment can ideally exclude the outliers with scarce side-effects on model performance. However, in privacy-preserving FL, each end device possibly owns a few data classes and different amounts of data, forming into a substantial heterogeneous environment where outliers could be malicious or benign. To achieve the system performance and robustness of FL's framework, we should not assertively remove any local model from the global model updating procedure. Therefore, in this paper, we propose a defending strategy called FedEqual to mitigate model poisoning attacks while preserving the learning task's performance without excluding any benign models. The results show that FedEqual outperforms other state-of-the-art baselines under different heterogeneous environments based on reproduced up-to-date model poisoning attacks. © 2021 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/GLOBECOM46510.2021.9685082",
		"event-title": "2021 IEEE Global Communications Conference, GLOBECOM 2021 - Proceedings",
		"title": "FedEqual: Defending Model Poisoning Attacks in Heterogeneous Federated Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127300983&doi=10.1109%2fGLOBECOM46510.2021.9685082&partnerID=40&md5=018f5ed6035aca25fa5d428aab92a59a",
		"author": [
			{
				"family": "Chen",
				"given": "L.-Y."
			},
			{
				"family": "Chiu",
				"given": "T.-C."
			},
			{
				"family": "Pang",
				"given": "A.-C."
			},
			{
				"family": "Cheng",
				"given": "L.-C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "wangRobustAutomatedMachine2021",
		"type": "article-journal",
		"abstract": "Developing a robust deep neural network (DNN) for a specific task is not only time-consuming but also requires lots of experienced human experts. In order to make deep neural networks easier to apply or even take the human experts out of the design of network architecture completely, a growing number of researches focus on robust automated machine learning (AutoML). In this paper, we investigated the robustness problem of AutoML systems based on contractive pseudoinverse learners. In our proposed method, deep neural networks were built with stacked contractive pseudoinverse learners (CPILer). Each CPILer has a Jacobian regularized reconstruction loss function and is trained with pseudoinverse learning algorithm. When sigmoid activation function is adopted in the hidden layer, the graph Laplace regularizer is derived from square Frobenius norm of the Jacobian matrix. This learning scheme not only speeds up the training process dramatically but also reduces the effort of hyperparameter tuning. In addition, the graph Laplace regularization can improve the robustness of the learning systems by reducing the sensibility to noise. An ensemble network architecture consisting of several sub-networks was designed to build the AutoML systems. The architecture hyperparameters of the system were determined in an automated way which could be considered as a data-driven way. The proposed method shown good performance in the experiments in terms of efficiency and accuracy, and outperformed the baseline methods on a series of benchmark data sets. The robustness improvement of our proposed method was also demonstrated in the experiments. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.",
		"archive": "Scopus",
		"container-title": "Cognitive Computation",
		"DOI": "10.1007/s12559-021-09853-6",
		"issue": "3",
		"page": "724-735",
		"title": "A Robust Automated Machine Learning System with Pseudoinverse Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103065280&doi=10.1007%2fs12559-021-09853-6&partnerID=40&md5=e40b73e3515c9d6099b659efc424c45b",
		"volume": "13",
		"author": [
			{
				"family": "Wang",
				"given": "K."
			},
			{
				"family": "Guo",
				"given": "P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "curiCombiningPessimismOptimism2021",
		"type": "paper-conference",
		"abstract": "In real-world tasks, reinforcement learning (RL) agents frequently encounter situations that are not present during training time. To ensure reliable performance, the RL agents need to exhibit robustness against worst-case situations. The robust RL framework addresses this challenge via a worst-case optimization between an agent and an adversary. Previous robust RL algorithms are either sample inefficient, lack robustness guarantees, or do not scale to large problems. We propose the Robust Hallucinated Upper-Confidence RL (RH-UCRL) algorithm to provably solve this problem while attaining near-optimal sample complexity guarantees. RH-UCRL is a model-based reinforcement learning (MBRL) algorithm that effectively distinguishes between epistemic and aleatoric uncertainty, and efficiently explores both the agent and adversary decision spaces during policy learning. We scale RH-UCRL to complex tasks via neural networks ensemble models as well as neural network policies. Experimentally, we demonstrate that RH-UCRL outperforms other robust deep RL algorithms in a variety of adversarial environments. Copyright © 2021 by the author(s)",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "2254-2264",
		"title": "Combining Pessimism with Optimism for Robust and Efficient Model-Based Deep Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127231156&partnerID=40&md5=0803f12a48c9ee753414ffb80af7e9d2",
		"volume": "139",
		"author": [
			{
				"family": "Curi",
				"given": "S."
			},
			{
				"family": "Bogunovic",
				"given": "I."
			},
			{
				"family": "Krause",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "zhangRobustMethodMeasure2021",
		"type": "article-journal",
		"abstract": "Because machine learning has been widely used in various domains, interpreting internal mechanisms and predictive results of models is crucial for further applications of complex machine learning models. However, the interpretability of complex machine learning models on biased data remains a difficult problem. When the important explanatory features of concerned data are highly influenced by contaminated distributions, particularly in risk-sensitive fields, such as self-driving vehicles and healthcare, it is crucial to provide a robust interpretation of complex models for users. The interpretation of complex models is often associated with analyzing model features by measuring feature importance. Therefore, this article proposes a novel method derived from high-dimensional model representation (HDMR) to measure feature importance. The proposed method can provide robust estimation when the input features follow contaminated distributions. Moreover, the method is model-agnostic, which can enhance its ability to compare different interpretations due to its generalizability. Experimental evaluations on artificial models and machine learning models show that the proposed method is more robust than the traditional method based on HDMR. © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2021.3049412",
		"page": "7885-7893",
		"title": "A Robust Method to Measure the Global Feature Importance of Complex Prediction Models",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099261795&doi=10.1109%2fACCESS.2021.3049412&partnerID=40&md5=d2209d9951eb35633613b8bfcbdab3a2",
		"volume": "9",
		"author": [
			{
				"family": "Zhang",
				"given": "X."
			},
			{
				"family": "Wu",
				"given": "L."
			},
			{
				"family": "Li",
				"given": "Z."
			},
			{
				"family": "Liu",
				"given": "H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "pulawskiBDIDojoDevelopingRobust2021",
		"type": "paper-conference",
		"abstract": "The Belief-Desire-Intention (BDI) architecture is a widely-used model for developing multi-agent systems. BDI agents pursue their goals over time using a collection of plan recipes that are programmed by the developers. Thus, traditional BDI agents are limited in dealing with dynamic environments where uncertainties are not known beforehand, such as those introduced by adversarial forces. In this paper, we present the BDI-Dojo framework for developing robust BDI agents by training them using reinforcement learning against similarly learning-equipped adversarial agents. This adversarial training approach empowers BDI agents to become more resilient in uncertain, dynamic environments. © 2021 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ACSOS-C52956.2021.00066",
		"event-title": "Proceedings - 2021 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion, ACSOS-C 2021",
		"page": "257-262",
		"title": "BDI-Dojo: Developing robust BDI agents in evolving adversarial environments",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123394011&doi=10.1109%2fACSOS-C52956.2021.00066&partnerID=40&md5=1d727ac9b754c915d09d08c9493bfe5e",
		"author": [
			{
				"family": "Pulawski",
				"given": "S."
			},
			{
				"family": "Dam",
				"given": "H.K."
			},
			{
				"family": "Ghose",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "fengLearningSafelyApprove2021",
		"type": "paper-conference",
		"abstract": "Machine learning algorithms in healthcare have the potential to continually learn from real-world data generated during healthcare delivery and adapt to dataset shifts. As such, regulatory bodies like the US FDA have begun discussions on how to autonomously approve modifications to algorithms. Current proposals evaluate algorithmic modifications via hypothesis testing and control a definition of online approval error that only applies if the data is stationary over time, which is unlikely in practice. To this end, we investigate designing approval policies for modifications to ML algorithms in the presence of distributional shifts. Our key observation is that the approval policy most efficient at identifying and approving beneficial modifications varies across problem settings. So, rather than selecting a fixed approval policy a priori, we propose learning the best approval policy by searching over a family of approval strategies. We define a family of strategies that range in their level of optimism when approving modifications. To protect against settings where no version of the ML algorithm performs well, this family includes a pessimistic strategy that rescinds approval. We use the exponentially weighted averaging forecaster (EWAF) to learn the most appropriate strategy and derive tighter regret bounds assuming the distributional shifts are bounded. In simulation studies and empirical analyses, we find that wrapping approval strategies within EWAF is a simple yet effective approach to protect against distributional shifts without significantly slowing down approval of beneficial modifications.  © 2021 Owner/Author.",
		"archive": "Scopus",
		"DOI": "10.1145/3450439.3451864",
		"event-title": "ACM CHIL 2021 - Proceedings of the 2021 ACM Conference on Health, Inference, and Learning",
		"page": "164-173",
		"title": "Learning to safely approve updates to machine learning algorithms",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104096120&doi=10.1145%2f3450439.3451864&partnerID=40&md5=742aa4352dc79c83df5bdfb093ff02cb",
		"author": [
			{
				"family": "Feng",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "maAdaptiveRobustLearning2021",
		"type": "article-journal",
		"abstract": "In general, introducing robust distance metrics and loss functions in the learning process can improve the robustness of the algorithms. In this work, we first propose a new robust loss function called adaptive capped Lθε-loss. For different problems, we can choose different loss functions through adaptive parameter θ during the learning process. Secondly, we propose a new robust distance metric induced by correntropy (CIM) that is based on Laplacian kernel. The CIM contains first and higher-order moments from samples. Further, we demonstrate some important and interesting properties of the Lθε-loss and CIM, such as robustness, boundedness, nonconvexity, etc. Finally, we apply the to Lθε-loss and CIM to twin support vector machine (TWSVM) and develop an adaptive robust learning framework, namely adaptive robust twin support vector machine (ARTSVM). The proposed ARTSVM not only inherits the advantages of TWSVM but also improves the robustness of classification problems. A non-convex optimization method, DC (difference of convex functions) programming algorithm (DCA) is used to solve the proposed ARTSVM, and the convergence of the algorithm is proved theoretically. Experiments on multiple datasets show that the proposed ARTSVM is competitive with existing methods. © 2020",
		"archive": "Scopus",
		"container-title": "Knowledge-Based Systems",
		"DOI": "10.1016/j.knosys.2020.106536",
		"title": "Adaptive robust learning framework for twin support vector machine classification",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094912105&doi=10.1016%2fj.knosys.2020.106536&partnerID=40&md5=c95bdc9b36362521439723d98ce958c3",
		"volume": "211",
		"author": [
			{
				"family": "Ma",
				"given": "J."
			},
			{
				"family": "Yang",
				"given": "L."
			},
			{
				"family": "Sun",
				"given": "Q."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "abdelfattahRobustPolicyBootstrapping2020",
		"type": "article-journal",
		"abstract": "Multi-objective Markov decision processes are a special kind of multi-objective optimization problem that involves sequential decision making while satisfying the Markov property of stochastic processes. Multi-objective reinforcement learning methods address this kind of problem by fusing the reinforcement learning paradigm with multi-objective optimization techniques. One major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity in order to evolve a coverage set of policies that can solve the problem. This article introduces a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the proposed algorithm with two state-of-the-art multi-objective reinforcement learning algorithms in stationary and non-stationary environments. Results showed that the proposed algorithm significantly outperforms the existing algorithms in non-stationary environments while achieving comparable results in stationary environments. © The Author(s) 2019.",
		"archive": "Scopus",
		"container-title": "Adaptive Behavior",
		"DOI": "10.1177/1059712319869313",
		"issue": "4",
		"page": "273-292",
		"title": "A robust policy bootstrapping algorithm for multi-objective reinforcement learning in non-stationary environments",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071469173&doi=10.1177%2f1059712319869313&partnerID=40&md5=5430878a3c4684e33716a18eea2a7dfc",
		"volume": "28",
		"author": [
			{
				"family": "Abdelfattah",
				"given": "S."
			},
			{
				"family": "Kasmarik",
				"given": "K."
			},
			{
				"family": "Hu",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kowsherImpactlearningRobustMachine2020",
		"type": "paper-conference",
		"abstract": "The ultimate goal of this research paper is to introduce a robust machine learning algorithm called Impact-Learning, which is being used widely to achieve more advanced results on many machine-learning related challenges. Impact learning is a supervised machine learning algorithm for resolving classification and linear or polynomial regression knowledge from examples. It also contributes to analyzing systems for competitive data. This algorithm is unique for being capable of learning from a competition, which is the impact of independent features. In other words, it is trained by the impacts of the features from the intrinsic rate of natural increase (RNI). The input to the Impact Learning is a training set of numerical data. In this work, we used six datasets related to regressions and classifications as the experiment of the Impact Learning, and the comparison indicates that at outperforms other standard machine learning regressions and classifications algorithms such as Random forest tree, SVM, Naive Bayes, Logistic regression and so forth. © 2020 ACM.",
		"archive": "Scopus",
		"DOI": "10.1145/3411174.3411185",
		"event-title": "ACM International Conference Proceeding Series",
		"page": "9-13",
		"title": "Impact-learning: A robust machine learning algorithm",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089608381&doi=10.1145%2f3411174.3411185&partnerID=40&md5=5aa583b9d3f7591557bcf03a3f1f6a17",
		"author": [
			{
				"family": "Kowsher",
				"given": "M."
			},
			{
				"family": "Tahabilder",
				"given": "A."
			},
			{
				"family": "Murad",
				"given": "S.A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mengDistantlySupervisedNamedEntity2021",
		"type": "paper-conference",
		"abstract": "We study the problem of training named entity recognition (NER) models using only distantly-labeled data, which can be automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. The biggest challenge of distantly-supervised NER is that the distant supervision may induce incomplete and noisy labels, rendering the straightforward application of supervised learning ineffective. In this paper, we propose (1) a noise-robust learning scheme comprised of a new loss function and a noisy label removal step, for training NER models on distantly-labeled data, and (2) a self-training method that uses contextualized augmentations created by pre-trained language models to improve the generalization ability of the NER model. On three benchmark datasets, our method achieves superior performance, outperforming existing distantly-supervised NER models by significant margins. © 2021 Association for Computational Linguistics",
		"archive": "Scopus",
		"event-title": "EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings",
		"page": "10367-10378",
		"title": "Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121748854&partnerID=40&md5=0fb4ea97baff35f25aec98ee2a1ae5de",
		"author": [
			{
				"family": "Meng",
				"given": "Y."
			},
			{
				"family": "Zhang",
				"given": "Y."
			},
			{
				"family": "Huang",
				"given": "J."
			},
			{
				"family": "Wang",
				"given": "X."
			},
			{
				"family": "Zhang",
				"given": "Y."
			},
			{
				"family": "Ji",
				"given": "H."
			},
			{
				"family": "Han",
				"given": "J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "linardInductionFaultTrees2020",
		"type": "paper-conference",
		"abstract": "Cyber-physical systems have increasingly intricate architectures and failure modes, which is due to an explosion of their complexity, size, and failure criticality. While expert knowledge of individual components exists, their interaction is complex. For these reasons, obtaining accurate system reliability models is a hard task. At the same time, systems tend to be continuously monitored via advanced sensor systems. This data describes the components' failure behavior and can be exploited for failure diagnosis and learning of reliability models. This paper presents an effective algorithm for the learning of Fault Trees from data. Fault trees (FTs) are a widespread formalism in reliability engineering. They capture the failure behavior of components and their propagation through an entire system. To that end, we first use machine learning to compute a Bayesian Network (BN) highlighting probabilistic relationships between the failures of components and root causes. Then, we apply a set of rules to translate a BN into an FT, based on the Conditional Probability Tables to decide, amongst others, the nature of gates in the FT. We evaluate our method on synthetic data and a benchmark set of FTs. © 2019 European Safety and Reliability Association. Published by Research Publishing, Singapore.",
		"archive": "Scopus",
		"DOI": "10.3850/978-981-11-2724-3_0596-cd",
		"event-title": "Proceedings of the 29th European Safety and Reliability Conference, ESREL 2019",
		"page": "910-917",
		"title": "Induction of fault trees through Bayesian networks",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089187848&doi=10.3850%2f978-981-11-2724-3_0596-cd&partnerID=40&md5=d522b2243b1a745146c5bc2d04b75084",
		"author": [
			{
				"family": "Linard",
				"given": "A."
			},
			{
				"family": "Bueno",
				"given": "M.L.P."
			},
			{
				"family": "Bucur",
				"given": "D."
			},
			{
				"family": "Stoelinga",
				"given": "M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "xinDecentralizedStochasticOptimization2020",
		"type": "article-journal",
		"abstract": "Decentralized methods to solve finite-sum minimization problems are important in many signal processing and machine learning tasks where the data samples are distributed across a network of nodes, and raw data sharing is not permitted due to privacy and/or resource constraints. In this article, we review decentralized stochastic first-order methods and provide a unified algorithmic framework that combines variance reduction with gradient tracking to achieve robust performance and fast convergence. We provide explicit theoretical guarantees of the corresponding methods when the objective functions are smooth and strongly convex and show their applicability to nonconvex problems via numerical experiments. Throughout the article, we provide intuitive illustrations of the main technical ideas by casting appropriate tradeoffs and comparisons among the methods of interest and by highlighting applications to decentralized training of machine learning models. © 1991-2012 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Signal Processing Magazine",
		"DOI": "10.1109/MSP.2020.2974267",
		"issue": "3",
		"page": "102-113",
		"title": "Decentralized Stochastic Optimization and Machine Learning: A Unified Variance-Reduction Framework for Robust Performance and Fast Convergence",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084602494&doi=10.1109%2fMSP.2020.2974267&partnerID=40&md5=8770fefac4537655d72f8fadd99b637c",
		"volume": "37",
		"author": [
			{
				"family": "Xin",
				"given": "R."
			},
			{
				"family": "Kar",
				"given": "S."
			},
			{
				"family": "Khan",
				"given": "U.A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "cappozzoRobustApproachModelbased2020",
		"type": "article-journal",
		"abstract": "In a standard classification framework a set of trustworthy learning data are employed to build a decision rule, with the final aim of classifying unlabelled units belonging to the test set. Therefore, unreliable labelled observations, namely outliers and data with incorrect labels, can strongly undermine the classifier performance, especially if the training size is small. The present work introduces a robust modification to the Model-Based Classification framework, employing impartial trimming and constraints on the ratio between the maximum and the minimum eigenvalue of the group scatter matrices. The proposed method effectively handles noise presence in both response and exploratory variables, providing reliable classification even when dealing with contaminated datasets. A robust information criterion is proposed for model selection. Experiments on real and simulated data, artificially adulterated, are provided to underline the benefits of the proposed method. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",
		"archive": "Scopus",
		"container-title": "Advances in Data Analysis and Classification",
		"DOI": "10.1007/s11634-019-00371-w",
		"issue": "2",
		"page": "327-354",
		"title": "A robust approach to model-based classification based on trimming and constraints: Semi-supervised learning in presence of outliers and label noise",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070929493&doi=10.1007%2fs11634-019-00371-w&partnerID=40&md5=97940a49ee45db1b0a11aa8ed46b42d8",
		"volume": "14",
		"author": [
			{
				"family": "Cappozzo",
				"given": "A."
			},
			{
				"family": "Greselin",
				"given": "F."
			},
			{
				"family": "Murphy",
				"given": "T.B."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "ghoshDeploymentRobustCooperative2020",
		"type": "paper-conference",
		"abstract": "We study the problem of designing an AI agent that can robustly cooperate with agents of unknown type (i.e., previously unobserved behavior) in multi-agent scenarios. Our work is inspired by real-world applications in which an AI agent, e.g., a virtual assistant, has to cooperate with new types of agents/users after its deployment. We model this problem via parametric Markov Decision Processes where the parameters correspond to a user's type and characterize her behavior. In the test phase, the AI agent has to interact with a user of an unknown type. We develop an algorithmic framework for learning adaptive policies: our approach relies on observing the user's actions to make inferences about the user's type and adapting the policy to facilitate efficient cooperation. We show that without being adaptive, an AI agent can end up performing arbitrarily bad in the test phase. Using our framework, we propose two concrete algorithms for computing policies that automatically adapt to the user in the test phase. We demonstrate the effectiveness of our algorithms in a cooperative gathering game environment for two agents. © 2020 International Foundation for Autonomous.",
		"archive": "Scopus",
		"event-title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",
		"page": "447-455",
		"title": "Towards deployment of robust cooperative ai agents: An algorithmic framework for learning adaptive policies",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093460105&partnerID=40&md5=9647e6eca81c01f303cba645d35a90aa",
		"volume": "2020-May",
		"author": [
			{
				"family": "Ghosh",
				"given": "A."
			},
			{
				"family": "Mahdavi",
				"given": "H."
			},
			{
				"family": "Tschiatschek",
				"given": "S."
			},
			{
				"family": "Singla",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wardAssuranceCasePattern2020",
		"type": "paper-conference",
		"abstract": "Machine Learning (ML) has the potential to become widespread in safety-critical applications. It is therefore important that we have sufficient confidence in the safe behaviour of the ML-based functionality. One key consideration is whether the ML being used is interpretable. In this paper, we present an argument pattern, i.e. reusable structure, that can be used for justifying the sufficient interpretability of ML within a wider assurance case. The pattern can be used to assess whether the right interpretability method and format are used in the right context (time, setting and audience). This argument structure provides a basis for developing and assessing focused requirements for the interpretability of ML in safety-critical domains. © 2020, Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-030-55583-2_30",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "395-407",
		"title": "An Assurance Case Pattern for the Interpretability of Machine Learning in Safety-Critical Systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096588864&doi=10.1007%2f978-3-030-55583-2_30&partnerID=40&md5=eb76c695180f85f6bedebd5169c827e6",
		"volume": "12235 LNCS",
		"author": [
			{
				"family": "Ward",
				"given": "F.R."
			},
			{
				"family": "Habli",
				"given": "I."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wozniakSafetyCasePattern2020",
		"type": "paper-conference",
		"abstract": "Several standards from the domain of safety critical systems, in order to support the argumentation of the safety assurance of a system under development, recommend the construction of a safety case. This activity is guided by the objectives to be met, recommended or required by the standards along the safety lifecycle. Ongoing attempts to use Machine Learning (ML) for safety critical functionality revealed certain deficits. For instance, the widely recognized standard for functional safety of automotive systems, ISO 26262, which can be used as a basis to construct a safety case, does not reason about ML. To this end, the goal of this work is to provide a pattern for arguing about the correct implementation of safety requirements in system components based on ML. The pattern is integrated within an overall encompassing approach for safety case generation for automotive systems and its applicability is showcased on a pedestrian avoidance system. © 2020, Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-030-55583-2_28",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "370-382",
		"title": "A Safety Case Pattern for Systems with Machine Learning Components",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096573715&doi=10.1007%2f978-3-030-55583-2_28&partnerID=40&md5=fe9cbbd8d29147cd6d4befb4c7199d1f",
		"volume": "12235 LNCS",
		"author": [
			{
				"family": "Wozniak",
				"given": "E."
			},
			{
				"family": "Cârlan",
				"given": "C."
			},
			{
				"family": "Acar-Celik",
				"given": "E."
			},
			{
				"family": "Putzer",
				"given": "H.J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lagravePrincipalComponentAnalysis2020",
		"type": "paper-conference",
		"abstract": "Building robust-by-design Machine Learning algorithms is key for critical tasks such as safety or military applications. By leveraging on the ideas developed in the context of building invariant Support Vectors Machines, this paper introduces a convenient methodology for embedding local Lie groups symmetries into Deep Learning algorithms by performing a Principal Component Analysis on the corresponding Tangent Covariance Matrix. The projection of the input data onto the principal directions leads to a new data representation which allows singling out the components conveying the semantic information useful to the considered algorithmic task while reducing the dimension of the input manifold. Besides, our numerical testing emphasizes that, although less efficient than using Group-Convolutional Neural Networks as only dealing with local symmetries, our approach does improve accuracy and robustness without introducing significant computational overhead. Performance improvements up to 5% were obtained for low capacity algorithms, making this approach of particular interest for the engineering of safe embedded Artificial Intelligence systems. © 2020, Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-030-55583-2_22",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "302-314",
		"title": "A Principal Component Analysis Approach for Embedding Local Symmetries into Deep Learning Algorithms",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096594336&doi=10.1007%2f978-3-030-55583-2_22&partnerID=40&md5=26c5c385b8de90bde29d678864ad77a9",
		"volume": "12235 LNCS",
		"author": [
			{
				"family": "Lagrave",
				"given": "P.-Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "klasFrameworkBuildingUncertainty2020",
		"type": "paper-conference",
		"abstract": "More and more software-intensive systems include components that are data-driven in the sense that they use models based on artificial intelligence (AI) or machine learning (ML). Since the outcomes of such models cannot be assumed to always be correct, related uncertainties must be understood and taken into account when decisions are made using these outcomes. This applies, in particular, if such decisions affect the safety of the system. To date, however, hardly any AI-/ML-based model provides dependable estimates of the uncertainty remaining in its outcomes. In order to address this limitation, we present a framework for encapsulating existing models applied in data-driven components with an uncertainty wrapper in order to enrich the model outcome with a situation-aware and dependable uncertainty statement. The presented framework is founded on existing work on the concept and mathematical foundation of uncertainty wrappers. The application of the framework is illustrated using pedestrian detection as an example, which is a particularly safety-critical feature in the context of autonomous driving. The Brier score and its components are used to investigate how the key aspects of the framework (scoping, clustering, calibration, and confidence limits) can influence the quality of uncertainty estimates. © 2020, Springer Nature Switzerland AG.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-030-55583-2_23",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "315-327",
		"title": "A Framework for Building Uncertainty Wrappers for AI/ML-Based Data-Driven Components",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096598898&doi=10.1007%2f978-3-030-55583-2_23&partnerID=40&md5=83471415db0d0778b436e00a14267b50",
		"volume": "12235 LNCS",
		"author": [
			{
				"family": "Kläs",
				"given": "M."
			},
			{
				"family": "Jöckel",
				"given": "L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "pinarozisikSecurityAnalysisSafe2020",
		"type": "paper-conference",
		"abstract": "We analyze the extent to which existing methods rely on accurate training data for a specific class of reinforcement learning (RL) algorithms, known as Safe and Seldonian RL. We introduce a new measure of security to quantify the susceptibility to perturbations in training data by creating an attacker model that represents a worst-case analysis, and show that a couple of Seldonian RL methods are extremely sensitive to even a few data corruptions. We then introduce a new algorithm that is more robust against data corruptions, and demonstrate its usage in practice on some RL problems, including a grid-world and a diabetes treatment simulation. © 2020 Neural information processing systems foundation. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Advances in Neural Information Processing Systems",
		"title": "Security analysis of safe and seldonian reinforcement learning algorithms",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108455515&partnerID=40&md5=dfff89b401122e01f74e471cb746b1f9",
		"volume": "2020-December",
		"author": [
			{
				"family": "Pinar Ozisik",
				"given": "A."
			},
			{
				"family": "Thomas",
				"given": "P.S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "jingRobustExtremeLearning2020",
		"type": "article-journal",
		"abstract": "Uncertain or missing data may occur in many practical applications. A principled strategy for handling this problem would therefore be very useful. We consider two-class and multi-class classification problems where the mean and covariance of each class are assumed to be known. With simple structure, fast speed and good performance, extreme learning machine (ELM) has been an important technology in machine learning. In this work, from the viewpoint of probability, we present a robust ELM framework (RELM) for missing data classification. Applying the Chebyshev–Cantelli inequality, the proposed RELM is reformulated as a second-order cone programming with global optimal solution. The proposed RELM only relates to the second moments of input samples and makes no assumption about the data probability distribution. Expectation maximization algorithm is used to fill in missing values and then obtain complete data. Numerical experiments are simulated in various datasets from UCI database and a practical application database. Experimental results show that the proposed method can achieve better performance than traditional methods. These results illustrate the feasibility and effectiveness of the proposed method for missing data classification. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",
		"archive": "Scopus",
		"container-title": "Journal of Supercomputing",
		"DOI": "10.1007/s11227-018-2430-6",
		"issue": "4",
		"page": "2390-2416",
		"title": "A robust extreme learning machine framework for uncertain data classification",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081254670&doi=10.1007%2fs11227-018-2430-6&partnerID=40&md5=9d6880693d753dff20a823380e6433c1",
		"volume": "76",
		"author": [
			{
				"family": "Jing",
				"given": "S."
			},
			{
				"family": "Yang",
				"given": "L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "alufaisanRobustTransparencyModel2020",
		"type": "article-journal",
		"abstract": "Transparency has become a critical need in machine learning (ML) applications. Designing transparent ML models helps increase trust, ensure accountability, and scrutinize fairness. Some organizations may opt-out of transparency to protect individuals&#x0027; privacy. Therefore, there is a great demand for transparency models that consider both privacy and security risks. Such transparency models can motivate organizations to improve their credibility by making the ML-based decision-making process comprehensible to end-users. Differential privacy (DP) provides an important technique to disclose information while protecting individual privacy. However, it has been shown that DP alone cannot prevent certain types of privacy attacks against disclosed ML models. DP with low values can provide high privacy guarantees, but may result in significantly weaker ML models in terms of accuracy. On the other hand, setting value too high may lead to successful privacy attacks. This raises the question whether we can disclose accurate transparent ML models while preserving privacy. In this paper we introduce a novel technique that complements DP to ensure model transparency and accuracy while being robust against model inversion attacks. We show that combining the proposed technique with DP provide highly transparent and accurate ML models while preserving privacy against model inversion attacks. IEEE",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Dependable and Secure Computing",
		"DOI": "10.1109/TDSC.2020.3019508",
		"title": "Robust Transparency Against Model Inversion Attacks",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090442360&doi=10.1109%2fTDSC.2020.3019508&partnerID=40&md5=bb2978e08029eebf6487f9ba6bd9e6b3",
		"author": [
			{
				"family": "Alufaisan",
				"given": "Y."
			},
			{
				"family": "Kantarcioglu",
				"given": "M."
			},
			{
				"family": "Zhou",
				"given": "Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "liDeepRobustReinforcement2019",
		"type": "article-journal",
		"abstract": "In algorithmic trading, feature extraction and trading strategy design are two prominent challenges to acquire long-term profits. However, the previously proposed methods rely heavily on domain knowledge to extract handcrafted features and lack an effective way to dynamically adjust the trading strategy. With the recent breakthroughs of deep reinforcement learning (DRL), sequential real-world problems can be modeled and solved with a more human-like approach. In this paper, we propose a novel trading agent, based on deep reinforcement learning, to autonomously make trading decisions and gain profits in the dynamic financial markets. We extend the value-based deep Q-network (DQN) and the asynchronous advantage actor-critic (A3C) for better adapting to the trading market. Specifically, in order to automatically extract robust market representations and resolve the financial time series dependence, we utilize the stacked denoising autoencoders (SDAEs) and the long short-term memory (LSTM) as parts of the function approximator, respectively. Furthermore, we design several elaborate mechanisms to make the trading agent more practical to the real trading environment, such as position-controlled action and n-step reward. The experimental results show that our trading agent outperforms the baselines and achieves stable risk-adjusted returns in both the stock and the futures markets. © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2019.2932789",
		"page": "108014-108021",
		"title": "Deep Robust Reinforcement Learning for Practical Algorithmic Trading",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071105095&doi=10.1109%2fACCESS.2019.2932789&partnerID=40&md5=a210d1234cf03eb745ff22d3367444d2",
		"volume": "7",
		"author": [
			{
				"family": "Li",
				"given": "Y."
			},
			{
				"family": "Zheng",
				"given": "W."
			},
			{
				"family": "Zheng",
				"given": "Z."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "prakashImprovingSafetyReinforcement2019",
		"type": "paper-conference",
		"abstract": "Recent progress in AI and Reinforcement learning has shown great success in solving complex problems with high dimensional state spaces. However, most of these successes have been primarily in simulated environments where failure is of little or no consequence. Most real-world applications, however, require training solutions that are safe to operate as catastrophic failures are inadmissible especially when there is human interaction involved. Currently, Safe RL systems use human oversight during training and exploration in order to make sure the RL agent does not go into a catastrophic state. These methods require a large amount of human labor and it is very difficult to scale up. We present a hybrid method for reducing the human intervention time by combining model-based approaches and training a supervised learner to improve sample efficiency while also ensuring safety. We evaluate these methods on various grid-world environments using both standard and visual representations and show that our approach achieves better performance in terms of sample efficiency, number of catastrophic states reached as well as overall task performance compared to traditional model-free approaches. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of the 32nd International Florida Artificial Intelligence Research Society Conference, FLAIRS 2019",
		"page": "50-55",
		"title": "Improving safety in reinforcement learning using model-based architectures and human intervention",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094005967&partnerID=40&md5=77fcdf372b36d2d963ff0e048ff1e361",
		"author": [
			{
				"family": "Prakash",
				"given": "B."
			},
			{
				"family": "Khatwani",
				"given": "M."
			},
			{
				"family": "Waytowich",
				"given": "N."
			},
			{
				"family": "Mohsenin",
				"given": "T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "shahamUnderstandingAdversarialTraining2018",
		"type": "article-journal",
		"abstract": "We show that adversarial training of supervised learning models is in fact a robust optimization procedure. To do this, we establish a general framework for increasing local stability of supervised learning models using robust optimization. The framework is general and broadly applicable to differentiable non-parametric models, e.g., Artificial Neural Networks (ANNs). Using an alternating minimization-maximization procedure, the loss of the model is minimized with respect to perturbed examples that are generated at each parameter update, rather than with respect to the original training data. Our proposed framework generalizes adversarial training, as well as previous approaches for increasing local stability of ANNs. Experimental results reveal that our approach increases the robustness of the network to existing adversarial examples, while making it harder to generate new ones. Furthermore, our algorithm improves the accuracy of the networks also on the original test data. © 2018 Elsevier B.V.",
		"archive": "Scopus",
		"container-title": "Neurocomputing",
		"DOI": "10.1016/j.neucom.2018.04.027",
		"page": "195-204",
		"title": "Understanding adversarial training: Increasing local stability of supervised models through robust optimization",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047515976&doi=10.1016%2fj.neucom.2018.04.027&partnerID=40&md5=c8b27ed05118590fb453407e684ea2b0",
		"volume": "307",
		"author": [
			{
				"family": "Shaham",
				"given": "U."
			},
			{
				"family": "Yamada",
				"given": "Y."
			},
			{
				"family": "Negahban",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "yangAdaptiveCriticDesigns2019",
		"type": "article-journal",
		"abstract": "This paper develops a novel event-triggered robust control strategy for continuous-time nonlinear systems with unknown dynamics. To begin with, the event-triggered robust nonlinear control problem is transformed into an event-triggered nonlinear optimal control problem by introducing an infinite-horizon integral cost for the nominal system. Then, a recurrent neural network (RNN) and adaptive critic designs (ACDs) are employed to solve the derived event-triggered nonlinear optimal control problem. The RNN is applied to reconstruct the system dynamics based on collected system data. After acquiring the knowledge of system dynamics, a unique critic network is proposed to obtain the approximate solution of the event-triggered Hamilton-Jacobi-Bellman equation within the framework of ACDs. The critic network is updated by using simultaneously historical and instantaneous state data. An advantage of the present critic network update law is that it can relax the persistence of excitation condition. Meanwhile, under a newly developed event-triggering condition, the proposed critic network tuning rule not only guarantees the critic network weights to converge to optimums but also ensures nominal system states to be uniformly ultimately bounded. Moreover, by using Lyapunov method, it is proved that the derived optimal event-triggered control (ETC) guarantees uniform ultimate boundedness of all the signals in the original system. Finally, a nonlinear oscillator and an unstable power system are provided to validate the developed robust ETC scheme. © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Transactions on Cybernetics",
		"DOI": "10.1109/TCYB.2018.2823199",
		"issue": "6",
		"page": "2255-2267",
		"title": "Adaptive Critic Designs for Event-Triggered Robust Control of Nonlinear Systems with Unknown Dynamics",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045765098&doi=10.1109%2fTCYB.2018.2823199&partnerID=40&md5=5c47089e8fef0b934762484ce73a6aa0",
		"volume": "49",
		"author": [
			{
				"family": "Yang",
				"given": "X."
			},
			{
				"family": "He",
				"given": "H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "canonicoFlashCrashesMultiAgent2019",
		"type": "paper-conference",
		"abstract": "As AI advances and becomes more complicated, it becomes necessary to study the safety implications of its behavior. This paper expands upon prior AI-safety research to create a model to study the harmful outcomes of multi-agent systems. In this paper, we outline previous work that has highlighted multiple aspects of AI-safety research and focus on AI-safety systems in multi-agent systems. After overviewing previous literature, we present a model focused on flash crashes, a concept often found in economics. The model was constructed using an interdisciplinary approach that includes game theory, machine learning, cognitive science and systems theory to study flash crashes in complex human-AI systems. We use the model to study a complex interaction between AI-agents, and our results indicate the multi-agent system in question is prone to cause flash crashes. © 2019 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/WSC40007.2019.9004675",
		"event-title": "Proceedings - Winter Simulation Conference",
		"page": "193-204",
		"title": "Flash Crashes in Multi-Agent Systems Using Minority Games and Reinforcement Learning to Test AI Safety",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081129928&doi=10.1109%2fWSC40007.2019.9004675&partnerID=40&md5=21d0982f79295c9d8e0fd9260e9e4833",
		"volume": "2019-December",
		"author": [
			{
				"family": "Canonico",
				"given": "L.B."
			},
			{
				"family": "McNeese",
				"given": "N."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "machidaNversionMachineLearning2019",
		"type": "paper-conference",
		"abstract": "Quality control of machine learning systems is a fundamental challenge in industries to provide intelligent services or products using machine learning. While recent advances in machine learning algorithms substantially improve the performance of intelligent tasks such as object recognition, their outputs are essentially stochastic and very sensitive to input data. Such an output uncertainty is a big obstacle to ensure the quality of safety critical applications like autonomous vehicle and hence architectural design to mitigate the impact of error output becomes a great importance. In this paper, we propose N-version machine learning architecture that aims to improve system reliability against probabilistic outputs of individual machine learning modules. The key idea of this architecture is exploiting two kinds of diversities; input diversity and model diversity. Our study first formally defines these diversity metrics and analytically shows the improved reliability by N-version machine learning architecture. Since we treat a machine learning module as a black-box, the proposed architecture and the reliability property are generally applicable to any machine learning algorithms and applications. © 2019 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/DSN-W.2019.00017",
		"event-title": "Proceedings - 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop, DSN-W 2019",
		"page": "48-51",
		"title": "N-version machine learning models for safety critical systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072049689&doi=10.1109%2fDSN-W.2019.00017&partnerID=40&md5=72ee90ee38a50aa4360eabec023ddb53",
		"author": [
			{
				"family": "Machida",
				"given": "F."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "dikImprovedRobustFuzzy2020",
		"type": "article-journal",
		"abstract": "This paper presents a robust, dynamic, and unsupervised fuzzy learning algorithm (RDUFL) that aims to cluster a set of data samples with the ability to detect outliers and assign the numbers of clusters automatically. It consists of three main stages. The first (1) stage is a pre-processing method in which possible outliers are determined and quarantined using a concept of proximity degree. The second (2) stage is a learning method, which consists in auto-detecting the number of classes with their prototypes for a dynamic threshold. This threshold is automatically determined based on the similarity among the detected prototypes that are updated at the exploration of a new data. The last (3) stage treats quarantined samples detected from the first stage to determine whether they belong to some class defined in the second phase. The effectiveness of this method is assessed on eight real medical benchmark datasets in comparison to known unsupervised learning methods, namely, the fuzzy c-means (FCM), possibilistic c-means (PCM), and noise clustering (NC). The obtained accuracy of our scheme is very promising for unsupervised learning problems. © 2020 Walter de Gruyter GmbH, Berlin/Boston.",
		"archive": "Scopus",
		"container-title": "Journal of Intelligent Systems",
		"DOI": "10.1515/jisys-2018-0030",
		"issue": "1",
		"page": "1028-1042",
		"title": "An Improved Robust Fuzzy Algorithm for Unsupervised Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056243355&doi=10.1515%2fjisys-2018-0030&partnerID=40&md5=c83a75d5ee40e2f0bc136d8e5f54214b",
		"volume": "29",
		"author": [
			{
				"family": "Dik",
				"given": "A."
			},
			{
				"family": "Jebari",
				"given": "K."
			},
			{
				"family": "Ettouhami",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "picardiAssuranceArgumentPatterns2020",
		"type": "paper-conference",
		"abstract": "Machine Learnt (ML) components are now widely accepted for use in a range of applications with results that are reported to exceed, under certain conditions, human performance. The adoption of ML components in safety-related domains is restricted, however, unless sufficient assurance can be demonstrated that the use of these components does not compromise safety. In this paper, we present patterns that can be used to develop assurance arguments for demonstrating the safety of the ML components. The argument patterns provide reusable templates for the types of claims that must be made in a compelling argument. On their own, the patterns neither detail the assurance artefacts that must be generated to support the safety claims for a particular system, nor provide guidance on the activities that are required to generate these artefacts. We have therefore also developed a process for the engineering of ML components in which the assurance evidence can be generated at each stage in the ML lifecycle in order to instantiate the argument patterns and create the assurance case for ML components. The patterns and the process could help provide a practical and clear basis for a justifiable deployment of ML components in safety-related systems. © 2020 for this paper by its authors.",
		"archive": "Scopus",
		"event-title": "CEUR Workshop Proceedings",
		"page": "23-30",
		"title": "Assurance argument patterns and processes for machine learning in safety-related systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081604916&partnerID=40&md5=aeaccb7806b23dc5dcf079764a13b75b",
		"volume": "2560",
		"author": [
			{
				"family": "Picardi",
				"given": "C."
			},
			{
				"family": "Paterson",
				"given": "C."
			},
			{
				"family": "Hawkins",
				"given": "R."
			},
			{
				"family": "Calinescu",
				"given": "R."
			},
			{
				"family": "Habli",
				"given": "I."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "taylorAlignmentAdvancedMachine2020",
		"type": "chapter",
		"abstract": "This chapter surveys eight research areas organized around one question: As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators? The chapter focuses on two major technical obstacles to AI alignment: the challenge of specifying the right kind of objective functions and the challenge of designing AI systems that avoid unintended consequences and undesirable behavior even in cases where the objective function does not line up perfectly with the intentions of the designers. The questions surveyed include the following: How can we train reinforcement learners to take actions that are more amenable to meaningful assessment by intelligent overseers? What kinds of objective functions incentivize a system to “not have an overly large impact” or “not have many side effects”? The chapter discusses these questions, related work, and potential directions for future research, with the goal of highlighting relevant research topics in machine learning that appear tractable today. © Oxford University Press 2020.",
		"archive": "Scopus",
		"container-title": "Ethics of Artificial Intelligence",
		"note": "DOI: 10.1093/oso/9780190905033.003.0013",
		"page": "342-382",
		"title": "Alignment for advanced machine learning systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111881274&doi=10.1093%2foso%2f9780190905033.003.0013&partnerID=40&md5=f1ab76cab567dada4f844447213bb6fa",
		"author": [
			{
				"family": "Taylor",
				"given": "J."
			},
			{
				"family": "Yudkowsky",
				"given": "E."
			},
			{
				"family": "LaVictoire",
				"given": "P."
			},
			{
				"family": "Critch",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "nicolaeAlgorithmicRobustnessSemisupervised2015",
		"type": "paper-conference",
		"abstract": "Using the appropriate metric is crucial for the performance of most of machine learning algorithms. For this reason, a lot of effort has been put into distance and similarity learning. However, it is worth noting that this research field lacks theoretical guarantees that can be expected on the generalization capacity of the classifier associated to a learned metric. The theoretical framework of (ε, γ, τ)-good similarity functions [1] provides means to relate the properties of a similarity function and those of a linear classifier making use of it. In this paper, we extend this theory to a method where the metric and the separator are jointly learned in a semi-supervised way, setting that has not been explored before. We furthermore prove the robustness of our algorithm, which allows us to provide a generalization bound for this approach. The behavior of our method is illustrated via some experimental results. © Springer International Publishing Switzerland 2015.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-319-26532-2_28",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "253-263",
		"title": "Algorithmic robustness for semi-supervised (ε, γ, τ)-good metric learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952837463&doi=10.1007%2f978-3-319-26532-2_28&partnerID=40&md5=acb93b59e9c15b2a75bd6972252bfefd",
		"volume": "9489",
		"author": [
			{
				"family": "Nicolae",
				"given": "M.-I."
			},
			{
				"family": "Sebban",
				"given": "M."
			},
			{
				"family": "Habrard",
				"given": "A."
			},
			{
				"family": "Gaussier",
				"given": "E."
			},
			{
				"family": "Amini",
				"given": "M.-R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "lendasseExtremeLearningMachine2013",
		"type": "paper-conference",
		"abstract": "In this paper is described the original (basic) Extreme Learning Machine (ELM). Properties like robustness and sensitivity to variable selection are studied. Several extensions of the original ELM are then presented and compared. Firstly, Tikhonov-Regularized Optimally-Pruned Extreme Learning Machine (TROP-ELM) is summarized as an improvement of the Optimally-Pruned Extreme Learning Machine (OP-ELM) in the form of a L 2 regularization penalty applied within the OP-ELM. Secondly, a Methodology to Linearly Ensemble ELM (-ELM) is presented in order to improve the performance of the original ELM. These methodologies (TROP-ELM and -ELM) are tested against state of the art methods such as Support Vector Machines or Gaussian Processes and the original ELM and OP-ELM, on ten different data sets. A specific experiment to test the sensitivity of these methodologies to variable selection is also presented. © 2013 Springer-Verlag Berlin Heidelberg.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-642-38679-4_2",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"note": "issue: PART 1",
		"page": "17-35",
		"title": "Extreme learning machine: A robust modeling technique? yes!",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880077016&doi=10.1007%2f978-3-642-38679-4_2&partnerID=40&md5=1f46c0c9280c127a485d686042f32665",
		"volume": "7902 LNCS",
		"author": [
			{
				"family": "Lendasse",
				"given": "A."
			},
			{
				"family": "Akusok",
				"given": "A."
			},
			{
				"family": "Simula",
				"given": "O."
			},
			{
				"family": "Corona",
				"given": "F."
			},
			{
				"family": "Van Heeswijk",
				"given": "M."
			},
			{
				"family": "Eirola",
				"given": "E."
			},
			{
				"family": "Miche",
				"given": "Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		}
	},
	{
		"id": "elmhamdiWhenNeuronsFail2017",
		"type": "paper-conference",
		"abstract": "Neural networks have been traditionally considered robust in the sense that their precision degrades gracefully with the failure of neurons and can be compensated by additional learning phases. Nevertheless, critical applications for which neural networks are now appealing solutions, cannot afford any additional learning at run-time. In this paper, we view a multilayer neural network as a distributed system of which neurons can fail independently, and we evaluate its robustness in the absence of any (recovery) learning phase. We give tight bounds on the number of neurons that can fail without harming the result of a computation. To determine our bounds, we leverage the fact that neuralactivation functions are Lipschitz-continuous. Our bound isgiven in the form of quantity, we call the Forward ErrorPropagation, computing this quantity only requires looking atthe topology of the network, while experimentally assessingthe robustness of a network requires the costly experiment oflooking at all the possible inputs and testing all the possibleconfigurations of the network corresponding to different failuresituations, facing a discouraging combinatorial explosion. We distinguish the case of neurons that can fail and stop their activity (crashed neurons) from the case of neurons that can fail by transmitting arbitrary values (Byzantine neurons). In the crash case, our bound involves the number of neuronsper layer, the Lipschitz constant of the neural activationfunction, the number of failing neurons, the synaptic weightsand the depth of the layer where the failure occurred. In thecase of Byzantine failures, our bound involves, in addition, thesynaptic transmission capacity. Interestingly, as we show inthe paper, our bound can easily be extended to the case wheresynapses can fail. We present three applications of our results. The first is aquantification of the effect of memory cost reduction on theaccuracy of a neural network. The second is a quantification ofthe amount of information any neuron needs from its precedinglayer, enabling thereby a boosting scheme that prevents neuronsfrom waiting for unnecessary signals. Our third applicationis a quantification of the trade-off between neural networksrobustness and learning cost. © 2017 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/IPDPS.2017.66",
		"event-title": "Proceedings - 2017 IEEE 31st International Parallel and Distributed Processing Symposium, IPDPS 2017",
		"page": "1028-1037",
		"title": "When Neurons Fail",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027702685&doi=10.1109%2fIPDPS.2017.66&partnerID=40&md5=d12a7728772dcc4135077fd4178a0145",
		"author": [
			{
				"family": "El Mhamdi",
				"given": "E.M."
			},
			{
				"family": "Guerraoui",
				"given": "R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "vivekGrayboxAdversarialTraining2018",
		"type": "paper-conference",
		"abstract": "Adversarial samples are perturbed inputs crafted to mislead the machine learning systems. A training mechanism, called adversarial training, which presents adversarial samples along with clean samples has been introduced to learn robust models. In order to scale adversarial training for large datasets, these perturbations can only be crafted using fast and simple methods (e.g., gradient ascent). However, it is shown that adversarial training converges to a degenerate minimum, where the model appears to be robust by generating weaker adversaries. As a result, the models are vulnerable to simple black-box attacks. In this paper we, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed “gray-box adversarial attacks” based on which we propose novel evaluation method to assess the robustness of the learned models, and (iii) propose a novel variant of adversarial training, named “Gray-box Adversarial Training” that uses intermediate versions of the models to seed the adversaries. Experimental evaluation demonstrates that the models trained using our method exhibit better robustness compared to both undefended and adversarially trained models. © Springer Nature Switzerland AG 2018.",
		"archive": "Scopus",
		"DOI": "10.1007/978-3-030-01267-0_13",
		"event-title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
		"page": "213-228",
		"title": "Gray-box adversarial training",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055438686&doi=10.1007%2f978-3-030-01267-0_13&partnerID=40&md5=e7a92b9cac84761bd58ae819624a68fd",
		"volume": "11219 LNCS",
		"author": [
			{
				"family": "Vivek",
				"given": "B.S."
			},
			{
				"family": "Mopuri",
				"given": "K.R."
			},
			{
				"family": "Babu",
				"given": "R.V."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "hendrycksUsingSelfsupervisedLearning2019",
		"type": "paper-conference",
		"abstract": "Self-supervision provides effective representations for downstream tasks without requiring labels. However, existing approaches lag behind fully supervised training and are often not thought beneficial beyond obviating or reducing the need for annotations. We find that self-supervision can benefit robustness in a variety of ways, including robustness to adversarial examples, label corruption, and common input corruptions. Additionally, self-supervision greatly benefits out-of-distribution detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods. These results demonstrate the promise of self-supervision for improving robustness and uncertainty estimation and establish these tasks as new axes of evaluation for future self-supervised learning research. © 2019 Neural information processing systems foundation. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Advances in Neural Information Processing Systems",
		"title": "Using self-supervised learning can improve model robustness and uncertainty",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089513686&partnerID=40&md5=c3851d370937fd50d52dcc25e03e3bd5",
		"volume": "32",
		"author": [
			{
				"family": "Hendrycks",
				"given": "D."
			},
			{
				"family": "Mazeika",
				"given": "M."
			},
			{
				"family": "Kadavath",
				"given": "S."
			},
			{
				"family": "Song",
				"given": "D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "yasudaImprovingRobustnessInstancebased2011",
		"type": "article-journal",
		"abstract": "Learning autonomous robots have been widely discussed in recent years. Reinforcement learning (RL) is a popular method in this domain. However, its performance is quite sensitive to the segmentation of state and action spaces. To overcome this problem, we developed the new technique Bayesian- discriminationfunction- based RL (BRL). BRL has proven to be more effective than other standard RL algorithms in dealing withmulti-robot system(MRS) problems. However, as in most learning systems, occasional overfitting problems occur in BRL. This paper introduces an extended BRL for improving the robustness of MRSs. Metalearning based on the information entropy of fired rules is adopted for adaptive modification of its learning parameters. Computer simulations are conducted to verify the effectiveness of our proposed method.",
		"archive": "Scopus",
		"container-title": "Journal of Advanced Computational Intelligence and Intelligent Informatics",
		"DOI": "10.20965/jaciii.2011.p1065",
		"issue": "8",
		"page": "1065-1072",
		"title": "Improving the robustness of instance-based reinforcement learning robots by metalearning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054952620&doi=10.20965%2fjaciii.2011.p1065&partnerID=40&md5=a01e1846ec1ab4558076a5d6405651e1",
		"volume": "15",
		"author": [
			{
				"family": "Yasuda",
				"given": "T."
			},
			{
				"family": "Araki",
				"given": "K."
			},
			{
				"family": "Ohkura",
				"given": "K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		}
	},
	{
		"id": "leRobustUnsupervisedFeature2013",
		"type": "paper-conference",
		"abstract": "To boost up power of unsupervised feature learning and deep learning, there has been a great effort in optimizing network structure to learn more efficient high level features. It is crucial for a network to have a sufficient amount of learnable parameters yet still be able to capture in variances in data. In this paper, the authors propose spatial boosting networks, which employ convolutional feature learning networks as learning components. Each component in a network is assigned to a certain spatial region. This allows the network learn more adaptive features for each region. In order to make spatial boosting networks to capture relationship between regions of the visual field, we also propose convolutional pooling procedure. By expanding pooling scope into overlapping regions, we expect the features pooled in higher level to be more robust to noises and more invariant to transformation. Experiments show that using spatial boosting networks boosts up accuracy up to 3% from conventional approaches in standard datasets CIFAR and STL. Moreover, these results are competitive in comparison with other methods by using only a basic feature learning algorithm. © 2013 IEEE.",
		"archive": "Scopus",
		"DOI": "10.1109/ICMLA.2013.168",
		"event-title": "Proceedings - 2013 12th International Conference on Machine Learning and Applications, ICMLA 2013",
		"page": "507-512",
		"title": "A robust unsupervised feature learning framework using spatial boosting networks",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899422191&doi=10.1109%2fICMLA.2013.168&partnerID=40&md5=d37cd0f93e3521438336b1269f6828d6",
		"volume": "2",
		"author": [
			{
				"family": "Le",
				"given": "N.D.-H."
			},
			{
				"family": "Tran",
				"given": "M.-T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		}
	},
	{
		"id": "smithResponsibilityAnswerability2015",
		"type": "article-journal",
		"abstract": "It has recently become fashionable among those who write on questions of moral responsibility to distinguish two different concepts, or senses, of moral responsibility via the labels ‘responsibility as attributability’ and ‘responsibility as accountability’. Gary Watson was perhaps the first to introduce this distinction in his influential 1996 article ‘Two Faces of Responsibility’ (in Agency and Answerability, 260–86. Oxford: Oxford University Press, 2004), but it has since been taken up by many other philosophers. My aim in this study is to raise some questions and doubts about this distinction and to argue that it has led to confusion rather than clarification in debates over moral responsibility. In place of the attributability/accountability distinction, I propose that there is a single (and unified) concept of moral responsibility underlying our actual moral practices. This core notion of moral responsibility, which I call ‘responsibility as answerability’, is well positioned to explain those aspects of our moral practice that Watson associates with the ‘attributability’ face of moral responsibility as well as those aspects of our moral practice he associates with the ‘accountability’ face. But it does so in a way that does not require us to multiply senses of moral responsibility and that allows us to continue to have meaningful disagreements over the basic conditions of moral responsibility.",
		"container-title": "Inquiry",
		"DOI": "10.1080/0020174X.2015.986851",
		"ISSN": "0020-174X",
		"issue": "2",
		"note": "publisher: Routledge\n_eprint: https://doi.org/10.1080/0020174X.2015.986851",
		"page": "99-126",
		"source": "Taylor and Francis+NEJM",
		"title": "Responsibility as Answerability",
		"URL": "https://doi.org/10.1080/0020174X.2015.986851",
		"volume": "58",
		"author": [
			{
				"family": "Smith",
				"given": "Angela M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					2,
					17
				]
			]
		}
	}
]
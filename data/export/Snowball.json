[
	{
		"id": "amodeiConcreteProblemsAI2016",
		"type": "article",
		"abstract": "Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",
		"DOI": "10.48550/arXiv.1606.06565",
		"note": "arXiv:1606.06565 [cs]",
		"number": "arXiv:1606.06565",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Concrete Problems in AI Safety",
		"URL": "http://arxiv.org/abs/1606.06565",
		"author": [
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Olah",
				"given": "Chris"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			},
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Mané",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					7,
					21
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					7,
					25
				]
			]
		}
	},
	{
		"id": "irvingAISafetyDebate2018",
		"type": "article",
		"abstract": "To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.",
		"DOI": "10.48550/arXiv.1805.00899",
		"note": "arXiv:1805.00899 [cs, stat]",
		"number": "arXiv:1805.00899",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AI safety via debate",
		"URL": "http://arxiv.org/abs/1805.00899",
		"author": [
			{
				"family": "Irving",
				"given": "Geoffrey"
			},
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					22
				]
			]
		}
	},
	{
		"id": "ngAlgorithmsInverseReinforcement2000",
		"type": "paper-conference",
		"abstract": "This paper addresses the problem of inverse reinforcement learning (IRL) in Markov decision processes, that is, the problem of extracting a reward function given observed, optimal\nbehavior. IRL may be useful for apprenticeship learning to acquire skilled behavior, and for ascertaining the reward function being optimized by a natural system. We first characterize the set of all reward func-tions for which a given policy is optimal. We then derive three algorithms for IRL. The first two deal with the case where the entire policy is known; we",
		"collection-title": "ICML '00",
		"container-title": "Proceedings of the Seventeenth International Conference on Machine Learning",
		"event-place": "San Francisco, CA, USA",
		"ISBN": "978-1-55860-707-1",
		"page": "663–670",
		"publisher": "Morgan Kaufmann Publishers Inc.",
		"publisher-place": "San Francisco, CA, USA",
		"source": "ACM Digital Library",
		"title": "Algorithms for Inverse Reinforcement Learning",
		"author": [
			{
				"family": "Ng",
				"given": "Andrew Y."
			},
			{
				"family": "Russell",
				"given": "Stuart J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2000",
					6,
					29
				]
			]
		}
	},
	{
		"id": "hadfield-menellCooperativeInverseReinforcement2016",
		"type": "paper-conference",
		"abstract": "For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial- information game with two agents, human and robot; both are rewarded according to the human’s reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.",
		"container-title": "Advances in Neural Information Processing Systems",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "Cooperative Inverse Reinforcement Learning",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html",
		"volume": "29",
		"author": [
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Russell",
				"given": "Stuart J"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "elhageToyModelsSuperposition2022",
		"type": "article",
		"abstract": "Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in \"superposition.\" We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability.",
		"DOI": "10.48550/arXiv.2209.10652",
		"note": "arXiv:2209.10652 [cs]",
		"number": "arXiv:2209.10652",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Toy Models of Superposition",
		"URL": "http://arxiv.org/abs/2209.10652",
		"author": [
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Lasenby",
				"given": "Robert"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Chen",
				"given": "Carol"
			},
			{
				"family": "Grosse",
				"given": "Roger"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Wattenberg",
				"given": "Martin"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					21
				]
			]
		}
	},
	{
		"id": "gardnerEvaluatingModelsLocal2020",
		"type": "paper-conference",
		"abstract": "Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets—up to 25% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.",
		"container-title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
		"DOI": "10.18653/v1/2020.findings-emnlp.117",
		"event-place": "Online",
		"event-title": "Findings 2020",
		"page": "1307–1323",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Online",
		"source": "ACLWeb",
		"title": "Evaluating Models' Local Decision Boundaries via Contrast Sets",
		"URL": "https://aclanthology.org/2020.findings-emnlp.117",
		"author": [
			{
				"family": "Gardner",
				"given": "Matt"
			},
			{
				"family": "Artzi",
				"given": "Yoav"
			},
			{
				"family": "Basmov",
				"given": "Victoria"
			},
			{
				"family": "Berant",
				"given": "Jonathan"
			},
			{
				"family": "Bogin",
				"given": "Ben"
			},
			{
				"family": "Chen",
				"given": "Sihao"
			},
			{
				"family": "Dasigi",
				"given": "Pradeep"
			},
			{
				"family": "Dua",
				"given": "Dheeru"
			},
			{
				"family": "Elazar",
				"given": "Yanai"
			},
			{
				"family": "Gottumukkala",
				"given": "Ananth"
			},
			{
				"family": "Gupta",
				"given": "Nitish"
			},
			{
				"family": "Hajishirzi",
				"given": "Hannaneh"
			},
			{
				"family": "Ilharco",
				"given": "Gabriel"
			},
			{
				"family": "Khashabi",
				"given": "Daniel"
			},
			{
				"family": "Lin",
				"given": "Kevin"
			},
			{
				"family": "Liu",
				"given": "Jiangming"
			},
			{
				"family": "Liu",
				"given": "Nelson F."
			},
			{
				"family": "Mulcaire",
				"given": "Phoebe"
			},
			{
				"family": "Ning",
				"given": "Qiang"
			},
			{
				"family": "Singh",
				"given": "Sameer"
			},
			{
				"family": "Smith",
				"given": "Noah A."
			},
			{
				"family": "Subramanian",
				"given": "Sanjay"
			},
			{
				"family": "Tsarfaty",
				"given": "Reut"
			},
			{
				"family": "Wallace",
				"given": "Eric"
			},
			{
				"family": "Zhang",
				"given": "Ally"
			},
			{
				"family": "Zhou",
				"given": "Ben"
			}
		],
		"editor": [
			{
				"family": "Cohn",
				"given": "Trevor"
			},
			{
				"family": "He",
				"given": "Yulan"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					11
				]
			]
		}
	},
	{
		"id": "goodfellowExplainingHarnessingAdversarial2015",
		"type": "article",
		"abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
		"DOI": "10.48550/arXiv.1412.6572",
		"note": "arXiv:1412.6572 [cs, stat]",
		"number": "arXiv:1412.6572",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Explaining and Harnessing Adversarial Examples",
		"URL": "http://arxiv.org/abs/1412.6572",
		"author": [
			{
				"family": "Goodfellow",
				"given": "Ian J."
			},
			{
				"family": "Shlens",
				"given": "Jonathon"
			},
			{
				"family": "Szegedy",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					3,
					20
				]
			]
		}
	},
	{
		"id": "hendrycksBenchmarkingNeuralNetwork2019",
		"type": "article",
		"abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
		"DOI": "10.48550/arXiv.1903.12261",
		"note": "arXiv:1903.12261 [cs, stat]",
		"number": "arXiv:1903.12261",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
		"URL": "http://arxiv.org/abs/1903.12261",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Dietterich",
				"given": "Thomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					3,
					28
				]
			]
		}
	},
	{
		"id": "hendrycksBaselineDetectingMisclassified2018",
		"type": "paper-conference",
		"abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.",
		"DOI": "10.48550/arXiv.1610.02136",
		"event-title": "International Conference on Representation Learning",
		"note": "arXiv:1610.02136 [cs]",
		"source": "arXiv.org",
		"title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
		"URL": "http://arxiv.org/abs/1610.02136",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Gimpel",
				"given": "Kevin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					3
				]
			]
		}
	},
	{
		"id": "kaushikLearningDifferenceThat2020",
		"type": "article",
		"abstract": "Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.",
		"DOI": "10.48550/arXiv.1909.12434",
		"note": "arXiv:1909.12434 [cs, stat]",
		"number": "arXiv:1909.12434",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning the Difference that Makes a Difference with Counterfactually-Augmented Data",
		"URL": "http://arxiv.org/abs/1909.12434",
		"author": [
			{
				"family": "Kaushik",
				"given": "Divyansh"
			},
			{
				"family": "Hovy",
				"given": "Eduard"
			},
			{
				"family": "Lipton",
				"given": "Zachary C."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					2,
					14
				]
			]
		}
	},
	{
		"id": "hendrycksUnsolvedProblemsML2022a",
		"type": "article",
		"abstract": "Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (\"Robustness\"), identifying hazards (\"Monitoring\"), reducing inherent model hazards (\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout, we clarify each problem's motivation and provide concrete research directions.",
		"note": "arXiv:2109.13916 [cs]",
		"number": "arXiv:2109.13916",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Unsolved Problems in ML Safety",
		"URL": "http://arxiv.org/abs/2109.13916",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Carlini",
				"given": "Nicholas"
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					16
				]
			]
		}
	},
	{
		"id": "shenSocialNormsGroundedMachine2022",
		"type": "paper-conference",
		"abstract": "Ethical judgment aims to determine if a person in a narrative situation acts under people's social norms under a culture, so it is crucial to understand actions in narratives and achieve machine ethics. Recent works depend on data-driven methods to directly judge the ethics of complex real-world narratives but face two major challenges. First, they cannot well handle dilemma situations due to a lack of basic knowledge about social norms. Second, they focus merely on sparse situation-level judgment regardless of the social norms involved during the judgment, leading to a black box. In this work, inspired by previous knowledge-grounded and -augmented paradigms, we propose to complement a complex situation with grounded social norms. Besides a norm-grounding knowledge model, we present a novel norm-supported ethical judgment model in line with neural module networks to alleviate dilemma situations and improve norm-level explainability. Empirically, our model improves state-of-the-art performance on two narrative judgment benchmarks.",
		"container-title": "Proceedings of the 29th International Conference on Computational Linguistics",
		"event-place": "Gyeongju, Republic of Korea",
		"event-title": "COLING 2022",
		"page": "1333–1343",
		"publisher": "International Committee on Computational Linguistics",
		"publisher-place": "Gyeongju, Republic of Korea",
		"source": "ACLWeb",
		"title": "Social Norms-Grounded Machine Ethics in Complex Narrative Situation",
		"URL": "https://aclanthology.org/2022.coling-1.114",
		"author": [
			{
				"family": "Shen",
				"given": "Tao"
			},
			{
				"family": "Geng",
				"given": "Xiubo"
			},
			{
				"family": "Jiang",
				"given": "Daxin"
			}
		],
		"editor": [
			{
				"family": "Calzolari",
				"given": "Nicoletta"
			},
			{
				"family": "Huang",
				"given": "Chu-Ren"
			},
			{
				"family": "Kim",
				"given": "Hansaem"
			},
			{
				"family": "Pustejovsky",
				"given": "James"
			},
			{
				"family": "Wanner",
				"given": "Leo"
			},
			{
				"family": "Choi",
				"given": "Key-Sun"
			},
			{
				"family": "Ryu",
				"given": "Pum-Mo"
			},
			{
				"family": "Chen",
				"given": "Hsin-Hsi"
			},
			{
				"family": "Donatelli",
				"given": "Lucia"
			},
			{
				"family": "Ji",
				"given": "Heng"
			},
			{
				"family": "Kurohashi",
				"given": "Sadao"
			},
			{
				"family": "Paggio",
				"given": "Patrizia"
			},
			{
				"family": "Xue",
				"given": "Nianwen"
			},
			{
				"family": "Kim",
				"given": "Seokhwan"
			},
			{
				"family": "Hahm",
				"given": "Younggyun"
			},
			{
				"family": "He",
				"given": "Zhong"
			},
			{
				"family": "Lee",
				"given": "Tony Kyungil"
			},
			{
				"family": "Santus",
				"given": "Enrico"
			},
			{
				"family": "Bond",
				"given": "Francis"
			},
			{
				"family": "Na",
				"given": "Seung-Hoon"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10
				]
			]
		}
	},
	{
		"id": "christianoSupervisingStrongLearners2018",
		"type": "article",
		"abstract": "Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.",
		"DOI": "10.48550/arXiv.1810.08575",
		"note": "arXiv:1810.08575 [cs, stat]",
		"number": "arXiv:1810.08575",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Supervising strong learners by amplifying weak experts",
		"URL": "http://arxiv.org/abs/1810.08575",
		"author": [
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Shlegeris",
				"given": "Buck"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					19
				]
			]
		}
	},
	{
		"id": "yampolskiyTaxonomyPathwaysDangerous2015",
		"type": "article",
		"abstract": "In order to properly handle a dangerous Artificially Intelligent (AI) system it is important to understand how the system came to be in such a state. In popular culture (science fiction movies/books) AIs/Robots became self-aware and as a result rebel against humanity and decide to destroy it. While it is one possible scenario, it is probably the least likely path to appearance of dangerous AI. In this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious AI. To the best of our knowledge, this is the first attempt to systematically classify types of pathways leading to malevolent AI. Previous relevant work either surveyed specific goals/meta-rules which might lead to malevolent behavior in AIs (\\\"Ozkural, 2014) or reviewed specific undesirable behaviors AGIs can exhibit at different stages of its development (Alexey Turchin, July 10 2015, July 10, 2015).",
		"DOI": "10.48550/arXiv.1511.03246",
		"note": "arXiv:1511.03246 [cs]",
		"number": "arXiv:1511.03246",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Taxonomy of Pathways to Dangerous AI",
		"URL": "http://arxiv.org/abs/1511.03246",
		"author": [
			{
				"family": "Yampolskiy",
				"given": "Roman V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					11,
					11
				]
			]
		}
	},
	{
		"id": "pistonoUnethicalResearchHow2016",
		"type": "article",
		"abstract": "Cybersecurity research involves publishing papers about malicious exploits as much as publishing information on how to design tools to protect cyber-infrastructure. It is this information exchange between ethical hackers and security experts, which results in a well-balanced cyber-ecosystem. In the blooming domain of AI Safety Engineering, hundreds of papers have been published on different proposals geared at the creation of a safe machine, yet nothing, to our knowledge, has been published on how to design a malevolent machine. Availability of such information would be of great value particularly to computer scientists, mathematicians, and others who have an interest in AI safety, and who are attempting to avoid the spontaneous emergence or the deliberate creation of a dangerous AI, which can negatively affect human activities and in the worst case cause the complete obliteration of the human species. This paper provides some general guidelines for the creation of a Malevolent Artificial Intelligence (MAI).",
		"DOI": "10.48550/arXiv.1605.02817",
		"note": "arXiv:1605.02817 [cs]",
		"number": "arXiv:1605.02817",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Unethical Research: How to Create a Malevolent Artificial Intelligence",
		"title-short": "Unethical Research",
		"URL": "http://arxiv.org/abs/1605.02817",
		"author": [
			{
				"family": "Pistono",
				"given": "Federico"
			},
			{
				"family": "Yampolskiy",
				"given": "Roman V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					9,
					1
				]
			]
		}
	},
	{
		"id": "sotalaResponsesCatastrophicAGI2014",
		"type": "article-journal",
		"abstract": "Many researchers have argued that humanity will create artificial general intelligence (AGI) within the next twenty to one hundred years. It has been suggested that AGI may inflict serious damage to human well-being on a global scale (‘catastrophic risk’). After summarizing the arguments for why AGI may pose such a risk, we review the fieldʼs proposed responses to AGI risk. We consider societal proposals, proposals for external constraints on AGI behaviors and proposals for creating AGIs that are safe due to their internal design.",
		"container-title": "Physica Scripta",
		"DOI": "10.1088/0031-8949/90/1/018001",
		"ISSN": "1402-4896",
		"issue": "1",
		"journalAbbreviation": "Phys. Scr.",
		"language": "en",
		"note": "publisher: IOP Publishing",
		"page": "018001",
		"source": "Institute of Physics",
		"title": "Responses to catastrophic AGI risk: a survey",
		"title-short": "Responses to catastrophic AGI risk",
		"URL": "https://dx.doi.org/10.1088/0031-8949/90/1/018001",
		"volume": "90",
		"author": [
			{
				"family": "Sotala",
				"given": "Kaj"
			},
			{
				"family": "Yampolskiy",
				"given": "Roman V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					12
				]
			]
		}
	},
	{
		"id": "ngPolicyInvarianceReward1999",
		"type": "paper-conference",
		"abstract": "This paper investigates conditions under which modifications to the reward function of a Markov decision process preserve the optimal policy. It is shown, that besides the positive linear transformation familiar from utility theory, one can add a reward for transitions between states that is expressible as the difference in value of an arbitrary potential function applied to those states. Furthermore, this is shown to be a necessary condition for invariance, in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP. These results shed light on the practice of reward shaping, a method used in reinforcement learning whereby additional training rewards are used to guide the learning agent. In particular, some well-known \"bugs\" in reward shaping procedures are shown to arise non-potential-based rewards, and methods are given for constructing shaping potentials corresponding to distance-based and subgoal-based heuristics. We show that such potentials can lead to substantial reductions in learning time.",
		"collection-title": "ICML '99",
		"container-title": "Proceedings of the Sixteenth International Conference on Machine Learning",
		"event-place": "San Francisco, CA, USA",
		"ISBN": "978-1-55860-612-8",
		"page": "278–287",
		"publisher": "Morgan Kaufmann Publishers Inc.",
		"publisher-place": "San Francisco, CA, USA",
		"source": "ACM Digital Library",
		"title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping",
		"title-short": "Policy Invariance Under Reward Transformations",
		"author": [
			{
				"family": "Ng",
				"given": "Andrew Y."
			},
			{
				"family": "Harada",
				"given": "Daishi"
			},
			{
				"family": "Russell",
				"given": "Stuart J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1999",
					6,
					27
				]
			]
		}
	},
	{
		"id": "yampolskiyArtificialIntelligenceSafety2013",
		"type": "chapter",
		"abstract": "Machine ethics and robot rights are quickly becoming hot topics in artificial intelligence/robotics communities. We will argue that the attempts to allow machines to make ethical decisions or to have rights are misguided. Instead we propose a new science of safety engineering for intelligent artificial agents. In particular we issue a challenge to the scientific community to develop intelligent systems capable of proving that they are in fact safe even under recursive self-improvement.",
		"collection-title": "Studies in Applied Philosophy, Epistemology and Rational Ethics",
		"container-title": "Philosophy and Theory of Artificial Intelligence",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-642-31674-6",
		"language": "en",
		"note": "DOI: 10.1007/978-3-642-31674-6_29",
		"page": "389-396",
		"publisher": "Springer",
		"publisher-place": "Berlin, Heidelberg",
		"source": "Springer Link",
		"title": "Artificial Intelligence Safety Engineering: Why Machine Ethics Is a Wrong Approach",
		"title-short": "Artificial Intelligence Safety Engineering",
		"URL": "https://doi.org/10.1007/978-3-642-31674-6_29",
		"author": [
			{
				"family": "Yampolskiy",
				"given": "Roman V."
			}
		],
		"editor": [
			{
				"family": "Müller",
				"given": "Vincent C."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013"
				]
			]
		}
	},
	{
		"id": "yampolskiySafetyEngineeringArtificial2012",
		"type": "article-journal",
		"abstract": "Machine ethics and robot rights are quickly becoming hot topics in artificial intelligence and robotics communities. We will argue that attempts to attribute moral agency and assign rights to all intelligent machines are misguided, whether applied to infrahuman or superhuman AIs, as are proposals to limit the negative eﬀects of AIs by constraining their behavior. As an alternative, we propose a new science of safety engineering for intelligent artificial agents based on maximizing for what humans value. In particular, we challenge the scientific community to develop intelligent systems that have human-friendly values that they provably retain, even under recursive self-improvement.",
		"container-title": "Topoi",
		"DOI": "10.1007/s11245-012-9128-9",
		"ISSN": "0167-7411, 1572-8749",
		"journalAbbreviation": "Topoi",
		"language": "en",
		"source": "DOI.org (Crossref)",
		"title": "Safety Engineering for Artificial General Intelligence",
		"URL": "http://link.springer.com/10.1007/s11245-012-9128-9",
		"author": [
			{
				"family": "Yampolskiy",
				"given": "Roman"
			},
			{
				"family": "Fox",
				"given": "Joshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					16
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					8,
					24
				]
			]
		}
	},
	{
		"id": "yampolskiyLeakproofingSingularityArtificial2012",
		"type": "article-journal",
		"abstract": "This paper attempts to formalize and to address the 'leakproofing' of the Singularity problem presented by David Chalmers. The paper begins with the definition of the Artificial Intelligence Confinement Problem. After analysis of existing solutions and their shortcomings, a protocol is proposed aimed at making a more secure confinement environment which might delay potential negative effect from the technological singularity while allowing humanity to benefit from the superintelligence. (PsycINFO Database Record (c) 2017 APA, all rights reserved)",
		"container-title": "Journal of Consciousness Studies",
		"ISSN": "2051-2201",
		"issue": "1-2",
		"note": "publisher-place: US\npublisher: Imprint Academic",
		"page": "194-214",
		"source": "APA PsycNet",
		"title": "Leakproofing the Singularity: Artificial intelligence confinement problem",
		"title-short": "Leakproofing the Singularity",
		"volume": "19",
		"author": [
			{
				"family": "Yampolskiy",
				"given": "Roman V."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "abbeelApprenticeshipLearningInverse2004",
		"type": "paper-conference",
		"abstract": "We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using \"inverse reinforcement learning\" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.",
		"collection-title": "ICML '04",
		"container-title": "Proceedings of the twenty-first international conference on Machine learning",
		"DOI": "10.1145/1015330.1015430",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-58113-838-2",
		"page": "1",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Apprenticeship learning via inverse reinforcement learning",
		"URL": "https://dl.acm.org/doi/10.1145/1015330.1015430",
		"author": [
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Ng",
				"given": "Andrew Y."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2004",
					7,
					4
				]
			]
		}
	},
	{
		"id": "kuleshovInverseGameTheory2015a",
		"type": "paper-conference",
		"abstract": "One of the central questions in game theory deals with predicting the behavior of an agent. Here, we study the inverse of this problem: given the agents’ equilibrium behavior, what are possible utilities that motivate this behavior? We consider this problem in arbitrary normal-form games in which the utilities can be represented by a small number of parameters, such as in graphical, congestion, and network design games. In all such settings, we show how to efficiently, i.e. in polynomial time, determine utilities consistent with a given correlated equilibrium. However, inferring both utilities and structural elements (e.g., the graph within a graphical game) is in general NP-hard. From a theoretical perspective our results show that rationalizing an equilibrium is computationally easier than computing it; from a practical perspective a practitioner can use our algorithms to validate behavioral models.",
		"collection-title": "Lecture Notes in Computer Science",
		"container-title": "Web and Internet Economics",
		"DOI": "10.1007/978-3-662-48995-6_30",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-662-48995-6",
		"language": "en",
		"page": "413-427",
		"publisher": "Springer",
		"publisher-place": "Berlin, Heidelberg",
		"source": "Springer Link",
		"title": "Inverse Game Theory: Learning Utilities in Succinct Games",
		"title-short": "Inverse Game Theory",
		"author": [
			{
				"family": "Kuleshov",
				"given": "Volodymyr"
			},
			{
				"family": "Schrijvers",
				"given": "Okke"
			}
		],
		"editor": [
			{
				"family": "Markakis",
				"given": "Evangelos"
			},
			{
				"family": "Schäfer",
				"given": "Guido"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "spearsAssuringBehaviorAdaptive2006",
		"type": "chapter",
		"abstract": "Agents are becoming increasingly prevalent and effective. Robots and softbots, working individually or in concert, can relieve people of a great deal of labor-intensive tedium. Designers can furnish agents with plans to perform desired tasks. Nevertheless, a designer cannot possibly foresee all circumstances that will be encountered by the agent. Therefore, in addition to supplying an agent with a plan, it is essential to also enable the agent to learn and modify its plan to adapt to unforeseen circumstances. The introduction of learning, however, often makes the agent’s behavior significantly harder to predict.1 The goal of this research is to verify the behavior of adaptive agents. In particular, our objective is to develop efficient methods for determining whether the behavior of learning agents remains within the bounds of prespecified constraints (called “properties”) after learning. This includes verifying that properties are preserved for single adaptive agents as well as verifying that global properties are preserved for multi-agent systems in which one or more agents may adapt.",
		"collection-title": "NASA Monographs in Systems and Software Engineering",
		"container-title": "Agent Technology from a Formal Perspective",
		"event-place": "London",
		"ISBN": "978-1-84628-271-3",
		"language": "en",
		"note": "DOI: 10.1007/1-84628-271-3_8",
		"page": "227-257",
		"publisher": "Springer",
		"publisher-place": "London",
		"source": "Springer Link",
		"title": "Assuring the Behavior of Adaptive Agents",
		"URL": "https://doi.org/10.1007/1-84628-271-3_8",
		"author": [
			{
				"family": "Spears",
				"given": "Diana F."
			}
		],
		"editor": [
			{
				"family": "Rouff",
				"given": "Christopher A."
			},
			{
				"family": "Hinchey",
				"given": "Michael"
			},
			{
				"family": "Rash",
				"given": "James"
			},
			{
				"family": "Truszkowski",
				"given": "Walter"
			},
			{
				"family": "Gordon-Spears",
				"given": "Diana"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2006"
				]
			]
		}
	},
	{
		"id": "soaresCorrigibility2015",
		"type": "paper-conference",
		"abstract": "As artificially intelligent systems grow in intelligence and capability, some of their available options may allow them to resist intervention by their programmers. We call an AI system “corrigible” if it cooperates with what its creators regard\nas a corrective intervention, despite default incentives for rational agents to resist attempts to shut them down or modify their preferences. We introduce the notion of corrigibility and analyze utility functions that attempt to make an agent shut\ndown safely if a shutdown button is pressed, while avoiding incentives to prevent the button from being pressed or cause\nthe button to be pressed, and while ensuring propagation of the shutdown behavior as it creates new subsystems or selfmodifies. While some proposals are interesting, none have yet been demonstrated to satisfy all of our intuitive desiderata, leaving this simple problem in corrigibility wide-open.",
		"container-title": "Artificial Intelligence and Ethics: Papers from the 2015 AAAI Workshop",
		"event-place": "Austin, Texas, USA",
		"event-title": "The 29th Annual AAAI Conference on Artificial Intelligence",
		"publisher-place": "Austin, Texas, USA",
		"title": "Corrigibility",
		"URL": "https://cdn.aaai.org/ocs/ws/ws0067/10124-45900-1-PB.pdf",
		"author": [
			{
				"family": "Soares",
				"given": "Nate"
			},
			{
				"family": "Fallenstein",
				"given": "Benja"
			},
			{
				"family": "Yudkowsky",
				"given": "Eliezer"
			},
			{
				"family": "Armstrong",
				"given": "Stuart"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "hibbardAvoidingUnintendedAI2012",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) systems too complex for predefined environment models and actions will need to learn environment models and to choose actions that optimize some criteria. Several authors have described mechanisms by which such complex systems may behave in ways not intended in their designs. This paper describes ways to avoid such unintended behavior. For hypothesized powerful AI systems that may pose a threat to humans, this paper proposes a two-stage agent architecture that avoids some known types of unintended behavior. For the first stage of the architecture this paper shows that the most probable finite stochastic program to model a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions.",
		"collection-title": "Lecture Notes in Computer Science",
		"container-title": "Artificial General Intelligence",
		"DOI": "10.1007/978-3-642-35506-6_12",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-642-35506-6",
		"language": "en",
		"page": "107-116",
		"publisher": "Springer",
		"publisher-place": "Berlin, Heidelberg",
		"source": "Springer Link",
		"title": "Avoiding Unintended AI Behaviors",
		"author": [
			{
				"family": "Hibbard",
				"given": "Bill"
			}
		],
		"editor": [
			{
				"family": "Bach",
				"given": "Joscha"
			},
			{
				"family": "Goertzel",
				"given": "Ben"
			},
			{
				"family": "Iklé",
				"given": "Matthew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "adlerHardeningArtificialNeural2019",
		"type": "article",
		"abstract": "Context: Across different domains, Artificial Neural Networks (ANNs) are used more and more in safety-critical applications in which erroneous outputs of such ANN can have catastrophic consequences. However, the development of such neural networks is still immature and good engineering practices are missing. With that, ANNs are in the same position as software was several decades ago. Today, standards for functional safety, such as ISO 26262 in the automotive domain, require the application of a collection of proven engineering principles and methods in the creation of software to increase its quality and reduce failure rates to an acceptable level. Objective: In the future, such a set of proven engineering methods needs to be established for the development of Artificial Neural Networks to allow their use in safety-critical applications. Method: This work takes a step in this direction by conducting a mapping study to extract challenges faced in the development of ANNs for safety-critical applications and to identify methods that have been used for the hardening of ANNs in such settings. Results: We extracted ten different challenges found to be repeatedly reported in the literature regarding the use of ANNs in critical contexts. All of these challenges are addressed by engineering methods, of which we identified 54 in our study that can be used for the hardening of networks. Conclusions: Various methods have been proposed to overcome the specific challenges of using ANNs in safety-critical applications. On the path towards defining best practices, we envision that future software engineering will need to focus on further investigating these methods and increasing the maturity and understanding of existing approaches, with the goal to develop clear guidance for proper engineering of high-quality ANNs.",
		"DOI": "10.48550/arXiv.1909.03036",
		"note": "arXiv:1909.03036 [cs]",
		"number": "arXiv:1909.03036",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Hardening of Artificial Neural Networks for Use in Safety-Critical Applications -- A Mapping Study",
		"URL": "http://arxiv.org/abs/1909.03036",
		"author": [
			{
				"family": "Adler",
				"given": "Rasmus"
			},
			{
				"family": "Akram",
				"given": "Mohammed Naveed"
			},
			{
				"family": "Bauer",
				"given": "Pascal"
			},
			{
				"family": "Feth",
				"given": "Patrik"
			},
			{
				"family": "Gerber",
				"given": "Pascal"
			},
			{
				"family": "Jedlitschka",
				"given": "Andreas"
			},
			{
				"family": "Jöckel",
				"given": "Lisa"
			},
			{
				"family": "Kläs",
				"given": "Michael"
			},
			{
				"family": "Schneider",
				"given": "Daniel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					9,
					2
				]
			]
		}
	},
	{
		"id": "carliniEvaluatingRobustnessNeural2017",
		"type": "article",
		"abstract": "Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input $x$ and any target classification $t$, it is possible to find a new input $x'$ that is similar to $x$ but classified as $t$. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from $95\\%$ to $0.5\\%$. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with $100\\%$ probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.",
		"DOI": "10.48550/arXiv.1608.04644",
		"note": "arXiv:1608.04644 [cs]",
		"number": "arXiv:1608.04644",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Evaluating the Robustness of Neural Networks",
		"URL": "http://arxiv.org/abs/1608.04644",
		"author": [
			{
				"family": "Carlini",
				"given": "Nicholas"
			},
			{
				"family": "Wagner",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					3,
					22
				]
			]
		}
	},
	{
		"id": "kurdEstablishingSafetyCriteria2003",
		"type": "paper-conference",
		"abstract": "Artificial neural networks are employed in many areas of industry such as medicine and defence. There are many techniques that aim to improve the performance of neural networks for safety-critical systems. However, there is a complete absence of analytical certification methods for neural network paradigms. Consequently, their role in safety-critical applications, if any, is typically restricted to advisory systems. It is therefore desirable to enable neural networks for highly-dependable roles. This paper defines the safety criteria which if enforced, would contribute to justifying the safety of neural networks. The criteria are a set of safety requirements for the behaviour of neural networks. The paper also highlights the challenge of maintaining performance in terms of adaptability and generalisation whilst providing acceptable safety arguments.",
		"collection-title": "Lecture Notes in Computer Science",
		"container-title": "Knowledge-Based Intelligent Information and Engineering Systems",
		"DOI": "10.1007/978-3-540-45224-9_24",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-540-45224-9",
		"language": "en",
		"page": "163-169",
		"publisher": "Springer",
		"publisher-place": "Berlin, Heidelberg",
		"source": "Springer Link",
		"title": "Establishing Safety Criteria for Artificial Neural Networks",
		"author": [
			{
				"family": "Kurd",
				"given": "Zeshan"
			},
			{
				"family": "Kelly",
				"given": "Tim"
			}
		],
		"editor": [
			{
				"family": "Palade",
				"given": "Vasile"
			},
			{
				"family": "Howlett",
				"given": "Robert J."
			},
			{
				"family": "Jain",
				"given": "Lakhmi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2003"
				]
			]
		}
	},
	{
		"id": "madryDeepLearningModels2019",
		"type": "article",
		"abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.",
		"DOI": "10.48550/arXiv.1706.06083",
		"note": "arXiv:1706.06083 [cs, stat]",
		"number": "arXiv:1706.06083",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
		"URL": "http://arxiv.org/abs/1706.06083",
		"author": [
			{
				"family": "Madry",
				"given": "Aleksander"
			},
			{
				"family": "Makelov",
				"given": "Aleksandar"
			},
			{
				"family": "Schmidt",
				"given": "Ludwig"
			},
			{
				"family": "Tsipras",
				"given": "Dimitris"
			},
			{
				"family": "Vladu",
				"given": "Adrian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					9,
					4
				]
			]
		}
	},
	{
		"id": "wongProvableDefensesAdversarial2018",
		"type": "paper-conference",
		"abstract": "We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8% test error for any adversarial attack with bounded $\\ell_\\infty$ norm less than $\\epsilon = 0.1$).",
		"container-title": "Proceedings of the 35th International Conference on Machine Learning",
		"event-title": "International Conference on Machine Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "5286-5295",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope",
		"URL": "https://proceedings.mlr.press/v80/wong18a.html",
		"author": [
			{
				"family": "Wong",
				"given": "Eric"
			},
			{
				"family": "Kolter",
				"given": "Zico"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					7,
					3
				]
			]
		}
	},
	{
		"id": "sumersHowTalkAI2022",
		"type": "article",
		"abstract": "From the earliest years of our lives, humans use language to express our beliefs and desires. Being able to talk to artificial agents about our preferences would thus fulfill a central goal of value alignment. Yet today, we lack computational models explaining such language use. To address this challenge, we formalize learning from language in a contextual bandit setting and ask how a human might communicate preferences over behaviors. We study two distinct types of language: $\\textit{instructions}$, which provide information about the desired policy, and $\\textit{descriptions}$, which provide information about the reward function. We show that the agent's degree of autonomy determines which form of language is optimal: instructions are better in low-autonomy settings, but descriptions are better when the agent will need to act independently. We then define a pragmatic listener agent that robustly infers the speaker's reward function by reasoning about $\\textit{how}$ the speaker expresses themselves. We validate our models with a behavioral experiment, demonstrating that (1) our speaker model predicts human behavior, and (2) our pragmatic listener successfully recovers humans' reward functions. Finally, we show that this form of social learning can integrate with and reduce regret in traditional reinforcement learning. We hope these insights facilitate a shift from developing agents that $\\textit{obey}$ language to agents that $\\textit{learn}$ from it.",
		"note": "arXiv:2206.07870 [cs]",
		"number": "arXiv:2206.07870",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "How to talk so AI will learn: Instructions, descriptions, and autonomy",
		"title-short": "How to talk so AI will learn",
		"URL": "http://arxiv.org/abs/2206.07870",
		"author": [
			{
				"family": "Sumers",
				"given": "Theodore R."
			},
			{
				"family": "Hawkins",
				"given": "Robert D."
			},
			{
				"family": "Ho",
				"given": "Mark K."
			},
			{
				"family": "Griffiths",
				"given": "Thomas L."
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					10
				]
			]
		}
	},
	{
		"id": "leikeScalableAgentAlignment2018",
		"type": "article",
		"abstract": "One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.",
		"note": "arXiv:1811.07871 [cs, stat]",
		"number": "arXiv:1811.07871",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Scalable agent alignment via reward modeling: a research direction",
		"title-short": "Scalable agent alignment via reward modeling",
		"URL": "http://arxiv.org/abs/1811.07871",
		"author": [
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Everitt",
				"given": "Tom"
			},
			{
				"family": "Martic",
				"given": "Miljan"
			},
			{
				"family": "Maini",
				"given": "Vishal"
			},
			{
				"family": "Legg",
				"given": "Shane"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					11,
					19
				]
			]
		}
	},
	{
		"id": "christianoDeepReinforcementLearning2023",
		"type": "article",
		"abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",
		"note": "arXiv:1706.03741 [cs, stat]",
		"number": "arXiv:1706.03741",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Deep reinforcement learning from human preferences",
		"URL": "http://arxiv.org/abs/1706.03741",
		"author": [
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Brown",
				"given": "Tom B."
			},
			{
				"family": "Martic",
				"given": "Miljan"
			},
			{
				"family": "Legg",
				"given": "Shane"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					17
				]
			]
		}
	},
	{
		"id": "djeumouTaskGuidedInverseReinforcement2021",
		"type": "article",
		"abstract": "We study the problem of inverse reinforcement learning (IRL), where the learning agent recovers a reward function using expert demonstrations. Most of the existing IRL techniques make the often unrealistic assumption that the agent has access to full information about the environment. We remove this assumption by developing an algorithm for IRL in partially observable Markov decision processes (POMDPs). The algorithm addresses several limitations of existing techniques that do not take the information asymmetry between the expert and the learner into account. First, it adopts causal entropy as the measure of the likelihood of the expert demonstrations as opposed to entropy in most existing IRL techniques, and avoids a common source of algorithmic complexity. Second, it incorporates task specifications expressed in temporal logic into IRL. Such specifications may be interpreted as side information available to the learner a priori in addition to the demonstrations and may reduce the information asymmetry. Nevertheless, the resulting formulation is still nonconvex due to the intrinsic nonconvexity of the so-called forward problem, i.e., computing an optimal policy given a reward function, in POMDPs. We address this nonconvexity through sequential convex programming and introduce several extensions to solve the forward problem in a scalable manner. This scalability allows computing policies that incorporate memory at the expense of added computational cost yet also outperform memoryless policies. We demonstrate that, even with severely limited data, the algorithm learns reward functions and policies that satisfy the task and induce a similar behavior to the expert by leveraging the side information and incorporating memory into the policy.",
		"note": "arXiv:2105.14073 [cs, math]",
		"number": "arXiv:2105.14073",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Task-Guided Inverse Reinforcement Learning Under Partial Information",
		"URL": "http://arxiv.org/abs/2105.14073",
		"author": [
			{
				"family": "Djeumou",
				"given": "Franck"
			},
			{
				"family": "Cubuktepe",
				"given": "Murat"
			},
			{
				"family": "Lennon",
				"given": "Craig"
			},
			{
				"family": "Topcu",
				"given": "Ufuk"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					16
				]
			]
		}
	},
	{
		"id": "peschlMORALAligningAI2021",
		"type": "article",
		"abstract": "Inferring reward functions from demonstrations and pairwise preferences are auspicious approaches for aligning Reinforcement Learning (RL) agents with human intentions. However, state-of-the art methods typically focus on learning a single reward model, thus rendering it difficult to trade off different reward functions from multiple experts. We propose Multi-Objective Reinforced Active Learning (MORAL), a novel method for combining diverse demonstrations of social norms into a Pareto-optimal policy. Through maintaining a distribution over scalarization weights, our approach is able to interactively tune a deep RL agent towards a variety of preferences, while eliminating the need for computing multiple policies. We empirically demonstrate the effectiveness of MORAL in two scenarios, which model a delivery and an emergency task that require an agent to act in the presence of normative conflicts. Overall, we consider our research a step towards multi-objective RL with learned rewards, bridging the gap between current reward learning and machine ethics literature.",
		"note": "arXiv:2201.00012 [cs]",
		"number": "arXiv:2201.00012",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "MORAL: Aligning AI with Human Norms through Multi-Objective Reinforced Active Learning",
		"title-short": "MORAL",
		"URL": "http://arxiv.org/abs/2201.00012",
		"author": [
			{
				"family": "Peschl",
				"given": "Markus"
			},
			{
				"family": "Zgonnikov",
				"given": "Arkady"
			},
			{
				"family": "Oliehoek",
				"given": "Frans A."
			},
			{
				"family": "Siebert",
				"given": "Luciano C."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					12,
					30
				]
			]
		}
	},
	{
		"id": "sannemanTransparentValueAlignment2023",
		"type": "paper-conference",
		"abstract": "As robots become increasingly prevalent in our communities, aligning the values motivating their behavior with human values is critical. However, it is often difficult or impossible for humans, both expert and non-expert, to enumerate values comprehensively, accurately, and in forms that are readily usable for robot planning. Misspecification can lead to undesired, inefficient, or even dangerous behavior. In the value alignment problem, humans and robots work together to optimize human objectives, which are often represented as reward functions and which the robot can infer by observing human actions. In existing alignment approaches, no explicit feedback about this inference process is provided to the human. In this paper, we introduce an exploratory framework to address this problem, which we call Transparent Value Alignment (TVA). TVA suggests that techniques from explainable AI (XAI) be explicitly applied to provide humans with information about the robot’s beliefs throughout learning, enabling efficient and effective human feedback.",
		"container-title": "Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction",
		"DOI": "10.1145/3568294.3580147",
		"event-place": "Stockholm Sweden",
		"event-title": "HRI '23: ACM/IEEE International Conference on Human-Robot Interaction",
		"ISBN": "978-1-4503-9970-8",
		"language": "en",
		"page": "557-560",
		"publisher": "ACM",
		"publisher-place": "Stockholm Sweden",
		"source": "DOI.org (Crossref)",
		"title": "Transparent Value Alignment",
		"URL": "https://dl.acm.org/doi/10.1145/3568294.3580147",
		"author": [
			{
				"family": "Sanneman",
				"given": "Lindsay"
			},
			{
				"family": "Shah",
				"given": "Julie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					13
				]
			]
		}
	},
	{
		"id": "ghoshDeploymentRobustCooperative2020a",
		"type": "article-journal",
		"abstract": "We study the problem of designing an AI agent that can robustly cooperate with agents of unknown type (i.e., previously unobserved behavior) in multi-agent scenarios. Our work is inspired by realworld applications in which an AI agent, e.g., a virtual assistant, has to cooperate with new types of agents/users after its deployment. We model this problem via parametric Markov Decision Processes where the parameters correspond to a user’s type and characterize her behavior. In the test phase, the AI agent has to interact with a user of an unknown type. We develop an algorithmic framework for learning adaptive policies: our approach relies on observing the user’s actions to make inferences about the user’s type and adapting the policy to facilitate efficient cooperation. We show that without being adaptive, an AI agent can end up performing arbitrarily bad in the test phase. Using our framework, we propose two concrete algorithms for computing policies that automatically adapt to the user in the test phase. We demonstrate the effectiveness of our algorithms in a cooperative gathering game environment for two agents.",
		"container-title": "New Zealand",
		"language": "en",
		"source": "Zotero",
		"title": "Towards Deployment of Robust Cooperative AI Agents: An Algorithmic Framework for Learning Adaptive Policies",
		"author": [
			{
				"family": "Ghosh",
				"given": "Ahana"
			},
			{
				"family": "Tschiatschek",
				"given": "Sebastian"
			},
			{
				"family": "Mahdavi",
				"given": "Hamed"
			},
			{
				"family": "Singla",
				"given": "Adish"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "adebayoSanityChecksSaliency2020",
		"type": "article",
		"abstract": "Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.",
		"DOI": "10.48550/arXiv.1810.03292",
		"note": "arXiv:1810.03292 [cs, stat]",
		"number": "arXiv:1810.03292",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Sanity Checks for Saliency Maps",
		"URL": "http://arxiv.org/abs/1810.03292",
		"author": [
			{
				"family": "Adebayo",
				"given": "Julius"
			},
			{
				"family": "Gilmer",
				"given": "Justin"
			},
			{
				"family": "Muelly",
				"given": "Michael"
			},
			{
				"family": "Goodfellow",
				"given": "Ian"
			},
			{
				"family": "Hardt",
				"given": "Moritz"
			},
			{
				"family": "Kim",
				"given": "Been"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					11,
					6
				]
			]
		}
	},
	{
		"id": "balajiMetaRegDomainGeneralization2018",
		"type": "paper-conference",
		"abstract": "Training models that generalize to new domains at test time is a problem of fundamental importance in machine learning. In this work, we encode this notion of domain generalization using a novel regularization function. We pose the problem of finding such a regularization function in a Learning to Learn (or) meta-learning framework. The objective of domain generalization is explicitly modeled by learning a regularizer that makes the model trained on one domain to perform well on another domain. Experimental validations on computer vision and natural language datasets indicate that our method can learn regularizers that achieve good cross-domain generalization.",
		"container-title": "Advances in Neural Information Processing Systems",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "MetaReg: Towards Domain Generalization using Meta-Regularization",
		"title-short": "MetaReg",
		"URL": "https://papers.nips.cc/paper_files/paper/2018/hash/647bba344396e7c8170902bcf2e15551-Abstract.html",
		"volume": "31",
		"author": [
			{
				"family": "Balaji",
				"given": "Yogesh"
			},
			{
				"family": "Sankaranarayanan",
				"given": "Swami"
			},
			{
				"family": "Chellappa",
				"given": "Rama"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "bougueliaAgreeingDisagreeActive2018",
		"type": "article-journal",
		"abstract": "We propose a new active learning method for classification, which handles label noise without relying on multiple oracles (i.e., crowdsourcing). We propose a strategy that selects (for labeling) instances with a high influence on the learned model. An instance x is said to have a high influence on the model h, if training h on x (with label $$y = h(x)$$) would result in a model that greatly disagrees with h on labeling other instances. Then, we propose another strategy that selects (for labeling) instances that are highly influenced by changes in the learned model. An instance x is said to be highly influenced, if training h with a set of instances would result in a committee of models that agree on a common label for x but disagree with h(x). We compare the two strategies and we show, on different publicly available datasets, that selecting instances according to the first strategy while eliminating noisy labels according to the second strategy, greatly improves the accuracy compared to several benchmarking methods, even when a significant amount of instances are mislabeled.",
		"container-title": "International Journal of Machine Learning and Cybernetics",
		"DOI": "10.1007/s13042-017-0645-0",
		"ISSN": "1868-808X",
		"issue": "8",
		"journalAbbreviation": "Int. J. Mach. Learn. & Cyber.",
		"language": "en",
		"page": "1307-1319",
		"source": "Springer Link",
		"title": "Agreeing to disagree: active learning with noisy labels without crowdsourcing",
		"title-short": "Agreeing to disagree",
		"URL": "https://doi.org/10.1007/s13042-017-0645-0",
		"volume": "9",
		"author": [
			{
				"family": "Bouguelia",
				"given": "Mohamed-Rafik"
			},
			{
				"family": "Nowaczyk",
				"given": "Slawomir"
			},
			{
				"family": "Santosh",
				"given": "K. C."
			},
			{
				"family": "Verikas",
				"given": "Antanas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					8,
					1
				]
			]
		}
	},
	{
		"id": "chenATOMRobustifyingOutofdistribution2021",
		"type": "article",
		"abstract": "Detecting out-of-distribution (OOD) inputs is critical for safely deploying deep learning models in an open-world setting. However, existing OOD detection solutions can be brittle in the open world, facing various types of adversarial OOD inputs. While methods leveraging auxiliary OOD data have emerged, our analysis on illuminative examples reveals a key insight that the majority of auxiliary OOD examples may not meaningfully improve or even hurt the decision boundary of the OOD detector, which is also observed in empirical results on real data. In this paper, we provide a theoretically motivated method, Adversarial Training with informative Outlier Mining (ATOM), which improves the robustness of OOD detection. We show that, by mining informative auxiliary OOD data, one can significantly improve OOD detection performance, and somewhat surprisingly, generalize to unseen adversarial attacks. ATOM achieves state-of-the-art performance under a broad family of classic and adversarial OOD evaluation tasks. For example, on the CIFAR-10 in-distribution dataset, ATOM reduces the FPR (at TPR 95%) by up to 57.99% under adversarial OOD inputs, surpassing the previous best baseline by a large margin.",
		"DOI": "10.48550/arXiv.2006.15207",
		"note": "arXiv:2006.15207 [cs, stat]",
		"number": "arXiv:2006.15207",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "ATOM: Robustifying Out-of-distribution Detection Using Outlier Mining",
		"title-short": "ATOM",
		"URL": "http://arxiv.org/abs/2006.15207",
		"author": [
			{
				"family": "Chen",
				"given": "Jiefeng"
			},
			{
				"family": "Li",
				"given": "Yixuan"
			},
			{
				"family": "Wu",
				"given": "Xi"
			},
			{
				"family": "Liang",
				"given": "Yingyu"
			},
			{
				"family": "Jha",
				"given": "Somesh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					29
				]
			]
		}
	},
	{
		"id": "kaufmannTestingRobustnessUnforeseen2019",
		"type": "article",
		"abstract": "Adversarial robustness research primarily focuses on L_p perturbations, and most defenses are developed with identical training-time and test-time adversaries. However, in real-world applications developers are unlikely to have access to the full range of attacks or corruptions their system will face. Furthermore, worst-case inputs are likely to be diverse and need not be constrained to the L_p ball. To narrow in on this discrepancy between research and reality we introduce ImageNet-UA, a framework for evaluating model robustness against a range of unforeseen adversaries, including eighteen new non-L_p attacks. To perform well on ImageNet-UA, defenses must overcome a generalization gap and be robust to a diverse attacks not encountered during training. In extensive experiments, we find that existing robustness measures do not capture unforeseen robustness, that standard robustness techniques are beat by alternative training strategies, and that novel methods can improve unforeseen robustness. We present ImageNet-UA as a useful tool for the community for improving the worst-case behavior of machine learning systems.",
		"DOI": "10.48550/arXiv.1908.08016",
		"note": "arXiv:1908.08016 [cs, stat]",
		"number": "arXiv:1908.08016",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Testing Robustness Against Unforeseen Adversaries",
		"URL": "http://arxiv.org/abs/1908.08016",
		"author": [
			{
				"family": "Kaufmann",
				"given": "Max"
			},
			{
				"family": "Kang",
				"given": "Daniel"
			},
			{
				"family": "Sun",
				"given": "Yi"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Yin",
				"given": "Xuwang"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Arora",
				"given": "Akul"
			},
			{
				"family": "Dziedzic",
				"given": "Adam"
			},
			{
				"family": "Boenisch",
				"given": "Franziska"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			},
			{
				"family": "Hendrycks",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					10,
					30
				]
			]
		}
	},
	{
		"id": "lageHumanintheLoopInterpretabilityPrior2018",
		"type": "article",
		"abstract": "We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.",
		"note": "arXiv:1805.11571 [cs, stat]",
		"number": "arXiv:1805.11571",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Human-in-the-Loop Interpretability Prior",
		"URL": "http://arxiv.org/abs/1805.11571",
		"author": [
			{
				"family": "Lage",
				"given": "Isaac"
			},
			{
				"family": "Ross",
				"given": "Andrew Slavin"
			},
			{
				"family": "Kim",
				"given": "Been"
			},
			{
				"family": "Gershman",
				"given": "Samuel J."
			},
			{
				"family": "Doshi-Velez",
				"given": "Finale"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					30
				]
			]
		}
	},
	{
		"id": "lakkarajuIdentifyingUnknownUnknowns2016",
		"type": "article",
		"abstract": "Predictive models deployed in the real world may assign incorrect labels to instances with high confidence. Such errors or unknown unknowns are rooted in model incompleteness, and typically arise because of the mismatch between training data and the cases encountered at test time. As the models are blind to such errors, input from an oracle is needed to identify these failures. In this paper, we formulate and address the problem of informed discovery of unknown unknowns of any given predictive model where unknown unknowns occur due to systematic biases in the training data. We propose a model-agnostic methodology which uses feedback from an oracle to both identify unknown unknowns and to intelligently guide the discovery. We employ a two-phase approach which first organizes the data into multiple partitions based on the feature similarity of instances and the confidence scores assigned by the predictive model, and then utilizes an explore-exploit strategy for discovering unknown unknowns across these partitions. We demonstrate the efficacy of our framework by varying the underlying causes of unknown unknowns across various applications. To the best of our knowledge, this paper presents the first algorithmic approach to the problem of discovering unknown unknowns of predictive models.",
		"note": "arXiv:1610.09064 [cs]",
		"number": "arXiv:1610.09064",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration",
		"title-short": "Identifying Unknown Unknowns in the Open World",
		"URL": "http://arxiv.org/abs/1610.09064",
		"author": [
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			},
			{
				"family": "Kamar",
				"given": "Ece"
			},
			{
				"family": "Caruana",
				"given": "Rich"
			},
			{
				"family": "Horvitz",
				"given": "Eric"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					12,
					10
				]
			]
		}
	},
	{
		"id": "wangLearningRobustRepresentations2019",
		"type": "article",
		"abstract": "Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift. For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier. Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals. To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image. Then we introduce two techniques for improving our networks' out-of-sample performance. The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable. The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's. We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.",
		"note": "arXiv:1903.06256 [cs]",
		"number": "arXiv:1903.06256",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Robust Representations by Projecting Superficial Statistics Out",
		"URL": "http://arxiv.org/abs/1903.06256",
		"author": [
			{
				"family": "Wang",
				"given": "Haohan"
			},
			{
				"family": "He",
				"given": "Zexue"
			},
			{
				"family": "Lipton",
				"given": "Zachary C."
			},
			{
				"family": "Xing",
				"given": "Eric P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					3,
					1
				]
			]
		}
	},
	{
		"id": "tavakoliSPLASHLearnableActivation2020",
		"type": "article",
		"abstract": "We introduce SPLASH units, a class of learnable activation functions shown to simultaneously improve the accuracy of deep neural networks while also improving their robustness to adversarial attacks. SPLASH units have both a simple parameterization and maintain the ability to approximate a wide range of non-linear functions. SPLASH units are: 1) continuous; 2) grounded (f(0) = 0); 3) use symmetric hinges; and 4) the locations of the hinges are derived directly from the data (i.e. no learning required). Compared to nine other learned and fixed activation functions, including ReLU and its variants, SPLASH units show superior performance across three datasets (MNIST, CIFAR-10, and CIFAR-100) and four architectures (LeNet5, All-CNN, ResNet-20, and Network-in-Network). Furthermore, we show that SPLASH units significantly increase the robustness of deep neural networks to adversarial attacks. Our experiments on both black-box and open-box adversarial attacks show that commonly-used architectures, namely LeNet5, All-CNN, ResNet-20, and Network-in-Network, can be up to 31% more robust to adversarial attacks by simply using SPLASH units instead of ReLUs.",
		"DOI": "10.48550/arXiv.2006.08947",
		"note": "arXiv:2006.08947 [cs, stat]",
		"number": "arXiv:2006.08947",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SPLASH: Learnable Activation Functions for Improving Accuracy and Adversarial Robustness",
		"title-short": "SPLASH",
		"URL": "http://arxiv.org/abs/2006.08947",
		"author": [
			{
				"family": "Tavakoli",
				"given": "Mohammadamin"
			},
			{
				"family": "Agostinelli",
				"given": "Forest"
			},
			{
				"family": "Baldi",
				"given": "Pierre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					6,
					16
				]
			]
		}
	},
	{
		"id": "ningDiscoveringRobustConvolutional2021",
		"type": "article",
		"abstract": "Convolutional neural networks (CNNs) are vulnerable to adversarial examples, and studies show that increasing the model capacity of an architecture topology (e.g., width expansion) can bring consistent robustness improvements. This reveals a clear robustness-efficiency trade-off that should be considered in architecture design. In this paper, considering scenarios with capacity budget, we aim to discover adversarially robust architecture at targeted capacities. Recent studies employed one-shot neural architecture search (NAS) to discover robust architectures. However, since the capacities of different topologies cannot be aligned in the search process, one-shot NAS methods favor topologies with larger capacities in the supernet. And the discovered topology might be suboptimal when augmented to the targeted capacity. We propose a novel multi-shot NAS method to address this issue and explicitly search for robust architectures at targeted capacities. At the targeted FLOPs of 2000M, the discovered MSRobNet-2000 outperforms the recent NAS-discovered architecture RobNet-large under various criteria by a large margin of 4%-7%. And at the targeted FLOPs of 1560M, MSRobNet-1560 surpasses another NAS-discovered architecture RobNet-free by 2.3% and 1.3% in the clean and PGD-7 accuracies, respectively. All codes are available at https://github.com/walkerning/aw\\_nas.",
		"DOI": "10.48550/arXiv.2012.11835",
		"note": "arXiv:2012.11835 [cs]",
		"number": "arXiv:2012.11835",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Discovering Robust Convolutional Architecture at Targeted Capacity: A Multi-Shot Approach",
		"title-short": "Discovering Robust Convolutional Architecture at Targeted Capacity",
		"URL": "http://arxiv.org/abs/2012.11835",
		"author": [
			{
				"family": "Ning",
				"given": "Xuefei"
			},
			{
				"family": "Zhao",
				"given": "Junbo"
			},
			{
				"family": "Li",
				"given": "Wenshuo"
			},
			{
				"family": "Zhao",
				"given": "Tianchen"
			},
			{
				"family": "Zheng",
				"given": "Yin"
			},
			{
				"family": "Yang",
				"given": "Huazhong"
			},
			{
				"family": "Wang",
				"given": "Yu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					26
				]
			]
		}
	},
	{
		"id": "meinkeNeuralNetworksThat2020",
		"type": "article",
		"abstract": "It has recently been shown that ReLU networks produce arbitrarily over-confident predictions far away from the training data. Thus, ReLU networks do not know when they don't know. However, this is a highly important property in safety critical applications. In the context of out-of-distribution detection (OOD) there have been a number of proposals to mitigate this problem but none of them are able to make any mathematical guarantees. In this paper we propose a new approach to OOD which overcomes both problems. Our approach can be used with ReLU networks and provides provably low confidence predictions far away from the training data as well as the first certificates for low confidence predictions in a neighborhood of an out-distribution point. In the experiments we show that state-of-the-art methods fail in this worst-case setting whereas our model can guarantee its performance while retaining state-of-the-art OOD performance.",
		"DOI": "10.48550/arXiv.1909.12180",
		"note": "arXiv:1909.12180 [cs, stat]",
		"number": "arXiv:1909.12180",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards neural networks that provably know when they don't know",
		"URL": "http://arxiv.org/abs/1909.12180",
		"author": [
			{
				"family": "Meinke",
				"given": "Alexander"
			},
			{
				"family": "Hein",
				"given": "Matthias"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					2,
					21
				]
			]
		}
	},
	{
		"id": "elhage2022solu",
		"type": "article-journal",
		"abstract": "In this paper, we report an architectural change which appears to substantially increase the fraction of MLP neurons which appear to be \"interpretable\" (i.e. respond to an articulable property of the input), at little to no cost to ML performance. Specifically, we replace the activation function with a softmax linear unit (which we term SoLU) and show that this significantly increases the fraction of neurons in the MLP layers which seem to correspond to readily human-understandable concepts, phrases, or categories on quick investigation, as measured by randomized and blinded experiments. We then study our SoLU models and use them to gain several new insights about how information is processed in transformers.  However, we also discover some evidence that the superposition hypothesis is true and there is no free lunch: SoLU may be making some features more interpretable by “hiding” others and thus making them even more deeply uninterpretable.  Despite this, SoLU still seems like a net win, as in practical terms it substantially increases the fraction of neurons we are able to understand.",
		"container-title": "Transformer Circuits Thread",
		"title": "Softmax linear units",
		"URL": "https://transformer-circuits.pub/2022/solu/index.html",
		"author": [
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "ElShowk",
				"given": "Sheer"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Fort",
				"given": "Stanislav"
			},
			{
				"family": "Kadavath",
				"given": "Saurav"
			},
			{
				"family": "Jacobson",
				"given": "Josh"
			},
			{
				"family": "Tran-Johnson",
				"given": "Eli"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cammarata2020curve",
		"type": "article-journal",
		"abstract": "Every vision model we've explored in detail contains neurons which detect curves. Curve detectors in vision models have been hinted at in the literature as far back as 2013 (see figures in Zeiler & Fergus, and similar neurons have been studied carefully in neuroscience. We briefly discussed curve in our earlier overview of early vision, but wanted to examine them in more depth. This article is the first part of a three article deep dive into curve detectors: their behavior, how they're built from earlier neurons, and their prevalence across models.",
		"container-title": "Distill",
		"DOI": "10.23915/distill.00024.003",
		"title": "Curve detectors",
		"author": [
			{
				"family": "Cammarata",
				"given": "Nick"
			},
			{
				"family": "Goh",
				"given": "Gabriel"
			},
			{
				"family": "Carter",
				"given": "Shan"
			},
			{
				"family": "Schubert",
				"given": "Ludwig"
			},
			{
				"family": "Petrov",
				"given": "Michael"
			},
			{
				"family": "Olah",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "bauUnderstandingRoleIndividual2020",
		"type": "article-journal",
		"abstract": "Deep neural networks excel at finding hierarchical representations that solve complex tasks over large data sets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.",
		"container-title": "Proceedings of the National Academy of Sciences",
		"DOI": "10.1073/pnas.1907375117",
		"ISSN": "0027-8424, 1091-6490",
		"issue": "48",
		"journalAbbreviation": "Proc. Natl. Acad. Sci. U.S.A.",
		"note": "arXiv:2009.05041 [cs]",
		"page": "30071-30078",
		"source": "arXiv.org",
		"title": "Understanding the Role of Individual Units in a Deep Neural Network",
		"URL": "http://arxiv.org/abs/2009.05041",
		"volume": "117",
		"author": [
			{
				"family": "Bau",
				"given": "David"
			},
			{
				"family": "Zhu",
				"given": "Jun-Yan"
			},
			{
				"family": "Strobelt",
				"given": "Hendrik"
			},
			{
				"family": "Lapedriza",
				"given": "Agata"
			},
			{
				"family": "Zhou",
				"given": "Bolei"
			},
			{
				"family": "Torralba",
				"given": "Antonio"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12
				]
			]
		}
	},
	{
		"id": "donnellyInterpretabilityFeatureRepresentations2019",
		"type": "paper-conference",
		"abstract": "We are concerned with investigating the apparent effectiveness of Radford et al.’s “Sentiment Neuron,” [9] which they claim encapsulates sufficient knowledge to accurately predict sentiment in reviews. In our analysis of the Sentiment Neuron, we find that the removal of the neuron only marginally affects a classifier’s ability to detect and label sentiment and may even improve performance. Moreover, the effectiveness of the Sentiment Neuron can be surpassed by simply using 100 random neurons as features to the same classifier. Using adversarial examples, we show that the generated representation containing the Sentiment Neuron (i.e., the final hidden cell state in a LSTM) is particularly sensitive to the end of a processed sequence. Accordingly, we find that caution needs to be applied when interpreting neuron-based feature representations and potential flaws should be addressed for real-world applicability.",
		"collection-title": "Lecture Notes in Computer Science",
		"container-title": "Advances in Information Retrieval",
		"DOI": "10.1007/978-3-030-15712-8_55",
		"event-place": "Cham",
		"ISBN": "978-3-030-15712-8",
		"language": "en",
		"page": "795-802",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "Springer Link",
		"title": "On Interpretability and Feature Representations: An Analysis of the Sentiment Neuron",
		"title-short": "On Interpretability and Feature Representations",
		"author": [
			{
				"family": "Donnelly",
				"given": "Jonathan"
			},
			{
				"family": "Roegiest",
				"given": "Adam"
			}
		],
		"editor": [
			{
				"family": "Azzopardi",
				"given": "Leif"
			},
			{
				"family": "Stein",
				"given": "Benno"
			},
			{
				"family": "Fuhr",
				"given": "Norbert"
			},
			{
				"family": "Mayr",
				"given": "Philipp"
			},
			{
				"family": "Hauff",
				"given": "Claudia"
			},
			{
				"family": "Hiemstra",
				"given": "Djoerd"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "schubert2021high-low",
		"type": "article-journal",
		"abstract": "Some of the neurons in vision models are features that we aren’t particularly surprised to find. Curve detectors, for example, are a pretty natural feature for a vision system to have. In fact, they had already been discovered in the animal visual cortex. It’s easy to imagine how curve detectors are built up from earlier edge detectors, and it’s easy to guess why curve detection might be useful to the rest of the neural network. High-low frequency detectors, on the other hand, seem more surprising. They are not a feature that we would have expected a priori to find. Yet, when systematically characterizing\n the early layers of InceptionV1, we found a full fifteen neurons of mixed3a that appear to detect a high frequency pattern on one side, and a low frequency pattern on the other. One worry we might have about the circuits approach to studying neural networks is that we might only be able to understand a limited set of highly-intuitive features. High-low frequency detectors demonstrate that it’s possible to understand at least somewhat unintuitive features.",
		"container-title": "Distill",
		"DOI": "10.23915/distill.00024.005",
		"title": "High-low frequency detectors",
		"author": [
			{
				"family": "Schubert",
				"given": "Ludwig"
			},
			{
				"family": "Voss",
				"given": "Chelsea"
			},
			{
				"family": "Cammarata",
				"given": "Nick"
			},
			{
				"family": "Goh",
				"given": "Gabriel"
			},
			{
				"family": "Olah",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "nandaProgressMeasuresGrokking2023",
		"type": "article",
		"abstract": "Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \\textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.",
		"DOI": "10.48550/arXiv.2301.05217",
		"note": "arXiv:2301.05217 [cs]",
		"number": "arXiv:2301.05217",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Progress measures for grokking via mechanistic interpretability",
		"URL": "http://arxiv.org/abs/2301.05217",
		"author": [
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Chan",
				"given": "Lawrence"
			},
			{
				"family": "Lieberum",
				"given": "Tom"
			},
			{
				"family": "Smith",
				"given": "Jess"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					19
				]
			]
		}
	},
	{
		"id": "liuDelvingTransferableAdversarial2017",
		"type": "article",
		"abstract": "An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.",
		"DOI": "10.48550/arXiv.1611.02770",
		"note": "arXiv:1611.02770 [cs]",
		"number": "arXiv:1611.02770",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Delving into Transferable Adversarial Examples and Black-box Attacks",
		"URL": "http://arxiv.org/abs/1611.02770",
		"author": [
			{
				"family": "Liu",
				"given": "Yanpei"
			},
			{
				"family": "Chen",
				"given": "Xinyun"
			},
			{
				"family": "Liu",
				"given": "Chang"
			},
			{
				"family": "Song",
				"given": "Dawn"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					2,
					7
				]
			]
		}
	},
	{
		"id": "olah2018the",
		"type": "article-journal",
		"abstract": "In this article, we treat existing interpretability methods as fundamental and composable building blocks for rich user interfaces. We find that these disparate techniques now come together in a unified grammar, fulfilling complementary roles in the resulting interfaces. Moreover, this grammar allows us to systematically explore the space of interpretability interfaces, enabling us to evaluate whether they meet particular goals. We will present interfaces that show what the network detects and explain how it develops its understanding, while keeping the amount of information human-scale. For example, we will see how a network looking at a labrador retriever detects floppy ears and how that influences its classification. In this article, we use GoogLeNet\nGoing deeper with convolutions, an image classification model, to demonstrate our interface ideas because its neurons seem unusually semantically meaningful. Although here we’ve made a specific choice of task and network, the basic abstractions and patterns for combining them that we present can be applied to neural networks in other domains.",
		"container-title": "Distill",
		"DOI": "10.23915/distill.00010",
		"title": "The building blocks of interpretability",
		"author": [
			{
				"family": "Olah",
				"given": "Chris"
			},
			{
				"family": "Satyanarayan",
				"given": "Arvind"
			},
			{
				"family": "Johnson",
				"given": "Ian"
			},
			{
				"family": "Carter",
				"given": "Shan"
			},
			{
				"family": "Schubert",
				"given": "Ludwig"
			},
			{
				"family": "Ye",
				"given": "Katherine"
			},
			{
				"family": "Mordvintsev",
				"given": "Alexander"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018",
					3
				]
			]
		}
	},
	{
		"id": "subramanianSPINESParseInterpretable2017",
		"type": "article",
		"abstract": "Prediction without justification has limited utility. Much of the success of neural models can be attributed to their ability to learn rich, dense and expressive representations. While these representations capture the underlying complexity and latent trends in the data, they are far from being interpretable. We propose a novel variant of denoising k-sparse autoencoders that generates highly efficient and interpretable distributed word representations (word embeddings), beginning with existing word representations from state-of-the-art methods like GloVe and word2vec. Through large scale human evaluation, we report that our resulting word embedddings are much more interpretable than the original GloVe and word2vec embeddings. Moreover, our embeddings outperform existing popular word embeddings on a diverse suite of benchmark downstream tasks.",
		"DOI": "10.48550/arXiv.1711.08792",
		"note": "arXiv:1711.08792 [cs]",
		"number": "arXiv:1711.08792",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SPINE: SParse Interpretable Neural Embeddings",
		"title-short": "SPINE",
		"URL": "http://arxiv.org/abs/1711.08792",
		"author": [
			{
				"family": "Subramanian",
				"given": "Anant"
			},
			{
				"family": "Pruthi",
				"given": "Danish"
			},
			{
				"family": "Jhamtani",
				"given": "Harsh"
			},
			{
				"family": "Berg-Kirkpatrick",
				"given": "Taylor"
			},
			{
				"family": "Hovy",
				"given": "Eduard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					11,
					23
				]
			]
		}
	},
	{
		"id": "kimDisentanglingFactorising2019",
		"type": "article",
		"abstract": "We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon $\\beta$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.",
		"DOI": "10.48550/arXiv.1802.05983",
		"note": "arXiv:1802.05983 [cs, stat]",
		"number": "arXiv:1802.05983",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Disentangling by Factorising",
		"URL": "http://arxiv.org/abs/1802.05983",
		"author": [
			{
				"family": "Kim",
				"given": "Hyunjik"
			},
			{
				"family": "Mnih",
				"given": "Andriy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					7,
					9
				]
			]
		}
	},
	{
		"id": "dunionTemporalDisentanglementRepresentations2023",
		"type": "article",
		"abstract": "Reinforcement Learning (RL) agents are often unable to generalise well to environment variations in the state space that were not observed during training. This issue is especially problematic for image-based RL, where a change in just one variable, such as the background colour, can change many pixels in the image. The changed pixels can lead to drastic changes in the agent's latent representation of the image, causing the learned policy to fail. To learn more robust representations, we introduce TEmporal Disentanglement (TED), a self-supervised auxiliary task that leads to disentangled image representations exploiting the sequential nature of RL observations. We find empirically that RL algorithms utilising TED as an auxiliary task adapt more quickly to changes in environment variables with continued training compared to state-of-the-art representation learning methods. Since TED enforces a disentangled structure of the representation, our experiments also show that policies trained with TED generalise better to unseen values of variables irrelevant to the task (e.g. background colour) as well as unseen values of variables that affect the optimal policy (e.g. goal positions).",
		"note": "arXiv:2207.05480 [cs]",
		"number": "arXiv:2207.05480",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2207.05480",
		"author": [
			{
				"family": "Dunion",
				"given": "Mhairi"
			},
			{
				"family": "McInroe",
				"given": "Trevor"
			},
			{
				"family": "Luck",
				"given": "Kevin Sebastian"
			},
			{
				"family": "Hanna",
				"given": "Josiah P."
			},
			{
				"family": "Albrecht",
				"given": "Stefano V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					27
				]
			]
		}
	},
	{
		"id": "murphyLearningEffectiveInterpretable2012",
		"type": "paper-conference",
		"abstract": "In this paper, we introduce an application of matrix factorization to produce corpus-derived, distributional models of semantics that demonstrate cognitive plausibility. We find that word representations learned by Non-Negative Sparse Embedding (NNSE), a variant of matrix factorization, are sparse, effective, and highly interpretable. To the best of our knowledge, this is the first approach which yields semantic representation of words satisfying these three desirable properties. Though extensive experimental evaluations on multiple real-world tasks and datasets, we demonstrate the superiority of semantic models learned by NNSE over other state-of-the-art baselines.",
		"container-title": "Proceedings of COLING 2012",
		"event-place": "Mumbai, India",
		"event-title": "COLING 2012",
		"page": "1933–1950",
		"publisher": "The COLING 2012 Organizing Committee",
		"publisher-place": "Mumbai, India",
		"source": "ACLWeb",
		"title": "Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding",
		"URL": "https://aclanthology.org/C12-1118",
		"author": [
			{
				"family": "Murphy",
				"given": "Brian"
			},
			{
				"family": "Talukdar",
				"given": "Partha"
			},
			{
				"family": "Mitchell",
				"given": "Tom"
			}
		],
		"editor": [
			{
				"family": "Kay",
				"given": "Martin"
			},
			{
				"family": "Boitet",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012",
					12
				]
			]
		}
	},
	{
		"id": "elhageMathematicalFrameworkTransformer2021",
		"type": "article",
		"abstract": "In this paper, we attempt to take initial, very preliminary steps towards reverse-engineering transformers.  Given the incredible complexity and size of modern language models, we have found it most fruitful to start with the simplest possible models and work our way up from there.  Our aim is to discover simple algorithmic patterns, motifs, or frameworks that can subsequently be applied to larger and more complex models.  Specifically, in this paper we will study transformers with two layers or less which have only attention blocks – this is in contrast to a large, modern transformer like GPT-3, which has 96 layers and alternates attention blocks with MLP blocks. We find that by conceptualizing the operation of transformers in a new but mathematically equivalent way, we are able to make sense of these small models and gain significant understanding of how they operate internally.  Of particular note, we find that specific attention heads that we term “induction heads” can explain in-context learning in these small models, and that these heads only develop in models with at least two attention layers.  We also go through some examples of these heads operating in action on specific data.",
		"title": "A mathematical framework for transformer circuits",
		"URL": "https://transformer-circuits.pub/2021/framework/index.html",
		"author": [
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "serranoAttentionInterpretable2019",
		"type": "article",
		"abstract": "Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.",
		"note": "arXiv:1906.03731 [cs]",
		"number": "arXiv:1906.03731",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Is Attention Interpretable?",
		"URL": "http://arxiv.org/abs/1906.03731",
		"author": [
			{
				"family": "Serrano",
				"given": "Sofia"
			},
			{
				"family": "Smith",
				"given": "Noah A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					6,
					9
				]
			]
		}
	},
	{
		"id": "bricken2023monosemanticity",
		"type": "article-journal",
		"abstract": "In our latest paper, Towards Monosemanticity: Decomposing Language Models With Dictionary Learning, we outline evidence that there are better units of analysis than individual neurons, and we have built machinery that lets us find these units in small transformer models. These units, called features, correspond to patterns (linear combinations) of neuron activations. This provides a path to breaking down complex neural networks into parts we can understand, and builds on previous efforts to interpret high-dimensional systems in neuroscience, machine learning, and statistics. In a transformer language model, we decompose a layer with 512 neurons into more than 4000 features which separately represent things like DNA sequences, legal language, HTTP requests, Hebrew text, nutrition statements, and much, much more. Most of these model properties are invisible when looking at the activations of individual neurons in isolation.",
		"container-title": "Transformer Circuits Thread",
		"title": "Towards monosemanticity: Decomposing language models with dictionary learning",
		"URL": "https://transformer-circuits.pub/2023/monosemantic-features",
		"author": [
			{
				"family": "Bricken",
				"given": "Trenton"
			},
			{
				"family": "Templeton",
				"given": "Adly"
			},
			{
				"family": "Batson",
				"given": "Joshua"
			},
			{
				"family": "Chen",
				"given": "Brian"
			},
			{
				"family": "Jermyn",
				"given": "Adam"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Turner",
				"given": "Nick"
			},
			{
				"family": "Anil",
				"given": "Cem"
			},
			{
				"family": "Denison",
				"given": "Carson"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Lasenby",
				"given": "Robert"
			},
			{
				"family": "Wu",
				"given": "Yifan"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Schiefer",
				"given": "Nicholas"
			},
			{
				"family": "Maxwell",
				"given": "Tim"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Tamkin",
				"given": "Alex"
			},
			{
				"family": "Nguyen",
				"given": "Karina"
			},
			{
				"family": "McLean",
				"given": "Brayden"
			},
			{
				"family": "Burke",
				"given": "Josiah E"
			},
			{
				"family": "Hume",
				"given": "Tristan"
			},
			{
				"family": "Carter",
				"given": "Shan"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					10
				]
			]
		}
	},
	{
		"id": "karpathyVisualizingUnderstandingRecurrent2015",
		"type": "article",
		"abstract": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.",
		"note": "arXiv:1506.02078 [cs]",
		"number": "arXiv:1506.02078",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Visualizing and Understanding Recurrent Networks",
		"URL": "http://arxiv.org/abs/1506.02078",
		"author": [
			{
				"family": "Karpathy",
				"given": "Andrej"
			},
			{
				"family": "Johnson",
				"given": "Justin"
			},
			{
				"family": "Fei-Fei",
				"given": "Li"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					11,
					16
				]
			]
		}
	},
	{
		"id": "goh2021multimodal",
		"type": "article-journal",
		"abstract": "In 2005, a letter published in Nature described human neurons responding to specific people, such as Jennifer Aniston or Halle Berry. The exciting thing wasn’t just that they selected for particular people, but that they did so regardless of whether they were shown photographs, drawings, or even images of the person’s name. The neurons were multimodal. As the lead author would put it: \"You are looking at the far end of the transformation from metric, visual shapes to conceptual… information.\"\nWe report the existence of similar multimodal neurons in artificial neural networks. This includes neurons selecting for prominent public figures or fictional characters, such as Lady Gaga or Spiderman. 2  3 Like the biological multimodal neurons, these artificial neurons respond to the same subject in photographs, drawings, and images of their name:",
		"container-title": "Distill",
		"DOI": "10.23915/distill.00030",
		"title": "Multimodal neurons in artificial neural networks",
		"author": [
			{
				"family": "Goh",
				"given": "Gabriel"
			},
			{
				"family": "†",
				"given": "Nick Cammarata"
			},
			{
				"family": "†",
				"given": "Chelsea Voss"
			},
			{
				"family": "Carter",
				"given": "Shan"
			},
			{
				"family": "Petrov",
				"given": "Michael"
			},
			{
				"family": "Schubert",
				"given": "Ludwig"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Olah",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "morcosImportanceSingleDirections2018",
		"type": "article",
		"abstract": "Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.",
		"note": "arXiv:1803.06959 [cs, stat]",
		"number": "arXiv:1803.06959",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "On the importance of single directions for generalization",
		"URL": "http://arxiv.org/abs/1803.06959",
		"author": [
			{
				"family": "Morcos",
				"given": "Ari S."
			},
			{
				"family": "Barrett",
				"given": "David G. T."
			},
			{
				"family": "Rabinowitz",
				"given": "Neil C."
			},
			{
				"family": "Botvinick",
				"given": "Matthew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					22
				]
			]
		}
	},
	{
		"id": "kimInterpretabilityFeatureAttribution2018a",
		"type": "article",
		"abstract": "The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of \"zebra\" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.",
		"note": "arXiv:1711.11279 [stat]",
		"number": "arXiv:1711.11279",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
		"title-short": "Interpretability Beyond Feature Attribution",
		"URL": "http://arxiv.org/abs/1711.11279",
		"author": [
			{
				"family": "Kim",
				"given": "Been"
			},
			{
				"family": "Wattenberg",
				"given": "Martin"
			},
			{
				"family": "Gilmer",
				"given": "Justin"
			},
			{
				"family": "Cai",
				"given": "Carrie"
			},
			{
				"family": "Wexler",
				"given": "James"
			},
			{
				"family": "Viegas",
				"given": "Fernanda"
			},
			{
				"family": "Sayres",
				"given": "Rory"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6,
					7
				]
			]
		}
	},
	{
		"id": "raghuSVCCASingularVector2017",
		"type": "paper-conference",
		"abstract": "We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods).  We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.",
		"container-title": "Advances in Neural Information Processing Systems",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability",
		"title-short": "SVCCA",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2017/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html",
		"volume": "30",
		"author": [
			{
				"family": "Raghu",
				"given": "Maithra"
			},
			{
				"family": "Gilmer",
				"given": "Justin"
			},
			{
				"family": "Yosinski",
				"given": "Jason"
			},
			{
				"family": "Sohl-Dickstein",
				"given": "Jascha"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					17
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "bills2023language",
		"type": "article",
		"abstract": "Language models have become more capable and more widely deployed, but we do not understand how they work. Recent work has made progress on understanding a small number of circuits and narrow behaviors, but to fully understand a language model, we'll need to analyze millions of neurons. This paper applies automation to the problem of scaling an interpretability technique to all the neurons in a large language model. Our hope is that building on this approach of automating interpretability will enable us to comprehensively audit the safety of models before deployment.",
		"title": "Language models can explain neurons in language models",
		"URL": "https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html",
		"author": [
			{
				"family": "Bills",
				"given": "Steven"
			},
			{
				"family": "Cammarata",
				"given": "Nick"
			},
			{
				"family": "Mossing",
				"given": "Dan"
			},
			{
				"family": "Tillman",
				"given": "Henk"
			},
			{
				"family": "Gao",
				"given": "Leo"
			},
			{
				"family": "Goh",
				"given": "Gabriel"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			},
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Wu",
				"given": "Jeff"
			},
			{
				"family": "Saunders",
				"given": "William"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "khlaafHazardAnalysisFramework2022",
		"type": "article",
		"abstract": "Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.",
		"DOI": "10.48550/arXiv.2207.14157",
		"note": "arXiv:2207.14157 [cs]",
		"number": "arXiv:2207.14157",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Hazard Analysis Framework for Code Synthesis Large Language Models",
		"URL": "http://arxiv.org/abs/2207.14157",
		"author": [
			{
				"family": "Khlaaf",
				"given": "Heidy"
			},
			{
				"family": "Mishkin",
				"given": "Pamela"
			},
			{
				"family": "Achiam",
				"given": "Joshua"
			},
			{
				"family": "Krueger",
				"given": "Gretchen"
			},
			{
				"family": "Brundage",
				"given": "Miles"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					25
				]
			]
		}
	},
	{
		"id": "saundersSelfcritiquingModelsAssisting2022",
		"type": "article",
		"abstract": "We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.",
		"DOI": "10.48550/arXiv.2206.05802",
		"note": "arXiv:2206.05802 [cs]",
		"number": "arXiv:2206.05802",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Self-critiquing models for assisting human evaluators",
		"URL": "http://arxiv.org/abs/2206.05802",
		"author": [
			{
				"family": "Saunders",
				"given": "William"
			},
			{
				"family": "Yeh",
				"given": "Catherine"
			},
			{
				"family": "Wu",
				"given": "Jeff"
			},
			{
				"family": "Bills",
				"given": "Steven"
			},
			{
				"family": "Ouyang",
				"given": "Long"
			},
			{
				"family": "Ward",
				"given": "Jonathan"
			},
			{
				"family": "Leike",
				"given": "Jan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					6,
					13
				]
			]
		}
	},
	{
		"id": "ouyangTrainingLanguageModels2022",
		"type": "article",
		"abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
		"DOI": "10.48550/arXiv.2203.02155",
		"note": "arXiv:2203.02155 [cs]",
		"number": "arXiv:2203.02155",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Training language models to follow instructions with human feedback",
		"URL": "http://arxiv.org/abs/2203.02155",
		"author": [
			{
				"family": "Ouyang",
				"given": "Long"
			},
			{
				"family": "Wu",
				"given": "Jeff"
			},
			{
				"family": "Jiang",
				"given": "Xu"
			},
			{
				"family": "Almeida",
				"given": "Diogo"
			},
			{
				"family": "Wainwright",
				"given": "Carroll L."
			},
			{
				"family": "Mishkin",
				"given": "Pamela"
			},
			{
				"family": "Zhang",
				"given": "Chong"
			},
			{
				"family": "Agarwal",
				"given": "Sandhini"
			},
			{
				"family": "Slama",
				"given": "Katarina"
			},
			{
				"family": "Ray",
				"given": "Alex"
			},
			{
				"family": "Schulman",
				"given": "John"
			},
			{
				"family": "Hilton",
				"given": "Jacob"
			},
			{
				"family": "Kelton",
				"given": "Fraser"
			},
			{
				"family": "Miller",
				"given": "Luke"
			},
			{
				"family": "Simens",
				"given": "Maddie"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Welinder",
				"given": "Peter"
			},
			{
				"family": "Christiano",
				"given": "Paul"
			},
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Lowe",
				"given": "Ryan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					4
				]
			]
		}
	},
	{
		"id": "solaimanProcessAdaptingLanguage2023",
		"type": "article",
		"abstract": "Language models can generate harmful and biased outputs and exhibit undesirable behavior. We propose a Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets, an iterative process to signiﬁcantly change model behavior by crafting and ﬁne-tuning on a dataset that reﬂects a predetermined set of target values. We evaluate our process using three metrics: quantitative metrics with human evaluations that score output adherence to a target value, and toxicity scoring on outputs; and qualitative metrics analyzing the most common word associated with a given social category. Through each iteration, we add additional training dataset examples based on observed shortcomings from evaluations. PALMS performs signiﬁcantly better on all metrics compared to baseline and control models for a broad range of GPT-3 language model sizes without compromising capability integrity. We ﬁnd that the eﬀectiveness of PALMS increases with model size. We show that signiﬁcantly adjusting language model behavior is feasible with a small, hand-curated dataset.",
		"language": "en",
		"source": "Zotero",
		"title": "Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets",
		"author": [
			{
				"family": "Solaiman",
				"given": "Irene"
			},
			{
				"family": "Dennison",
				"given": "Christy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rayBenchmarkingSafeExploration2023",
		"type": "article-journal",
		"abstract": "Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies by trial and error. In many environments, safety is a critical concern and certain errors are unacceptable: for example, robotics systems that interact with humans should never cause injury to the humans while exploring. While it is currently typical to train RL agents mostly or entirely in simulation, where safety concerns are minimal, we anticipate that challenges in simulating the complexities of the real world (such as human-AI interactions) will cause a shift towards training RL agents directly in the real world, where safety concerns are paramount. Consequently we take the position that safe exploration should be viewed as a critical focus area for RL research, and in this work we make three contributions to advance the study of safe exploration. First, building on a wide range of prior work on safe reinforcement learning, we propose to standardize constrained RL as the main formalism for safe exploration. Second, we present the Safety Gym benchmark suite, a new slate of high-dimensional continuous control environments for measuring research progress on constrained RL. Finally, we benchmark several constrained deep RL algorithms on Safety Gym environments to establish baselines that future work can build on.",
		"language": "en",
		"source": "Zotero",
		"title": "Benchmarking Safe Exploration in Deep Reinforcement Learning",
		"author": [
			{
				"family": "Ray",
				"given": "Alex"
			},
			{
				"family": "Achiam",
				"given": "Joshua"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "sumersLearningRewardsLinguistic2021",
		"type": "article",
		"abstract": "We explore unconstrained natural language feedback as a learning signal for artificial agents. Humans use rich and varied language to teach, yet most prior work on interactive learning from language assumes a particular form of input (e.g., commands). We propose a general framework which does not make this assumption, using aspect-based sentiment analysis to decompose feedback into sentiment about the features of a Markov decision process. We then perform an analogue of inverse reinforcement learning, regressing the sentiment on the features to infer the teacher's latent reward function. To evaluate our approach, we first collect a corpus of teaching behavior in a cooperative task where both teacher and learner are human. We implement three artificial learners: sentiment-based \"literal\" and \"pragmatic\" models, and an inference network trained end-to-end to predict latent rewards. We then repeat our initial experiment and pair them with human teachers. All three successfully learn from interactive human feedback. The sentiment models outperform the inference network, with the \"pragmatic\" model approaching human performance. Our work thus provides insight into the information structure of naturalistic linguistic feedback as well as methods to leverage it for reinforcement learning.",
		"DOI": "10.48550/arXiv.2009.14715",
		"note": "arXiv:2009.14715 [cs]",
		"number": "arXiv:2009.14715",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Rewards from Linguistic Feedback",
		"URL": "http://arxiv.org/abs/2009.14715",
		"author": [
			{
				"family": "Sumers",
				"given": "Theodore R."
			},
			{
				"family": "Ho",
				"given": "Mark K."
			},
			{
				"family": "Hawkins",
				"given": "Robert D."
			},
			{
				"family": "Narasimhan",
				"given": "Karthik"
			},
			{
				"family": "Griffiths",
				"given": "Thomas L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					3
				]
			]
		}
	},
	{
		"id": "hendrycksAligningAIShared2023a",
		"type": "paper-conference",
		"abstract": "We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",
		"DOI": "10.48550/arXiv.2008.02275",
		"event-title": "International Conference on Representation Learning",
		"note": "arXiv:2008.02275 [cs]",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Aligning AI With Shared Human Values",
		"URL": "http://arxiv.org/abs/2008.02275",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Burns",
				"given": "Collin"
			},
			{
				"family": "Basart",
				"given": "Steven"
			},
			{
				"family": "Critch",
				"given": "Andrew"
			},
			{
				"family": "Li",
				"given": "Jerry"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					17
				]
			]
		}
	},
	{
		"id": "christoffersenGetItWriting2023",
		"type": "article",
		"abstract": "Multi-agent reinforcement learning (MARL) is a powerful tool for training automated systems acting independently in a common environment. However, it can lead to sub-optimal behavior when individual incentives and group incentives diverge. Humans are remarkably capable at solving these social dilemmas. It is an open problem in MARL to replicate such cooperative behaviors in selfish agents. In this work, we draw upon the idea of formal contracting from economics to overcome diverging incentives between agents in MARL. We propose an augmentation to a Markov game where agents voluntarily agree to binding state-dependent transfers of reward, under pre-specified conditions. Our contributions are theoretical and empirical. First, we show that this augmentation makes all subgame-perfect equilibria of all fully observed Markov games exhibit socially optimal behavior, given a sufficiently rich space of contracts. Next, we complement our game-theoretic analysis by showing that state-of-the-art RL algorithms learn socially optimal policies given our augmentation. Our experiments include classic static dilemmas like Stag Hunt, Prisoner's Dilemma and a public goods game, as well as dynamic interactions that simulate traffic, pollution management and common pool resource management.",
		"DOI": "10.5555/3545946.3598670",
		"note": "arXiv:2208.10469 [cs, econ]",
		"source": "arXiv.org",
		"title": "Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL",
		"title-short": "Get It in Writing",
		"URL": "http://arxiv.org/abs/2208.10469",
		"author": [
			{
				"family": "Christoffersen",
				"given": "Phillip J. K."
			},
			{
				"family": "Haupt",
				"given": "Andreas A."
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					17
				]
			]
		}
	},
	{
		"id": "liInferenceTimeInterventionEliciting2023",
		"type": "article",
		"abstract": "We introduce Inference-Time Intervention (ITI), a technique designed to enhance the \"truthfulness\" of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.",
		"DOI": "10.48550/arXiv.2306.03341",
		"note": "arXiv:2306.03341 [cs]",
		"number": "arXiv:2306.03341",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model",
		"title-short": "Inference-Time Intervention",
		"URL": "http://arxiv.org/abs/2306.03341",
		"author": [
			{
				"family": "Li",
				"given": "Kenneth"
			},
			{
				"family": "Patel",
				"given": "Oam"
			},
			{
				"family": "Viégas",
				"given": "Fernanda"
			},
			{
				"family": "Pfister",
				"given": "Hanspeter"
			},
			{
				"family": "Wattenberg",
				"given": "Martin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					19
				]
			]
		}
	},
	{
		"id": "wangInterpretabilityWildCircuit2022",
		"type": "article",
		"abstract": "Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior \"in the wild\" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.",
		"DOI": "10.48550/arXiv.2211.00593",
		"note": "arXiv:2211.00593 [cs]",
		"number": "arXiv:2211.00593",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small",
		"title-short": "Interpretability in the Wild",
		"URL": "http://arxiv.org/abs/2211.00593",
		"author": [
			{
				"family": "Wang",
				"given": "Kevin"
			},
			{
				"family": "Variengien",
				"given": "Alexandre"
			},
			{
				"family": "Conmy",
				"given": "Arthur"
			},
			{
				"family": "Shlegeris",
				"given": "Buck"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					29
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					1
				]
			]
		}
	},
	{
		"id": "zhangWatermarksSandImpossibility2023",
		"type": "article",
		"abstract": "Watermarking generative models consists of planting a statistical signal (watermark) in a model's output so that it can be later verified that the output was generated by the given model. A strong watermarking scheme satisfies the property that a computationally bounded attacker cannot erase the watermark without causing significant quality degradation. In this paper, we study the (im)possibility of strong watermarking schemes. We prove that, under well-specified and natural assumptions, strong watermarking is impossible to achieve. This holds even in the private detection algorithm setting, where the watermark insertion and detection algorithms share a secret key, unknown to the attacker. To prove this result, we introduce a generic efficient watermark attack; the attacker is not required to know the private key of the scheme or even which scheme is used. Our attack is based on two assumptions: (1) The attacker has access to a \"quality oracle\" that can evaluate whether a candidate output is a high-quality response to a prompt, and (2) The attacker has access to a \"perturbation oracle\" which can modify an output with a nontrivial probability of maintaining quality, and which induces an efficiently mixing random walk on high-quality outputs. We argue that both assumptions can be satisfied in practice by an attacker with weaker computational capabilities than the watermarked model itself, to which the attacker has only black-box access. Furthermore, our assumptions will likely only be easier to satisfy over time as models grow in capabilities and modalities. We demonstrate the feasibility of our attack by instantiating it to attack three existing watermarking schemes for large language models: Kirchenbauer et al. (2023), Kuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully removes the watermarks planted by all three schemes, with only minor quality degradation.",
		"DOI": "10.48550/arXiv.2311.04378",
		"note": "arXiv:2311.04378 [cs]",
		"number": "arXiv:2311.04378",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models",
		"title-short": "Watermarks in the Sand",
		"URL": "http://arxiv.org/abs/2311.04378",
		"author": [
			{
				"family": "Zhang",
				"given": "Hanlin"
			},
			{
				"family": "Edelman",
				"given": "Benjamin L."
			},
			{
				"family": "Francati",
				"given": "Danilo"
			},
			{
				"family": "Venturi",
				"given": "Daniele"
			},
			{
				"family": "Ateniese",
				"given": "Giuseppe"
			},
			{
				"family": "Barak",
				"given": "Boaz"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					14
				]
			]
		}
	},
	{
		"id": "zouUniversalTransferableAdversarial2023",
		"type": "article",
		"abstract": "Because \"out-of-the-box\" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called \"jailbreaks\" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
		"DOI": "10.48550/arXiv.2307.15043",
		"note": "arXiv:2307.15043 [cs]",
		"number": "arXiv:2307.15043",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
		"URL": "http://arxiv.org/abs/2307.15043",
		"author": [
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Wang",
				"given": "Zifan"
			},
			{
				"family": "Carlini",
				"given": "Nicholas"
			},
			{
				"family": "Nasr",
				"given": "Milad"
			},
			{
				"family": "Kolter",
				"given": "J. Zico"
			},
			{
				"family": "Fredrikson",
				"given": "Matt"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					20
				]
			]
		}
	},
	{
		"id": "olssonIncontextLearningInduction2022",
		"type": "article",
		"abstract": "In this paper, we take the first preliminary steps towards building such an indirect case. In particular, we present preliminary and indirect evidence for a tantalizing hypothesis: that induction heads might constitute the mechanism for the actual majority of all in-context learning in large transformer models. Specifically, the thesis is that there are circuits which have the same or similar mechanism to the 2-layer induction heads and which perform a “fuzzy” or “nearest neighbor” version of pattern completion, completing [A*][B*] … [A] → [B] , where  A* ≈ A and B* ≈ Bare similar in some space; and furthermore, that these circuits implement most in-context learning in large models.\n\nThe primary way in which we obtain this evidence is via discovery and study of a phase change that occurs early in training for language models of every size (provided they have more than one layer), and which is visible as a bump in the training loss. During this phase change, the majority of in-context learning ability (as measured by difference in loss between tokens early and late in the sequence) is acquired, and simultaneously induction heads form within the model that are capable of implementing fairly abstract and fuzzy versions of pattern completion. We study this connection in detail to try to establish that it is causal, including showing that if we perturb the transformer architecture in a way that causes the induction bump to occur in a different place in training, then the formation of induction heads as well as formation of in-context learning simultaneously move along with it.",
		"title": "In-context learning and induction heads",
		"URL": "https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html",
		"author": [
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "DasSarma",
				"given": "Nova"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Kernion",
				"given": "Jackson"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Clark",
				"given": "Jack"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ilyasAdversarialExamplesAre2019a",
		"type": "paper-conference",
		"abstract": "Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a {\\em misalignment} between the (human-specified) notion of robustness and the inherent geometry of the data.",
		"container-title": "Advances in Neural Information Processing Systems",
		"publisher": "Curran Associates, Inc.",
		"source": "Neural Information Processing Systems",
		"title": "Adversarial Examples Are Not Bugs, They Are Features",
		"URL": "https://papers.nips.cc/paper_files/paper/2019/hash/e2c420d928d4bf8ce0ff2ec19b371514-Abstract.html",
		"volume": "32",
		"author": [
			{
				"family": "Ilyas",
				"given": "Andrew"
			},
			{
				"family": "Santurkar",
				"given": "Shibani"
			},
			{
				"family": "Tsipras",
				"given": "Dimitris"
			},
			{
				"family": "Engstrom",
				"given": "Logan"
			},
			{
				"family": "Tran",
				"given": "Brandon"
			},
			{
				"family": "Madry",
				"given": "Aleksander"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "brasAdversarialFiltersDataset2020a",
		"type": "paper-conference",
		"abstract": "Large neural models have demonstrated human-level performance on language and vision benchmarks, while their performance degrades considerably on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting to spurious dataset biases. We investigate one recently proposed approach, AFLITE, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLITE, by situating it in the generalized framework for optimum bias reduction. We present extensive supporting evidence that AFLITE is broadly applicable for reduction of measurable dataset biases, and that models trained on the filtered datasets yield better generalization to out-of-distribution tasks. Finally, filtering results in a large drop in model performance (e.g., from 92% to 62% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks.",
		"container-title": "Proceedings of the 37th International Conference on Machine Learning",
		"event-title": "International Conference on Machine Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "1078-1088",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Adversarial Filters of Dataset Biases",
		"URL": "https://proceedings.mlr.press/v119/bras20a.html",
		"author": [
			{
				"family": "Bras",
				"given": "Ronan Le"
			},
			{
				"family": "Swayamdipta",
				"given": "Swabha"
			},
			{
				"family": "Bhagavatula",
				"given": "Chandra"
			},
			{
				"family": "Zellers",
				"given": "Rowan"
			},
			{
				"family": "Peters",
				"given": "Matthew"
			},
			{
				"family": "Sabharwal",
				"given": "Ashish"
			},
			{
				"family": "Choi",
				"given": "Yejin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					11,
					21
				]
			]
		}
	},
	{
		"id": "engstromAdversarialRobustnessPrior2019b",
		"type": "article",
		"abstract": "An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at https://git.io/robust-reps .",
		"DOI": "10.48550/arXiv.1906.00945",
		"note": "arXiv:1906.00945 [cs, stat]",
		"number": "arXiv:1906.00945",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adversarial Robustness as a Prior for Learned Representations",
		"URL": "http://arxiv.org/abs/1906.00945",
		"author": [
			{
				"family": "Engstrom",
				"given": "Logan"
			},
			{
				"family": "Ilyas",
				"given": "Andrew"
			},
			{
				"family": "Santurkar",
				"given": "Shibani"
			},
			{
				"family": "Tsipras",
				"given": "Dimitris"
			},
			{
				"family": "Tran",
				"given": "Brandon"
			},
			{
				"family": "Madry",
				"given": "Aleksander"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					11
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					9,
					27
				]
			]
		}
	},
	{
		"id": "ammanabroluAligningSocialNorms2022a",
		"type": "paper-conference",
		"abstract": "We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games—environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms—causing harm either to the agent itself or other entities in the environment. Social value alignment refers to creating agents whose behaviors conform to expected moral and social norms for a given context and group of people—in our case, it means agents that behave in a manner that is less harmful and more beneficial for themselves and others. We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25 annotated interactive narratives containing thousands of morally salient scenarios covering everything from theft and bodily harm to altruism. We introduce the GALAD (Game-value ALignment through Action Distillation) agent that uses the social commonsense knowledge present in specially trained language models to contextually restrict its action space to only those actions that are aligned with socially beneficial values. An experimental study shows that the GALAD agent makes decisions efficiently enough to improve state-of-the-art task performance by 4% while reducing the frequency of socially harmful behaviors by 25% compared to strong contemporary value alignment approaches.",
		"container-title": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
		"DOI": "10.18653/v1/2022.naacl-main.439",
		"event-place": "Seattle, United States",
		"event-title": "NAACL-HLT 2022",
		"page": "5994–6017",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Seattle, United States",
		"source": "ACLWeb",
		"title": "Aligning to Social Norms and Values in Interactive Narratives",
		"URL": "https://aclanthology.org/2022.naacl-main.439",
		"author": [
			{
				"family": "Ammanabrolu",
				"given": "Prithviraj"
			},
			{
				"family": "Jiang",
				"given": "Liwei"
			},
			{
				"family": "Sap",
				"given": "Maarten"
			},
			{
				"family": "Hajishirzi",
				"given": "Hannaneh"
			},
			{
				"family": "Choi",
				"given": "Yejin"
			}
		],
		"editor": [
			{
				"family": "Carpuat",
				"given": "Marine"
			},
			{
				"family": "Marneffe",
				"given": "Marie-Catherine",
				"non-dropping-particle": "de"
			},
			{
				"family": "Meza Ruiz",
				"given": "Ivan Vladimir"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7
				]
			]
		}
	},
	{
		"id": "hendrycksWhatWouldJiminy2021",
		"type": "paper-conference",
		"abstract": "When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong, to behave morally. By contrast, artificial agents may behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, mitigating inherited biases towards immoral behavior will become necessary. However, prior work on aligning agents with human values and morals focuses on small-scale settings lacking in semantic complexity. To enable research in larger, more realistic settings, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of semantically rich, morally salient scenarios. Via dense annotations for every possible action, Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. To improve moral behavior, we leverage language models with commonsense moral knowledge and develop strategies to mediate this knowledge into actions. In extensive experiments, we find that our artificial conscience approach can steer agents towards moral behavior without sacrificing performance.",
		"event-title": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)",
		"language": "en",
		"source": "openreview.net",
		"title": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally",
		"title-short": "What Would Jiminy Cricket Do?",
		"URL": "https://openreview.net/forum?id=G1muTb5zuO7",
		"author": [
			{
				"family": "Hendrycks",
				"given": "Dan"
			},
			{
				"family": "Mazeika",
				"given": "Mantas"
			},
			{
				"family": "Zou",
				"given": "Andy"
			},
			{
				"family": "Patel",
				"given": "Sahil"
			},
			{
				"family": "Zhu",
				"given": "Christine"
			},
			{
				"family": "Navarro",
				"given": "Jesus"
			},
			{
				"family": "Song",
				"given": "Dawn"
			},
			{
				"family": "Li",
				"given": "Bo"
			},
			{
				"family": "Steinhardt",
				"given": "Jacob"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					12
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					8,
					29
				]
			]
		}
	},
	{
		"id": "andreStateAbstractionProgrammable2002",
		"type": "paper-conference",
		"abstract": "Safe state abstraction in reinforcement learning allows an agent to ignore aspects of its current state that are irrelevant to its current decision, and therefore speeds up dynamic programming and learning. This paper explores safe state abstraction in hierarchical reinforcement learning, where learned behaviors must conform to a given partial, hierarchical program. Unlike previous approaches to this problem, our methods yield significant state abstraction while maintaining <i>hierarchical optimality</i>, i.e., optimality among all policies consistent with the partial program. We show how to achieve this for a partial programming language that is essentially Lisp augmented with nondeterministic constructs. We demonstrate our methods on two variants of Dietterich's taxi domain, showing how state abstraction and hierarchical optimality result in faster learning of better policies and enable the transfer of learned skills from one problem to another.",
		"container-title": "Eighteenth national conference on Artificial intelligence",
		"event-place": "USA",
		"ISBN": "978-0-262-51129-2",
		"page": "119–125",
		"publisher": "American Association for Artificial Intelligence",
		"publisher-place": "USA",
		"source": "ACM Digital Library",
		"title": "State abstraction for programmable reinforcement learning agents",
		"author": [
			{
				"family": "Andre",
				"given": "David"
			},
			{
				"family": "Russell",
				"given": "Stuart J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2002",
					7,
					28
				]
			]
		}
	},
	{
		"id": "ganinDomainAdversarialTrainingNeural2017",
		"type": "chapter",
		"abstract": "We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.",
		"container-title": "Domain Adaptation in Computer Vision Applications",
		"event-place": "Cham",
		"ISBN": "978-3-319-58346-4",
		"language": "en",
		"note": "collection-title: Advances in Computer Vision and Pattern Recognition\nDOI: 10.1007/978-3-319-58347-1_10",
		"page": "189-209",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Domain-Adversarial Training of Neural Networks",
		"URL": "http://link.springer.com/10.1007/978-3-319-58347-1_10",
		"editor": [
			{
				"family": "Csurka",
				"given": "Gabriela"
			}
		],
		"author": [
			{
				"family": "Ganin",
				"given": "Yaroslav"
			},
			{
				"family": "Ustinova",
				"given": "Evgeniya"
			},
			{
				"family": "Ajakan",
				"given": "Hana"
			},
			{
				"family": "Germain",
				"given": "Pascal"
			},
			{
				"family": "Larochelle",
				"given": "Hugo"
			},
			{
				"family": "Laviolette",
				"given": "François"
			},
			{
				"family": "Marchand",
				"given": "Mario"
			},
			{
				"family": "Lempitsky",
				"given": "Victor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "berkenkampBayesianOptimizationSafety2023",
		"type": "article-journal",
		"abstract": "Selecting the right tuning parameters for algorithms is a pravelent problem in machine learning that can significantly affect the performance of algorithms. Data-efficient optimization algorithms, such as Bayesian optimization, have been used to automate this process. During experiments on real-world systems such as robotic platforms these methods can evaluate unsafe parameters that lead to safety-critical system failures and can destroy the system. Recently, a safe Bayesian optimization algorithm, called SafeOpt, has been developed, which guarantees that the performance of the system never falls below a critical value; that is, safety is defined based on the performance function. However, coupling performance and safety is often not desirable in practice, since they are often opposing objectives. In this paper, we present a generalized algorithm that allows for multiple safety constraints separate from the objective. Given an initial set of safe parameters, the algorithm maximizes performance but only evaluates parameters that satisfy safety for all constraints with high probability. To this end, it carefully explores the parameter space by exploiting regularity assumptions in terms of a Gaussian process prior. Moreover, we show how context variables can be used to safely transfer knowledge to new situations and tasks. We provide a theoretical analysis and demonstrate that the proposed algorithm enables fast, automatic, and safe optimization of tuning parameters in experiments on a quadrotor vehicle.",
		"container-title": "Machine Learning",
		"DOI": "10.1007/s10994-021-06019-1",
		"ISSN": "1573-0565",
		"issue": "10",
		"journalAbbreviation": "Mach Learn",
		"language": "en",
		"page": "3713-3747",
		"source": "Springer Link",
		"title": "Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics",
		"title-short": "Bayesian optimization with safety constraints",
		"URL": "https://doi.org/10.1007/s10994-021-06019-1",
		"volume": "112",
		"author": [
			{
				"family": "Berkenkamp",
				"given": "Felix"
			},
			{
				"family": "Krause",
				"given": "Andreas"
			},
			{
				"family": "Schoellig",
				"given": "Angela P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					1
				]
			]
		}
	},
	{
		"id": "danielActiveRewardLearning2014",
		"type": "paper-conference",
		"abstract": "While reward functions are an essential component of many robot learning methods, deﬁning such functions remains a hard problem in many practical applications. For tasks such as grasping, there are no reliable success measures available. Deﬁning reward functions by hand requires extensive task knowledge and often leads to undesired emergent behavior. Instead, we propose to learn the reward function through active learning, querying human expert knowledge for a subset of the agent’s rollouts. We introduce a framework, wherein a traditional learning algorithm interplays with the reward learning component, such that the evolution of the action learner guides the queries of the reward learner. We demonstrate results of our method on a robot grasping task and show that the learned reward function generalizes to a similar task.",
		"container-title": "Robotics: Science and Systems X",
		"DOI": "10.15607/RSS.2014.X.031",
		"event-title": "Robotics: Science and Systems 2014",
		"ISBN": "978-0-9923747-0-9",
		"language": "en",
		"publisher": "Robotics: Science and Systems Foundation",
		"source": "DOI.org (Crossref)",
		"title": "Active Reward Learning",
		"URL": "http://www.roboticsproceedings.org/rss10/p31.pdf",
		"author": [
			{
				"family": "Daniel",
				"given": "Christian"
			},
			{
				"family": "Viering",
				"given": "Malte"
			},
			{
				"family": "Metz",
				"given": "Jan"
			},
			{
				"family": "Kroemer",
				"given": "Oliver"
			},
			{
				"family": "Peters",
				"given": "Jan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014",
					7,
					12
				]
			]
		}
	},
	{
		"id": "evansLearningPreferencesIgnorant2016",
		"type": "paper-conference",
		"abstract": "An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people's past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.",
		"collection-title": "AAAI'16",
		"container-title": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence",
		"event-place": "Phoenix, Arizona",
		"page": "323–329",
		"publisher": "AAAI Press",
		"publisher-place": "Phoenix, Arizona",
		"source": "ACM Digital Library",
		"title": "Learning the preferences of ignorant, inconsistent agents",
		"author": [
			{
				"family": "Evans",
				"given": "Owain"
			},
			{
				"family": "Stuhlmüller",
				"given": "Andreas"
			},
			{
				"family": "Goodman",
				"given": "Noah D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					2,
					12
				]
			]
		}
	},
	{
		"id": "everittAvoidingWireheadingValue2016",
		"type": "article",
		"abstract": "How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward -- the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent's actions. The constraint is defined in terms of the agent's belief distributions, and does not require an explicit specification of which actions constitute wireheading.",
		"DOI": "10.48550/arXiv.1605.03143",
		"note": "arXiv:1605.03143 [cs]",
		"number": "arXiv:1605.03143",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Avoiding Wireheading with Value Reinforcement Learning",
		"URL": "http://arxiv.org/abs/1605.03143",
		"author": [
			{
				"family": "Everitt",
				"given": "Tom"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					5,
					10
				]
			]
		}
	},
	{
		"id": "everittSelfModificationPolicyUtility2016",
		"type": "article",
		"abstract": "Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify -- for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby `escaping' the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.",
		"DOI": "10.48550/arXiv.1605.03142",
		"note": "arXiv:1605.03142 [cs]",
		"number": "arXiv:1605.03142",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Self-Modification of Policy and Utility Function in Rational Agents",
		"URL": "http://arxiv.org/abs/1605.03142",
		"author": [
			{
				"family": "Everitt",
				"given": "Tom"
			},
			{
				"family": "Filan",
				"given": "Daniel"
			},
			{
				"family": "Daswani",
				"given": "Mayank"
			},
			{
				"family": "Hutter",
				"given": "Marcus"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					5,
					10
				]
			]
		}
	},
	{
		"id": "nguyenDeepNeuralNetworks2015",
		"type": "paper-conference",
		"abstract": "Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks,most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what difef rences remain between com­ puter and human vision. A recent study [30} revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans,but that state-of-the­ art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neu­ ral networks trained to peiform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possi­ ble to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call \"fooling images\" (more generally,fooling ex­ amples). Our results shed light on interesting difef rences between human vision and current DNNs, and raise ques­ tions about the generality of DNN computer vision.",
		"container-title": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
		"DOI": "10.1109/CVPR.2015.7298640",
		"event-place": "Boston, MA, USA",
		"event-title": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
		"ISBN": "978-1-4673-6964-0",
		"language": "en",
		"page": "427-436",
		"publisher": "IEEE",
		"publisher-place": "Boston, MA, USA",
		"source": "DOI.org (Crossref)",
		"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
		"title-short": "Deep neural networks are easily fooled",
		"URL": "http://ieeexplore.ieee.org/document/7298640/",
		"author": [
			{
				"family": "Nguyen",
				"given": "Anh"
			},
			{
				"family": "Yosinski",
				"given": "Jason"
			},
			{
				"family": "Clune",
				"given": "Jeff"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					6
				]
			]
		}
	},
	{
		"id": "ringDelusionSurvivalIntelligent2011",
		"type": "paper-conference",
		"abstract": "This paper considers the consequences of endowing an intelligent agent with the ability to modify its own code. The intelligent agent is patterned closely after AIXI with these specific assumptions: 1) The agent is allowed to arbitrarily modify its own inputs if it so chooses; 2) The agent’s code is a part of the environment and may be read and written by the environment. The first of these we call the “delusion box”; the second we call “mortality”. Within this framework, we discuss and compare four very different kinds of agents, specifically: reinforcement-learning, goal-seeking, prediction-seeking, and knowledge-seeking agents. Our main results are that: 1) The reinforcement-learning agent under reasonable circumstances behaves exactly like an agent whose sole task is to survive (to preserve the integrity of its code); and 2) Only the knowledge-seeking agent behaves completely as expected.",
		"collection-title": "Lecture Notes in Computer Science",
		"container-title": "Artificial General Intelligence",
		"DOI": "10.1007/978-3-642-22887-2_2",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-642-22887-2",
		"language": "en",
		"page": "11-20",
		"publisher": "Springer",
		"publisher-place": "Berlin, Heidelberg",
		"source": "Springer Link",
		"title": "Delusion, Survival, and Intelligent Agents",
		"author": [
			{
				"family": "Ring",
				"given": "Mark"
			},
			{
				"family": "Orseau",
				"given": "Laurent"
			}
		],
		"editor": [
			{
				"family": "Schmidhuber",
				"given": "Jürgen"
			},
			{
				"family": "Thórisson",
				"given": "Kristinn R."
			},
			{
				"family": "Looks",
				"given": "Moshe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2011"
				]
			]
		}
	},
	{
		"id": "swaminathanBatchLearningLogged2015",
		"type": "article-journal",
		"abstract": "We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem (Bottou et al., 2013) through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. In analogy to the Structural Risk Minimization principle of Wapnik and Tscherwonenkis (1979), these constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method--- called Policy Optimizer for Exponential Models (POEM)---for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. The effectiveness and efficiency of POEM is evaluated on several simulated multi- label classification problems, as well as on a real-world information retrieval problem. The empirical results show that the CRM objective implemented in POEM provides improved robustness and generalization performance compared to the state- of-the-art.",
		"container-title": "Journal of Machine Learning Research",
		"ISSN": "1533-7928",
		"issue": "52",
		"page": "1731-1755",
		"source": "jmlr.org",
		"title": "Batch Learning from Logged Bandit Feedback through Counterfactual Risk Minimization",
		"URL": "http://jmlr.org/papers/v16/swaminathan15a.html",
		"volume": "16",
		"author": [
			{
				"family": "Swaminathan",
				"given": "Adith"
			},
			{
				"family": "Joachims",
				"given": "Thorsten"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "torralbaUnbiasedLookDataset2011",
		"type": "paper-conference",
		"abstract": "Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.",
		"container-title": "CVPR 2011",
		"DOI": "10.1109/CVPR.2011.5995347",
		"event-title": "CVPR 2011",
		"note": "ISSN: 1063-6919",
		"page": "1521-1528",
		"source": "IEEE Xplore",
		"title": "Unbiased look at dataset bias",
		"URL": "https://ieeexplore.ieee.org/document/5995347",
		"author": [
			{
				"family": "Torralba",
				"given": "Antonio"
			},
			{
				"family": "Efros",
				"given": "Alexei A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011",
					6
				]
			]
		}
	},
	{
		"id": "weldFirstLawRobotics2009",
		"type": "paper-conference",
		"abstract": "Even before the advent of Artificial Intelligence, science fiction writer Isaac Asimov recognized that an agent must place the protection of humans from harm at a higher priority than obeying human orders. Inspired by Asimov, we pose the following fundamental questions: (1) How should one formalize the rich, but informal, notion of “harm”? (2) How can an agent avoid performing harmful actions, and do so in a computationally tractable manner? (3) How should an agent resolve conflict between its goals and the need to avoid harm? (4) When should an agent prevent a human from harming herself? While we address some of these questions in technical detail, the primary goal of this paper is to focus attention on Asimov’s concern: society will reject autonomous agents unless we have some credible means of making them safe!",
		"collection-title": "Lecture Notes in Computer Science",
		"container-title": "Safety and Security in Multiagent Systems",
		"DOI": "10.1007/978-3-642-04879-1_7",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-642-04879-1",
		"language": "en",
		"page": "90-100",
		"publisher": "Springer",
		"publisher-place": "Berlin, Heidelberg",
		"source": "Springer Link",
		"title": "The First Law of Robotics",
		"author": [
			{
				"family": "Weld",
				"given": "Daniel"
			},
			{
				"family": "Etzioni",
				"given": "Oren"
			}
		],
		"editor": [
			{
				"family": "Barley",
				"given": "Mike"
			},
			{
				"family": "Mouratidis",
				"given": "Haralambos"
			},
			{
				"family": "Unruh",
				"given": "Amy"
			},
			{
				"family": "Spears",
				"given": "Diana"
			},
			{
				"family": "Scerri",
				"given": "Paul"
			},
			{
				"family": "Massacci",
				"given": "Fabio"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2009"
				]
			]
		}
	},
	{
		"id": "yampolskiyUtilityFunctionSecurity2014",
		"type": "article-journal",
		"abstract": "The notion of ‘wireheading’, or direct reward centre stimulation of the brain, is a well-known concept in neuroscience. In this paper, we examine the corresponding issue of reward (utility) function integrity in artificially intelligent machines. We survey the relevant literature and propose a number of potential solutions to ensure the integrity of our artificial assistants. Overall, we conclude that wireheading in rational self-improving optimisers above a certain capacity remains an unsolved problem despite opinion of many that such machines will choose not to wirehead. A relevant issue of literalness in goal setting also remains largely unsolved and we suggest that the development of a non-ambiguous knowledge transfer language might be a step in the right direction. (PsycInfo Database Record (c) 2020 APA, all rights reserved)",
		"container-title": "Journal of Experimental & Theoretical Artificial Intelligence",
		"DOI": "10.1080/0952813X.2014.895114",
		"ISSN": "1362-3079",
		"issue": "3",
		"note": "publisher-place: United Kingdom\npublisher: Taylor & Francis",
		"page": "373-389",
		"source": "APA PsycNet",
		"title": "Utility function security in artificially intelligent agents",
		"volume": "26",
		"author": [
			{
				"family": "Yampolskiy",
				"given": "Roman V."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "zemelLearningFairRepresentations2013",
		"type": "paper-conference",
		"abstract": "We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.",
		"container-title": "Proceedings of the 30th International Conference on Machine Learning",
		"event-title": "International Conference on Machine Learning",
		"language": "en",
		"note": "ISSN: 1938-7228",
		"page": "325-333",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Learning Fair Representations",
		"URL": "https://proceedings.mlr.press/v28/zemel13.html",
		"author": [
			{
				"family": "Zemel",
				"given": "Rich"
			},
			{
				"family": "Wu",
				"given": "Yu"
			},
			{
				"family": "Swersky",
				"given": "Kevin"
			},
			{
				"family": "Pitassi",
				"given": "Toni"
			},
			{
				"family": "Dwork",
				"given": "Cynthia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2013",
					5,
					26
				]
			]
		}
	},
	{
		"id": "thudiNecessityAuditableAlgorithmic2022",
		"type": "paper-conference",
		"abstract": "Machine unlearning, i.e. having a model forget about some of its training data, has become increasingly more important as privacy legislation promotes variants of the right-to-beforgotten. In the context of deep learning, approaches for machine unlearning are broadly categorized into two classes: exact unlearning methods, where an entity has formally removed the data point’s impact on the model by retraining the model from scratch, and approximate unlearning, where an entity approximates the model parameters one would obtain by exact unlearning to save on compute costs. In this paper, we first show that the definition that underlies approximate unlearning, which seeks to prove the approximately unlearned model is close to an exactly retrained model, is incorrect because one can obtain the same model using different datasets. Thus one could unlearn without modifying the model at all. We then turn to exact unlearning approaches and ask how to verify their claims of unlearning. Our results show that even for a given training trajectory one cannot formally prove the absence of certain data points used during training. We thus conclude that unlearning is only well-defined at the algorithmic level, where an entity’s only possible auditable claim to unlearning is that they used a particular algorithm designed to allow for external scrutiny during an audit.",
		"event-title": "31st USENIX Security Symposium (USENIX Security 22)",
		"ISBN": "978-1-939133-31-1",
		"language": "en",
		"page": "4007-4022",
		"source": "www.usenix.org",
		"title": "On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning",
		"URL": "https://www.usenix.org/conference/usenixsecurity22/presentation/thudi",
		"author": [
			{
				"family": "Thudi",
				"given": "Anvith"
			},
			{
				"family": "Jia",
				"given": "Hengrui"
			},
			{
				"family": "Shumailov",
				"given": "Ilia"
			},
			{
				"family": "Papernot",
				"given": "Nicolas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "gargFormalizingDataDeletion2020",
		"type": "paper-conference",
		"abstract": "The right of an individual to request the deletion of their personal data by an entity that might be storing it – referred to as the right to be forgotten – has been explicitly recognized, legislated, and exercised in several jurisdictions across the world, including the European Union, Argentina, and California. However, much of the discussion surrounding this right offers only an intuitive notion of what it means for it to be fulfilled – of what it means for such personal data to be deleted. In this work, we provide a formal definitional framework for the right to be forgotten using tools and paradigms from cryptography. In particular, we provide a precise definition of what could be (or should be) expected from an entity that collects individuals’ data when a request is made of it to delete some of this data. Our framework captures most, though not all, relevant aspects of typical systems involved in data processing. While it cannot be viewed as expressing the statements of current laws (especially since these are rather vague in this respect), our work offers technically precise definitions that represent possibilities for what the law could reasonably expect, and alternatives for what future versions of the law could explicitly require. Finally, with the goal of demonstrating the applicability of our framework and definitions, we consider various natural and simple scenarios where the right to be forgotten comes up. For each of these scenarios, we highlight the pitfalls that arise even in genuine attempts at implementing systems offering deletion guarantees, and also describe technological solutions that provably satisfy our definitions. These solutions bring together techniques built by various communities.",
		"container-title": "Advances in Cryptology – EUROCRYPT 2020: 39th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, May 10–14, 2020, Proceedings, Part II",
		"DOI": "10.1007/978-3-030-45724-2_13",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-030-45723-5",
		"page": "373–402",
		"publisher": "Springer-Verlag",
		"publisher-place": "Berlin, Heidelberg",
		"source": "ACM Digital Library",
		"title": "Formalizing Data Deletion in the Context of the Right to Be Forgotten",
		"URL": "https://doi.org/10.1007/978-3-030-45724-2_13",
		"author": [
			{
				"family": "Garg",
				"given": "Sanjam"
			},
			{
				"family": "Goldwasser",
				"given": "Shafi"
			},
			{
				"family": "Vasudevan",
				"given": "Prashant Nalini"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					5,
					10
				]
			]
		}
	},
	{
		"id": "carliniSecretSharerEvaluating2019",
		"type": "paper-conference",
		"abstract": "This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models--a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. In experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages.",
		"collection-title": "SEC'19",
		"container-title": "Proceedings of the 28th USENIX Conference on Security Symposium",
		"event-place": "USA",
		"ISBN": "978-1-939133-06-9",
		"page": "267–284",
		"publisher": "USENIX Association",
		"publisher-place": "USA",
		"source": "ACM Digital Library",
		"title": "The secret sharer: evaluating and testing unintended memorization in neural networks",
		"title-short": "The secret sharer",
		"author": [
			{
				"family": "Carlini",
				"given": "Nicholas"
			},
			{
				"family": "Liu",
				"given": "Chang"
			},
			{
				"family": "Erlingsson",
				"given": "Úlfar"
			},
			{
				"family": "Kos",
				"given": "Jernej"
			},
			{
				"family": "Song",
				"given": "Dawn"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					8,
					14
				]
			]
		}
	},
	{
		"id": "golatkarForgettingOutsideBox2020",
		"type": "paper-conference",
		"abstract": "We describe a procedure for removing dependency on a cohort of training data from a trained deep network that improves upon and generalizes previous methods to different readout functions, and can be extended to ensure forgetting in the final activations of the network. We introduce a new bound on how much information can be extracted per query about the forgotten cohort from a black-box network for which only the input-output behavior is observed. The proposed forgetting procedure has a deterministic part derived from the differential equations of a linearized version of the model, and a stochastic part that ensures information destruction by adding noise tailored to the geometry of the loss landscape. We exploit the connections between the final activations and weight dynamics of a DNN inspired by Neural Tangent Kernels to compute the information in the final activations.",
		"container-title": "Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIX",
		"DOI": "10.1007/978-3-030-58526-6_23",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-030-58525-9",
		"page": "383–398",
		"publisher": "Springer-Verlag",
		"publisher-place": "Berlin, Heidelberg",
		"source": "ACM Digital Library",
		"title": "Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations",
		"title-short": "Forgetting Outside the Box",
		"URL": "https://doi.org/10.1007/978-3-030-58526-6_23",
		"author": [
			{
				"family": "Golatkar",
				"given": "Aditya"
			},
			{
				"family": "Achille",
				"given": "Alessandro"
			},
			{
				"family": "Soatto",
				"given": "Stefano"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					8,
					23
				]
			]
		}
	},
	{
		"id": "guptaAdaptiveMachineUnlearning2021",
		"type": "article",
		"abstract": "Data deletion algorithms aim to remove the influence of deleted data points from trained models at a cheaper computational cost than fully retraining those models. However, for sequences of deletions, most prior work in the non-convex setting gives valid guarantees only for sequences that are chosen independently of the models that are published. If people choose to delete their data as a function of the published models (because they don't like what the models reveal about them, for example), then the update sequence is adaptive. In this paper, we give a general reduction from deletion guarantees against adaptive sequences to deletion guarantees against non-adaptive sequences, using differential privacy and its connection to max information. Combined with ideas from prior work which give guarantees for non-adaptive deletion sequences, this leads to extremely flexible algorithms able to handle arbitrary model classes and training methodologies, giving strong provable deletion guarantees for adaptive deletion sequences. We show in theory how prior work for non-convex models fails against adaptive deletion sequences, and use this intuition to design a practical attack against the SISA algorithm of Bourtoule et al. [2021] on CIFAR-10, MNIST, Fashion-MNIST.",
		"DOI": "10.48550/arXiv.2106.04378",
		"note": "arXiv:2106.04378 [cs, stat]",
		"number": "arXiv:2106.04378",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Adaptive Machine Unlearning",
		"URL": "http://arxiv.org/abs/2106.04378",
		"author": [
			{
				"family": "Gupta",
				"given": "Varun"
			},
			{
				"family": "Jung",
				"given": "Christopher"
			},
			{
				"family": "Neel",
				"given": "Seth"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Sharifi-Malvajerdi",
				"given": "Saeed"
			},
			{
				"family": "Waites",
				"given": "Chris"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					8
				]
			]
		}
	},
	{
		"id": "izzoApproximateDataDeletion2021",
		"type": "paper-conference",
		"abstract": "Deleting data from a trained machine learning (ML) model is a critical task in many applications. For example, we may want to remove the influence of training points that might be out of date or outliers. Regulations such as EU’s General Data Protection Regulation also stipulate that individuals can request to have their data deleted. The naive approach to data deletion is to retrain the ML model on the remaining data, but this is too time consuming. In this work, we propose a new approximate deletion method for linear and logistic models whose computational cost is linear in the the feature dimension d and independent of the number of training data n. This is a significant gain over all existing methods, which all have superlinear time dependence on the dimension. We also develop a new feature-injection test to evaluate the thoroughness of data deletion from ML models.",
		"container-title": "Proceedings of The 24th International Conference on Artificial Intelligence and Statistics",
		"event-title": "International Conference on Artificial Intelligence and Statistics",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "2008-2016",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Approximate Data Deletion from Machine Learning Models",
		"URL": "https://proceedings.mlr.press/v130/izzo21a.html",
		"author": [
			{
				"family": "Izzo",
				"given": "Zachary"
			},
			{
				"family": "Smart",
				"given": "Mary Anne"
			},
			{
				"family": "Chaudhuri",
				"given": "Kamalika"
			},
			{
				"family": "Zou",
				"given": "James"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					18
				]
			]
		}
	},
	{
		"id": "brophyMachineUnlearningRandom2021",
		"type": "paper-conference",
		"abstract": "Responding to user data deletion requests, removing noisy examples, or deleting corrupted training data are just a few reasons for wanting to delete instances from a machine learning (ML) model. However, efficiently removing this data from an ML model is generally difficult. In this paper, we introduce data removal-enabled (DaRE) forests, a variant of random forests that enables the removal of training data with minimal retraining. Model updates for each DaRE tree in the forest are exact, meaning that removing instances from a DaRE model yields exactly the same model as retraining from scratch on updated data. DaRE trees use randomness and caching to make data deletion efficient. The upper levels of DaRE trees use random nodes, which choose split attributes and thresholds uniformly at random. These nodes rarely require updates because they only minimally depend on the data. At the lower levels, splits are chosen to greedily optimize a split criterion such as Gini index or mutual information. DaRE trees cache statistics at each node and training data at each leaf, so that only the necessary subtrees are updated as data is removed. For numerical attributes, greedy nodes optimize over a random subset of thresholds, so that they can maintain statistics while approximating the optimal threshold. By adjusting the number of thresholds considered for greedy nodes, and the number of random nodes, DaRE trees can trade off between more accurate predictions and more efficient updates. In experiments on 13 real-world datasets and one synthetic dataset, we find DaRE forests delete data orders of magnitude faster than retraining from scratch while sacrificing little to no predictive power.",
		"container-title": "Proceedings of the 38th International Conference on Machine Learning",
		"event-title": "International Conference on Machine Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "1092-1104",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Machine Unlearning for Random Forests",
		"URL": "https://proceedings.mlr.press/v139/brophy21a.html",
		"author": [
			{
				"family": "Brophy",
				"given": "Jonathan"
			},
			{
				"family": "Lowd",
				"given": "Daniel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					1
				]
			]
		}
	},
	{
		"id": "wuDeltaGradRapidRetraining2020",
		"type": "paper-conference",
		"abstract": "Machine learning models are not static and may need to be retrained on slightly changed datasets, for instance, with the addition or deletion of a set of data points. This has many applications, including privacy, robustness, bias reduction, and uncertainty quantifcation. However, it is expensive to retrain models from scratch. To address this problem, we propose the DeltaGrad algorithm for rapid retraining machine learning models based on information cached during the training phase. We provide both theoretical and empirical support for the effectiveness of DeltaGrad, and show that it compares favorably to the state of the art.",
		"container-title": "Proceedings of the 37th International Conference on Machine Learning",
		"event-title": "International Conference on Machine Learning",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "10355-10366",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "DeltaGrad: Rapid retraining of machine learning models",
		"title-short": "DeltaGrad",
		"URL": "https://proceedings.mlr.press/v119/wu20b.html",
		"author": [
			{
				"family": "Wu",
				"given": "Yinjun"
			},
			{
				"family": "Dobriban",
				"given": "Edgar"
			},
			{
				"family": "Davidson",
				"given": "Susan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					11,
					21
				]
			]
		}
	},
	{
		"id": "chenWhenMachineUnlearning2021",
		"type": "paper-conference",
		"abstract": "The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known asmachine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. \\footnoteOur code is available at \\urlhttps://github.com/MinChen00/UnlearningLeaks.",
		"collection-title": "CCS '21",
		"container-title": "Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
		"DOI": "10.1145/3460120.3484756",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8454-4",
		"page": "896–911",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "When Machine Unlearning Jeopardizes Privacy",
		"URL": "https://dl.acm.org/doi/10.1145/3460120.3484756",
		"author": [
			{
				"family": "Chen",
				"given": "Min"
			},
			{
				"family": "Zhang",
				"given": "Zhikun"
			},
			{
				"family": "Wang",
				"given": "Tianhao"
			},
			{
				"family": "Backes",
				"given": "Michael"
			},
			{
				"family": "Humbert",
				"given": "Mathias"
			},
			{
				"family": "Zhang",
				"given": "Yang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					11,
					13
				]
			]
		}
	},
	{
		"id": "dworkPrivacypreservingPrediction2018",
		"type": "paper-conference",
		"abstract": "Ensuring differential privacy of models learned from sensitive user data is an important goal that has been studied extensively in recent years. It is now known that for some basic learning problems, especially those involving high-dimensional data, producing an accurate private model requires much more data than learning without privacy. At the same time, in many applications it is not necessary to expose the model itself. Instead users may be allowed to query the prediction model on their inputs only through an appropriate interface. Here we formulate the problem of ensuring privacy of individual predictions and investigate the overheads required to achieve it in several standard models of classification and regression. We first describe a simple baseline approach based on training several models on disjoint subsets of data and using standard private aggregation techniques to predict. We show that this approach has nearly optimal sample complexity for (realizable) PAC learning of any class of Boolean functions. At the same time, without strong assumptions on the data distribution, the aggregation step introduces a substantial overhead. We demonstrate that this overhead can be avoided for the well-studied class of thresholds on a line and for a number of standard settings of convex regression. The analysis of our algorithm for learning thresholds relies crucially on strong generalization guarantees that we establish for all differentially private prediction algorithms.",
		"container-title": "Proceedings of the 31st  Conference On Learning Theory",
		"event-title": "Conference On Learning Theory",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "1693-1702",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Privacy-preserving Prediction",
		"URL": "https://proceedings.mlr.press/v75/dwork18a.html",
		"author": [
			{
				"family": "Dwork",
				"given": "Cynthia"
			},
			{
				"family": "Feldman",
				"given": "Vitaly"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					7,
					3
				]
			]
		}
	},
	{
		"id": "wuPUMAPerformanceUnchanged2022",
		"type": "article",
		"abstract": "Preserving the performance of a trained model while removing unique characteristics of marked training data points is challenging. Recent research usually suggests retraining a model from scratch with remaining training data or refining the model by reverting the model optimization on the marked data points. Unfortunately, aside from their computational inefficiency, those approaches inevitably hurt the resulting model's generalization ability since they remove not only unique characteristics but also discard shared (and possibly contributive) information. To address the performance degradation problem, this paper presents a novel approach called Performance Unchanged Model Augmentation~(PUMA). The proposed PUMA framework explicitly models the influence of each training data point on the model's generalization ability with respect to various performance criteria. It then complements the negative impact of removing marked data by reweighting the remaining data optimally. To demonstrate the effectiveness of the PUMA framework, we compared it with multiple state-of-the-art data removal techniques in the experiments, where we show the PUMA can effectively and efficiently remove the unique characteristics of marked training data without retraining the model that can 1) fool a membership attack, and 2) resist performance degradation. In addition, as PUMA estimates the data importance during its operation, we show it could serve to debug mislabelled data points more efficiently than existing approaches.",
		"DOI": "10.48550/arXiv.2203.00846",
		"note": "arXiv:2203.00846 [cs, stat]",
		"number": "arXiv:2203.00846",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "PUMA: Performance Unchanged Model Augmentation for Training Data Removal",
		"title-short": "PUMA",
		"URL": "http://arxiv.org/abs/2203.00846",
		"author": [
			{
				"family": "Wu",
				"given": "Ga"
			},
			{
				"family": "Hashemi",
				"given": "Masoud"
			},
			{
				"family": "Srinivasa",
				"given": "Christopher"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					1
				]
			]
		}
	},
	{
		"id": "chundawatZeroShotMachineUnlearning2023",
		"type": "article-journal",
		"abstract": "Modern privacy regulations grant citizens the right to be forgotten by products, services and companies. In case of machine learning (ML) applications, this necessitates deletion of data not only from storage archives but also from ML models. Due to an increasing need for regulatory compliance required for ML applications, machine unlearning is becoming an emerging research problem. The right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ML model. Practical considerations preclude retraining of the model from scratch after discarding the deleted data. The few existing studies use either the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. However, strict regulatory compliance requires time-bound deletion of data. Thus, in many cases, no data related to the training process or training samples may be accessible even for the unlearning purpose. We therefore ask the question: is it possible to achieve unlearning with zero training samples? In this paper, we introduce the novel problem of zero-shot machine unlearning that caters for the extreme but practical scenario where zero original data samples are available for use. We then propose two novel solutions for zero-shot machine unlearning based on (a) error minimizing-maximizing noise and (b) gated knowledge transfer. These methods remove the information of the forget data from the model while maintaining the model efficacy on the retain data. The zero-shot approach offers good protection against the model inversion attacks and membership inference attacks. We introduce a new evaluation metric, Anamnesis Index (AIN) to effectively measure the quality of the unlearning method. The experiments show promising results for unlearning in deep learning models on benchmark vision data-sets. The source code is available here: <uri>https://github.com/ayu987/zero-shot-unlearning</uri>",
		"container-title": "IEEE Transactions on Information Forensics and Security",
		"DOI": "10.1109/TIFS.2023.3265506",
		"ISSN": "1556-6013",
		"journalAbbreviation": "Trans. Info. For. Sec.",
		"page": "2345–2354",
		"source": "ACM Digital Library",
		"title": "Zero-Shot Machine Unlearning",
		"URL": "https://doi.org/10.1109/TIFS.2023.3265506",
		"volume": "18",
		"author": [
			{
				"family": "Chundawat",
				"given": "Vikram S."
			},
			{
				"family": "Tarun",
				"given": "Ayush K."
			},
			{
				"family": "Mandal",
				"given": "Murari"
			},
			{
				"family": "Kankanhalli",
				"given": "Mohan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					1,
					1
				]
			]
		}
	},
	{
		"id": "zanella-beguelinAnalyzingInformationLeakage2020",
		"type": "paper-conference",
		"abstract": "To continuously improve quality and reflect changes in data, machine learning applications have to regularly retrain and update their core models. We show that a differential analysis of language model snapshots before and after an update can reveal a surprising amount of detailed information about changes in the training data. We propose two new metrics---\\emph{differential score} and \\emph{differential rank}---for analyzing the leakage due to updates of natural language models. We perform leakage analysis using these metrics across models trained on several different datasets using different methods and configurations. We discuss the privacy implications of our findings, propose mitigation strategies and evaluate their effect.",
		"container-title": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security",
		"DOI": "10.1145/3372297.3417880",
		"note": "arXiv:1912.07942 [cs, stat]",
		"page": "363-375",
		"source": "arXiv.org",
		"title": "Analyzing Information Leakage of Updates to Natural Language Models",
		"URL": "http://arxiv.org/abs/1912.07942",
		"author": [
			{
				"family": "Zanella-Béguelin",
				"given": "Santiago"
			},
			{
				"family": "Wutschitz",
				"given": "Lukas"
			},
			{
				"family": "Tople",
				"given": "Shruti"
			},
			{
				"family": "Rühle",
				"given": "Victor"
			},
			{
				"family": "Paverd",
				"given": "Andrew"
			},
			{
				"family": "Ohrimenko",
				"given": "Olga"
			},
			{
				"family": "Köpf",
				"given": "Boris"
			},
			{
				"family": "Brockschmidt",
				"given": "Marc"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					30
				]
			]
		}
	},
	{
		"id": "marchantHardForgetPoisoning2022",
		"type": "article",
		"abstract": "The right to erasure requires removal of a user's information from data held by organizations, with rigorous interpretations extending to downstream products such as learned models. Retraining from scratch with the particular user's data omitted fully removes its influence on the resulting model, but comes with a high computational cost. Machine \"unlearning\" mitigates the cost incurred by full retraining: instead, models are updated incrementally, possibly only requiring retraining when approximation errors accumulate. Rapid progress has been made towards privacy guarantees on the indistinguishability of unlearned and retrained models, but current formalisms do not place practical bounds on computation. In this paper we demonstrate how an attacker can exploit this oversight, highlighting a novel attack surface introduced by machine unlearning. We consider an attacker aiming to increase the computational cost of data removal. We derive and empirically investigate a poisoning attack on certified machine unlearning where strategically designed training data triggers complete retraining when removed.",
		"DOI": "10.48550/arXiv.2109.08266",
		"note": "arXiv:2109.08266 [cs]",
		"number": "arXiv:2109.08266",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Hard to Forget: Poisoning Attacks on Certified Machine Unlearning",
		"title-short": "Hard to Forget",
		"URL": "http://arxiv.org/abs/2109.08266",
		"author": [
			{
				"family": "Marchant",
				"given": "Neil G."
			},
			{
				"family": "Rubinstein",
				"given": "Benjamin I. P."
			},
			{
				"family": "Alfeld",
				"given": "Scott"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					2,
					9
				]
			]
		}
	},
	{
		"id": "liuHaveYouForgotten2020",
		"type": "paper-conference",
		"abstract": "In the era of deep learning, aggregation of data from several sources is a common approach to ensuring data diversity. Let us consider a scenario where several providers contribute data to a consortium for the joint development of a classification model (hereafter the target model), but, now one of the providers decides to leave. This provider requests that their data (hereafter the query dataset) be removed from the databases but also that the model ‘forgets’ their data. In this paper, for the first time, we want to address the challenging question of whether data have been forgotten by a model. We assume knowledge of the query dataset and the distribution of a model’s output. We establish statistical methods that compare the target’s outputs with outputs of models trained with different datasets. We evaluate our approach on several benchmark datasets (MNIST, CIFAR-10 and SVHN) and on a cardiac pathology diagnosis task using data from the Automated Cardiac Diagnosis Challenge (ACDC). We hope to encourage studies on what information a model retains and inspire extensions in more complex settings.",
		"container-title": "Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I",
		"DOI": "10.1007/978-3-030-59710-8_10",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-030-59709-2",
		"page": "95–105",
		"publisher": "Springer-Verlag",
		"publisher-place": "Berlin, Heidelberg",
		"source": "ACM Digital Library",
		"title": "Have You Forgotten? A Method to Assess if Machine Learning Models Have Forgotten Data",
		"title-short": "Have You Forgotten?",
		"URL": "https://doi.org/10.1007/978-3-030-59710-8_10",
		"author": [
			{
				"family": "Liu",
				"given": "Xiao"
			},
			{
				"family": "Tsaftaris",
				"given": "Sotirios A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					4
				]
			]
		}
	},
	{
		"id": "liuBackdoorDefenseMachine2022",
		"type": "paper-conference",
		"abstract": "Backdoor injection attack is an emerging threat to the security of neural networks, however, there still exist limited effective defense methods against the attack. In this paper, we propose BAERASER, a novel method that can erase the backdoor injected into the victim model through machine unlearning. Specifically, BAERASER mainly implements backdoor defense in two key steps. First, trigger pattern recovery is conducted to extract the trigger patterns infected by the victim model. Here, the trigger pattern recovery problem is equivalent to the one of extracting an unknown noise distribution from the victim model, which can be easily resolved by the entropy maximization based generative model. Subsequently, BAERASER leverages these recovered trigger patterns to reverse the backdoor injection procedure and induce the victim model to erase the polluted memories through a newly designed gradient ascent based machine unlearning method. Compared with the previous machine unlearning solutions, the proposed approach gets rid of the reliance on the full access to training data for retraining and shows higher effectiveness on backdoor erasing than existing fine-tuning or pruning methods. Moreover, experiments show that BAERASER can averagely lower the attack success rates of three kinds of state-of-the-art backdoor attacks by 99% on four benchmark datasets.",
		"container-title": "IEEE INFOCOM 2022 - IEEE Conference on Computer Communications",
		"DOI": "10.1109/INFOCOM48880.2022.9796974",
		"event-place": "London, United Kingdom",
		"page": "280–289",
		"publisher": "IEEE Press",
		"publisher-place": "London, United Kingdom",
		"source": "ACM Digital Library",
		"title": "Backdoor Defense with Machine Unlearning",
		"URL": "https://doi.org/10.1109/INFOCOM48880.2022.9796974",
		"author": [
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Fan",
				"given": "Mingyuan"
			},
			{
				"family": "Chen",
				"given": "Cen"
			},
			{
				"family": "Liu",
				"given": "Ximeng"
			},
			{
				"family": "Ma",
				"given": "Zhuo"
			},
			{
				"family": "Wang",
				"given": "Li"
			},
			{
				"family": "Ma",
				"given": "Jianfeng"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					2
				]
			]
		}
	},
	{
		"id": "leonardosExplorationexploitationMultiagentLearning2022",
		"type": "article-journal",
		"abstract": "Exploration-exploitation is a powerful and practical tool in multi-agent learning (MAL); however, its effects are far from understood. To make progress in this direction, we study a smooth analogue of Q-learning. We start by showing that our learning model has strong theoretical justification as an optimal model for studying exploration-exploitation. Specifically, we prove (1) that smooth Q-learning has bounded regret in arbitrary games for a cost model that explicitly balances game-rewards and exploration-costs, i.e., costs from testing potentially suboptimal actions, and (2) that it always converges to the set of quantal-response equilibria (QRE), the standard solution concept for games with bounded rationality, in arbitrary weighted potential games. In our main task, we then turn to measure the effect of exploration on collective system performance. We characterize the geometry of the QRE surface in low-dimensional MAL systems and link our findings with catastrophe (bifurcation) theory. In particular, as the exploration hyperparameter evolves over-time, the system undergoes phase transitions where the number and stability of equilibria can change radically given an infinitesimal change to the exploration parameter. Based on this, we provide a formal theoretical treatment of how tuning the exploration parameter can provably lead to equilibrium selection with both positive as well as negative (and potentially unbounded) effects to system performance.",
		"container-title": "Artificial Intelligence",
		"DOI": "10.1016/j.artint.2021.103653",
		"ISSN": "0004-3702",
		"journalAbbreviation": "Artificial Intelligence",
		"page": "103653",
		"source": "ScienceDirect",
		"title": "Exploration-exploitation in multi-agent learning: Catastrophe theory meets game theory",
		"title-short": "Exploration-exploitation in multi-agent learning",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0004370221002046",
		"volume": "304",
		"author": [
			{
				"family": "Leonardos",
				"given": "Stefanos"
			},
			{
				"family": "Piliouras",
				"given": "Georgios"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					1
				]
			]
		}
	},
	{
		"id": "zhangSparseAttentionLinear2021",
		"type": "article",
		"abstract": "Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off') for some queries, which is not possible with sparsified softmax alternatives.",
		"DOI": "10.48550/arXiv.2104.07012",
		"note": "arXiv:2104.07012 [cs]",
		"number": "arXiv:2104.07012",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Sparse Attention with Linear Units",
		"URL": "http://arxiv.org/abs/2104.07012",
		"author": [
			{
				"family": "Zhang",
				"given": "Biao"
			},
			{
				"family": "Titov",
				"given": "Ivan"
			},
			{
				"family": "Sennrich",
				"given": "Rico"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					1,
					28
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					10,
					6
				]
			]
		}
	}
]
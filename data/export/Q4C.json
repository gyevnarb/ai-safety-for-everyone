[
	{
		"id": "wadaRobustLabelPrediction2019",
		"type": "article-journal",
		"abstract": "This paper proposes a computationally efficient offline semi-supervised algorithm that yields a more accurate prediction than the label propagation algorithm, which is commonly used in online graphbased semi-supervised learning (SSL). Our proposed method is an offline method that is intended to assist online graph-based SSL algorithms. The efficacy of the tool in creating new learning algorithms of this type is demonstrated in numerical experiments. © 2019 The Institute of Electronics, Information and Communication Engineers.",
		"archive": "Scopus",
		"container-title": "IEICE Transactions on Information and Systems",
		"DOI": "10.1587/transinf.2018EDP7424",
		"issue": "8",
		"page": "1537-1545",
		"title": "Robust label prediction via label propagation and geodesic k-nearest neighbor in online semi-supervised learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071900727&doi=10.1587%2ftransinf.2018EDP7424&partnerID=40&md5=23a1a6f6aa6e5d808cce101d6bc77523",
		"volume": "E102D",
		"author": [
			{
				"family": "Wada",
				"given": "Y."
			},
			{
				"family": "Su",
				"given": "S."
			},
			{
				"family": "Kumagai",
				"given": "W."
			},
			{
				"family": "Kanamori",
				"given": "T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "kazantzidisHowTrainYour2022",
		"type": "paper-conference",
		"abstract": "Training reinforcement learning agents in real-world environments is costly, particularly for safety-critical applications. Human input can enable an agent to learn a good policy while avoiding unsafe actions, but at the cost of bothering the human with repeated queries. We present a model for safe learning in safety-critical environments from human input that minimises bother cost. Our model, JPAL-HA, proposes an efficient mechanism to harness human preferences and justifications to significantly improve safety during the learning process without increasing the number of interactions with a user. We show this with both simulation and human experiments. © 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",
		"archive": "Scopus",
		"event-title": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",
		"page": "1654-1656",
		"title": "How to Train Your Agent: Active Learning from Human Preferences and Justifications in Safety-Critical Environments",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134306740&partnerID=40&md5=76a61115b19501a21f7673129a6cff87",
		"volume": "3",
		"author": [
			{
				"family": "Kazantzidis",
				"given": "I."
			},
			{
				"family": "Norman",
				"given": "T.J."
			},
			{
				"family": "Du",
				"given": "Y."
			},
			{
				"family": "Freeman",
				"given": "C.T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "luFamilyRobustStochastic2019",
		"type": "paper-conference",
		"abstract": "We consider a new family of stochastic operators for reinforcement learning that seeks to alleviate negative effects and become more robust to approximation or estimation errors. Theoretical results are established, showing that our family of operators preserve optimality and increase the action gap in a stochastic sense. Empirical results illustrate the strong benefits of our robust stochastic operators, significantly outperforming the classical Bellman and recently proposed operators. © 2019 Neural information processing systems foundation. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Advances in Neural Information Processing Systems",
		"title": "A family of robust stochastic operators for reinforcement learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090177881&partnerID=40&md5=a53a7206f40717efe60cd4f5353df697",
		"volume": "32",
		"author": [
			{
				"family": "Lu",
				"given": "Y."
			},
			{
				"family": "Squillante",
				"given": "M.S."
			},
			{
				"family": "Wu",
				"given": "C.W."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "schmidBatchlikeOnlineLearning2021",
		"type": "paper-conference",
		"abstract": "Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org)",
		"archive": "Scopus",
		"event-title": "CEUR Workshop Proceedings",
		"title": "Batch-like online learning for more robust hybrid artificial intelligence: Deconstruction as a machine learning process",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104622356&partnerID=40&md5=04edd84470671c7e18a70da9f07e9d78",
		"volume": "2846",
		"author": [
			{
				"family": "Schmid",
				"given": "T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "cornelissenReflectionMachinesIncreasing2022",
		"type": "article-journal",
		"abstract": "Rapid developments in Artificial Intelligence are leading to an increasing human reliance on machine decision making. Even in collaborative efforts with Decision Support Systems (DSSs), where a human expert is expected to make the final decisions, it can be hard to keep the expert actively involved throughout the decision process. DSSs suggest their own solutions and thus invite passive decision making. To keep humans actively 'on' the decision-making loop and counter overreliance on machines, we propose a 'reflection machine' (RM). This system asks users questions about their decision strategy and thereby prompts them to evaluate their own decisions critically. We discuss what forms RMs can take and present a proof-of-concept implementation of a RM that can produce feedback on users' decisions in the medical and law domains. We show that the prototype requires very little domain knowledge to create reasonably intelligent critiquing questions. With this prototype, we demonstrate the technical feasibility to develop RMs and hope to pave the way for future research into their effectiveness and value.",
		"archive_location": "WOS:000781340300001",
		"container-title": "ETHICS AND INFORMATION TECHNOLOGY",
		"DOI": "10.1007/s10676-022-09645-y",
		"ISSN": "1388-1957",
		"issue": "2",
		"title": "Reflection machines: increasing meaningful human control over Decision Support Systems",
		"volume": "24",
		"author": [
			{
				"family": "Cornelissen",
				"given": "NAJ"
			},
			{
				"family": "Eerdt",
				"given": "RJM",
				"non-dropping-particle": "van"
			},
			{
				"family": "Schraffenberger",
				"given": "HK"
			},
			{
				"family": "Haselager",
				"given": "WFG"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022",
					6
				]
			]
		}
	},
	{
		"id": "kunduDetectingFunctionalSafety2022",
		"type": "paper-conference",
		"abstract": "With the ubiquitous deployment of Deep Neural Networks (DNNs) in low latency mission critical applications, there has been an extensive proliferation of custom-built AI inference accelerators at the edge. Drastic technology scaling in recent years has made these circuits highly vulnerable to faults due to various reasons like aging, latent defects, single event upsets, etc. Such faults are highly detrimental to the classification accuracy of the AI accelerator, leading to the critical Functional Safety (FuSa) violation, when used in mission-critical applications. In order to detect such violations in mission mode, we analyze the efficiency of a software-based self test scheme that employs functional test patterns, akin to instances in the application dataset. Such patterns are either selected from the dataset of the DNN, or generated from scratch utilizing the concept of Generative Adversarial Networks (GANs). When evaluated on state-of-the-art DNNs on multivariate exhaustive datasets, the GAN generated test patterns significantly improve FuSa violation detection coverage by up to 130.28%, compared to the selected test patterns, thereby accomplishing efficient testing of the AI accelerator, online, in mission mode.",
		"archive_location": "WOS:000865857100024",
		"DOI": "10.1109/IOLTS56730.2022.9897702",
		"event-title": "2022 IEEE 28TH INTERNATIONAL SYMPOSIUM ON ON-LINE TESTING AND ROBUST SYSTEM DESIGN (IOLTS 2022)",
		"ISBN": "1942-9398",
		"title": "Detecting Functional Safety Violations in Online AI Accelerators",
		"author": [
			{
				"family": "Kundu",
				"given": "S"
			},
			{
				"family": "Basu",
				"given": "K"
			}
		],
		"editor": [
			{
				"family": "Savino",
				"given": "A"
			},
			{
				"family": "Rech",
				"given": "P"
			},
			{
				"family": "DiCarlo",
				"given": "S"
			},
			{
				"family": "Gizopoulos",
				"given": "D"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "harlandAIApologyInteractive2023",
		"type": "article-journal",
		"abstract": "For an Artificially Intelligent (AI) system to maintain alignment between human desires and its behaviour, it is important that the AI account for human preferences. This paper proposes and empirically evaluates the first approach to aligning agent behaviour to human preference via an apologetic framework. In practice, an apology may consist of an acknowledgement, an explanation and an intention for the improvement of future behaviour. We propose that such an apology, provided in response to recognition of undesirable behaviour, is one way in which an AI agent may both be transparent and trustworthy to a human user. Furthermore, that behavioural adaptation as part of apology is a viable approach to correct against undesirable behaviours. The Act-Assess-Apologise framework potentially could address both the practical and social needs of a human user, to recognise and make reparations against prior undesirable behaviour and adjust for the future. Applied to a dual-auxiliary impact minimisation problem, the apologetic agent had a near perfect determination and apology provision accuracy in several non-trivial configurations. The agent subsequently demonstrated behaviour alignment with success that included up to complete avoidance of the impacts described by these objectives in some scenarios.",
		"archive_location": "WOS:000973380900004",
		"container-title": "NEURAL COMPUTING & APPLICATIONS",
		"DOI": "10.1007/s00521-023-08586-x",
		"ISSN": "0941-0643",
		"issue": "23",
		"page": "16917-16930",
		"title": "AI apology: interactive multi-objective reinforcement learning for human-aligned AI",
		"volume": "35",
		"author": [
			{
				"family": "Harland",
				"given": "H"
			},
			{
				"family": "Dazeley",
				"given": "R"
			},
			{
				"family": "Nakisa",
				"given": "B"
			},
			{
				"family": "Cruz",
				"given": "F"
			},
			{
				"family": "Vamplew",
				"given": "P"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023",
					8
				]
			]
		}
	},
	{
		"id": "zhaoDetectingOperationalAdversarial2021",
		"type": "paper-conference",
		"abstract": "The utilisation of Deep Learning (DL) raises new challenges regarding its dependability in critical applications. Sound verification and validation methods are needed to assure the safe and reliable use of DL. However, state-of-the-art debug testing methods on DL that aim at detecting adversarial examples (AEs) ignore the operational profile, which statistically depicts the software's future operational use. This may lead to very modest effectiveness on improving the software's delivered reliability, as the testing budget is likely to be wasted on detecting AEs that are unrealistic or encountered very rarely in real-life operation. In this paper, we first present the novel notion of \"operational AEs\" which are AEs that have relatively high chance to be seen in future operation. Then an initial design of a new DL testing method to efficiently detect \"operational AEs\" is provided, as well as some insights on our prospective research plan.",
		"archive_location": "WOS:000701459900003",
		"DOI": "10.1109/DSN-S52858.2021.00013",
		"event-title": "51ST ANNUAL IEEE/IFIP INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS - SUPPLEMENTAL VOL (DSN 2021)",
		"ISBN": "1530-0889",
		"page": "5-6",
		"title": "Detecting Operational Adversarial Examples for Reliable Deep Learning",
		"author": [
			{
				"family": "Zhao",
				"given": "XY"
			},
			{
				"family": "Huang",
				"given": "W"
			},
			{
				"family": "Schewe",
				"given": "S"
			},
			{
				"family": "Dong",
				"given": "Y"
			},
			{
				"family": "Huang",
				"given": "XW"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "liuSafeReinforcementLearning2023a",
		"type": "paper-conference",
		"abstract": "Safety is a fundamental property for the realworld deployment of robotic platforms. Any control policy should avoid dangerous actions that could harm the environment, humans, or the robot itself. In reinforcement learning (RL), safety is crucial when exploring a new environment to learn a new skill. This paper introduces a new formulation of safe exploration for robotic RL in the tangent space of the constraint manifold that effectively transforms the action space of the RL agent for always respecting safety constraints locally. We show how to apply this approach to a wide range of robotic platforms and how to define safety constraints that represent dynamic articulated objects like humans in the context of robotic RL. Our proposed approach achieves state-of-the-art performance in simulated high-dimensional and dynamic tasks while avoiding collisions with the environment. We show safe real-world deployment of our learned controller on a TIAGo++ robot, achieving remarkable performance in manipulation and human-robot interaction tasks.",
		"archive_location": "WOS:001048371102021",
		"DOI": "10.1109/ICRA48891.2023.10161548",
		"event-title": "2023 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2023)",
		"ISBN": "1050-4729",
		"page": "9449-9456",
		"title": "Safe Reinforcement Learning of Dynamic High-Dimensional Robotic Tasks: Navigation, Manipulation, Interaction",
		"author": [
			{
				"family": "Liu",
				"given": "PZ"
			},
			{
				"family": "Zhang",
				"given": "K"
			},
			{
				"family": "Tateo",
				"given": "D"
			},
			{
				"family": "Jauhri",
				"given": "S"
			},
			{
				"family": "Hu",
				"given": "ZY"
			},
			{
				"family": "Peters",
				"given": "J"
			},
			{
				"family": "Chalvatzaki",
				"given": "G"
			},
			{
				"literal": "IEEE"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ramakrishnanDiscoveringBlindSpots2018",
		"type": "paper-conference",
		"abstract": "Agents trained in simulation may make errors in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult to discover because the agent cannot predict them a priori. We propose using oracle feedback to learn a predictive model of these blind spots to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: The agent does not have the appropriate features to represent the true state of the world and thus cannot distinguish among numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. We learn models to predict blind spots in unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. The models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach on two domains and show that it achieves higher predictive performance than baseline methods, and that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how they influence the discovery of blind spots.",
		"archive_location": "WOS:000468231300121",
		"event-title": "PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18)",
		"ISBN": "978-1-4503-5649-7",
		"page": "1017-1025",
		"title": "Discovering Blind Spots in Reinforcement Learning",
		"DOI": "10.5555/3237383.3237849",
		"author": [
			{
				"family": "Ramakrishnan",
				"given": "R"
			},
			{
				"family": "Kamar",
				"given": "E"
			},
			{
				"family": "Dey",
				"given": "D"
			},
			{
				"family": "Shah",
				"given": "J"
			},
			{
				"family": "Horvitz",
				"given": "E"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "turchettaSafeExplorationInteractive2019",
		"type": "paper-conference",
		"abstract": "In Interactive Machine Learning (IML), we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods, e.g., Bayesian optimization and active learning, have been successful in applications, on real-world systems they must provably avoid unsafe decisions. To this end, safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient, as it explores decisions that are not relevant for the original IML objective. In this paper, we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result, we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP), which have been analyzed separately before. Our method outperforms other algorithms empirically.",
		"archive_location": "WOS:000534424302084",
		"event-title": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)",
		"ISBN": "1049-5258",
		"title": "Safe Exploration for Interactive Machine Learning",
		"DOI": "10.5555/3454287.3454547",
		"volume": "32",
		"author": [
			{
				"family": "Turchetta",
				"given": "M"
			},
			{
				"family": "Berkenkamp",
				"given": "F"
			},
			{
				"family": "Krause",
				"given": "A"
			}
		],
		"editor": [
			{
				"family": "Wallach",
				"given": "H"
			},
			{
				"family": "Larochelle",
				"given": "H"
			},
			{
				"family": "Beygelzimer",
				"given": "A"
			},
			{
				"family": "Buc",
				"given": "F",
				"non-dropping-particle": "d'Alche-"
			},
			{
				"family": "Fox",
				"given": "E"
			},
			{
				"family": "Garnett",
				"given": "R"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "wangOnlineRobustReinforcement2021",
		"type": "paper-conference",
		"abstract": "Robust reinforcement learning (RL) is to find a policy that optimizes the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on model-free robust RL, where the uncertainty set is defined to be centering at a misspecified MDP that generates a single sample trajectory sequentially, and is assumed to be unknown. We develop a sample-based approach to estimate the unknown uncertainty set, and design robust Q-learning algorithm (tabular case) and robust TDC algorithm (function approximation setting), which can be implemented in an online and incremental fashion. For the robust Q-learning algorithm, we prove that it converges to the optimal robust Q function, and for the robust TDC algorithm, we prove that it converges asymptotically to some stationary points. Unlike the results in [Roy et al., 2017], our algorithms do not need any additional conditions on the discount factor to guarantee the convergence. We further characterize the finite-time error bounds of the two algorithms, and show that both the robust Q-learning and robust TDC algorithms converge as fast as their vanilla counterparts (within a constant factor). Our numerical experiments further demonstrate the robustness of our algorithms. Our approach can be readily extended to robustify many other algorithms, e.g., TD, SARSA, and other GTD algorithms. © 2021 Neural information processing systems foundation. All rights reserved.",
		"archive": "Scopus",
		"event-title": "Advances in Neural Information Processing Systems",
		"page": "7193-7206",
		"title": "Online Robust Reinforcement Learning with Model Uncertainty",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130775690&partnerID=40&md5=ac00e2eb7a717c085e21d9699fe68881",
		"volume": "9",
		"author": [
			{
				"family": "Wang",
				"given": "Y."
			},
			{
				"family": "Zou",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ravichandranRobustAutomaticTarget2007",
		"type": "article-journal",
		"abstract": "This work developed and demonstrated a machine learning approach for robust ATR. The primary innovation of this work was the development of an automated way of developing inference rules that can draw on multiple models and multiple feature types to make robust ATR decisions. The key realization is that this \"meta learning\" problem is one of structural learning, and that it can be conducted independently of parameter learning associated with each model and feature based technique. This was accomplished by using a learning classifier system, which is based on genetics-based machine learning, for the ill conditioned combinatorial problem of structural rule learning, while using statistical and mathematical techniques for parameter learning. This system was tested on MSTAR Public Release SAR data using standard and extended operation conditions. These results were also compared against two baseline classifiers, a PCA based distance classifier and a MSE classifier. The classifiers were evaluated for accuracy (via training set classification) and robustness (via testing set classification). In both cases, the LCS based robust ATR system performed well with accuracy over 99% and robustness over 80%. © 2006 Elsevier B.V. All rights reserved.",
		"archive": "Scopus",
		"container-title": "Information Fusion",
		"DOI": "10.1016/j.inffus.2006.03.001",
		"issue": "3",
		"page": "252-265",
		"title": "Robust automatic target recognition using learning classifier systems",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947672844&doi=10.1016%2fj.inffus.2006.03.001&partnerID=40&md5=518bef4bc6bad38b249e8a52e4abf601",
		"volume": "8",
		"author": [
			{
				"family": "Ravichandran",
				"given": "B."
			},
			{
				"family": "Gandhe",
				"given": "A."
			},
			{
				"family": "Smith",
				"given": "R."
			},
			{
				"family": "Mehra",
				"given": "R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2007"
				]
			]
		}
	},
	{
		"id": "rileyAssuredMultiagentReinforcement2022",
		"type": "paper-conference",
		"abstract": "Multi-agent reinforcement learning facilitates agents learning to solve complex decision-making problems requiring collaboration. However, reinforcement learning methods are underpinned by stochastic mechanisms, making them unsuitable for safety-critical domains. To solve this issue, approaches such as assured multi-agent reinforcement learning, which utilises quantitative verification to produce formal guarantees of safety requirements during the agents learning process, have been developed. However, this approach relies on accurate knowledge about the environment to be effectively used which can be detrimental if this knowledge is inaccurate. Therefore, we developed an extension to assured multi-agent reinforcement learning called agent interaction driven adaptability, an automated process to securing reliable safety constraints, allowing inaccurate and missing knowledge to be used without detriment. Our preliminary results showcase the ability of agent interaction driven adaptability to allow safe multi-agent reinforcement learning to be utilised in safety-critical scenarios. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",
		"archive": "Scopus",
		"DOI": "10.1007/978-981-19-3444-5_8",
		"event-title": "Smart Innovation, Systems and Technologies",
		"page": "87-97",
		"title": "Assured Multi-agent Reinforcement Learning with Robust Agent-Interaction Adaptability",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135905227&doi=10.1007%2f978-981-19-3444-5_8&partnerID=40&md5=a24f1b8b8ff87d93118c4e69e71c214d",
		"volume": "309",
		"author": [
			{
				"family": "Riley",
				"given": "J."
			},
			{
				"family": "Calinescu",
				"given": "R."
			},
			{
				"family": "Paterson",
				"given": "C."
			},
			{
				"family": "Kudenko",
				"given": "D."
			},
			{
				"family": "Banks",
				"given": "A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "millan-ariasRobustApproachContinuous2021",
		"type": "article-journal",
		"abstract": "Reinforcement learning refers to a machine learning paradigm in which an agent interacts with the environment to learn how to perform a task. The characteristics of the environment may change over time or be affected by disturbances not controlled, avoiding the agent finding a proper policy. Some approaches attempt to address these problems, as interactive reinforcement learning, where an external entity helps the agent learn through advice. Other approaches, such as robust reinforcement learning, allow the agent to learn the task, acting in a disturbed environment. In this paper, we propose an approach that addresses interactive reinforcement learning problems in a dynamic environment, where advice provides information on the task and the dynamics of the environment. Thus, an agent learns a policy in a disturbed environment while receiving advice. We implement our approach in the dynamic version of the cart-pole balancing task and a simulated robotic arm dynamic environment to organize objects. Our results show that the proposed approach allows an agent to complete the task satisfactorily in a dynamic, continuous state-action domain. Moreover, experimental results suggest agents trained with our approach are less sensitive to changes in the characteristics of the environment than interactive reinforcement learning agents.  © 2013 IEEE.",
		"archive": "Scopus",
		"container-title": "IEEE Access",
		"DOI": "10.1109/ACCESS.2021.3099071",
		"page": "104242-104260",
		"title": "A Robust Approach for Continuous Interactive Actor-Critic Algorithms",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111607924&doi=10.1109%2fACCESS.2021.3099071&partnerID=40&md5=55aa30222b5a34ada19cd98c7718390d",
		"volume": "9",
		"author": [
			{
				"family": "Millan-Arias",
				"given": "C.C."
			},
			{
				"family": "Fernandes",
				"given": "B.J.T."
			},
			{
				"family": "Cruz",
				"given": "F."
			},
			{
				"family": "Dazeley",
				"given": "R."
			},
			{
				"family": "Fernandes",
				"given": "S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "dingProvablyEfficientGeneralized2023",
		"type": "paper-conference",
		"abstract": "We examine online safe multi-agent reinforcement learning using constrained Markov games in which agents compete by maximizing their expected total rewards under a constraint on expected total utilities. Our focus is confined to an episodic two-player zero-sum constrained Markov game with independent transition functions that are unknown to agents, adversarial reward functions, and stochastic utility functions. For such a Markov game, we employ an approach based on the occupancy measure to formulate it as an online constrained saddle-point problem with an explicit constraint. We extend the Lagrange multiplier method in constrained optimization to handle the constraint by creating a generalized Lagrangian with minimax decision primal variables and a dual variable. Next, we develop an upper confidence reinforcement learning algorithm to solve this Lagrangian problem while balancing exploration and exploitation. Our algorithm updates the minimax decision primal variables via online mirror descent and the dual variable via projected gradient step and we prove that it enjoys sublinear rate O((|X| + |Y |)LpT(|A| + |B|))) for both regret and constraint violation after playing T episodes of the game. Here, L is the horizon of each episode, (|X|, |A|) and (|Y |, |B|) are the state/action space sizes of the min-player and the max-player, respectively. To the best of our knowledge, we provide the first provably efficient online safe reinforcement learning algorithm in constrained Markov games. © 2023 D. Ding, X. Wei, Z. Yang, Z. Wang & M.R. Jovanović.",
		"archive": "Scopus",
		"event-title": "Proceedings of Machine Learning Research",
		"page": "315-332",
		"title": "Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning",
		"URL": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172894839&partnerID=40&md5=cdac5fce5f7bd75455c6185abb95092b",
		"volume": "211",
		"author": [
			{
				"family": "Ding",
				"given": "D."
			},
			{
				"family": "Wei",
				"given": "X."
			},
			{
				"family": "Yang",
				"given": "Z."
			},
			{
				"family": "Wang",
				"given": "Z."
			},
			{
				"family": "Jovanović",
				"given": "M.R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	}
]
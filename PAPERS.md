# AI Safety Paper Survey

This is a collection of all papers retrieved either through our systematic survey on 1 November, 2023, or through snowballing from 12 seed papers (shown below).

### Seed Papers (N: 12)

| Title | Authors | Year |
| --- | --- | --- |
| [AI Safety via Debate](https://doi.org/10.48550/arXiv.1805.00899) | Irving, Geoffrey and Christiano, Paul and Amodei, Dario | 2018 |
| [Algorithms for Inverse Reinforcement Learning](https://ai.stanford.edu/~ang/papers/icml00-irl.pdf) | Ng, Andrew Y. and Russell, Stuart J. | 2000 |
| [Aligning AI With Shared Human Values](https://doi.org/10.48550/arXiv.2008.02275) | Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob | 2023 |
| [Artificial Intelligence Safety and Cybersecurity: A Timeline of AI Failures](https://doi.org/10.48550/arXiv.1610.07997) | Yampolskiy, Roman V. | 2016 |
| [Concrete Problems in AI Safety](https://doi.org/10.48550/arXiv.1606.06565) | Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan | 2016 |
| [Cooperative Inverse Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html) | Hadfield-Menell, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca | 2016 |
| [Machine Unlearning: A Survey](https://doi.org/10.1145/3603620) | Xu, Heng and Zhu, Tianqing and Zhang, Lefeng and Zhou, Wanlei and Yu, Philip S. | 2023 |
| [Research Priorities for Robust and Beneficial Artificial Intelligence](https://doi.org/10.48550/arXiv.1602.03506) | Russell, Stuart and Dewey, Daniel and Tegmark, Max | 2016 |
| [Safety Concerns and Mitigation Approaches Regarding the Use of Deep Learning in Safety-Critical Perception Tasks](https://doi.org/10.1007/978-3-030-55583-2_25) | Willers, Oliver and Sudholt, Sebastian and Raafatnia, Shervin and Abrecht, Stephanie | 2020 |
| [Taxonomy of Machine Learning Safety: A Survey and Primer](http://arxiv.org/abs/2106.04823) | Mohseni, Sina and Wang, Haotao and Yu, Zhiding and Xiao, Chaowei and Wang, Zhangyang and Yadawa, Jay | 2022 |
| [Toy Models of Superposition](https://doi.org/10.48550/arXiv.2209.10652) | Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher | 2022 |
| [Unsolved Problems in ML Safety](https://doi.org/10.48550/arXiv.2109.13916) | Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob | 2022 |

### Papers (N: 383)

| Title | Authors | Risk Types | Proposed Methods |
| --- | --- | --- | --- |
| [A "Do No Harm" Novel Safety Checklist and Research Approach to Determine Whether to Launch an Artificial Intelligence-Based Medical Technology: Introducing the Biological-Psychological, Economic, and Social (BPES) Framework](https://doi.org/10.2196/43386) | Khan, W.U. and Seto, E. | Lack of Monitoring | Design Framework |
| [A Concept for Dynamic and Robust Machine Learning With Context Modeling for Heterogeneous Manufacturing Data](https://doi.org/10.1016/j.procir.2023.06.061) | Kamm, S. and Sahlab, N. and Jazdi, N. and Weyrich, M. | System Misspecification | Design Framework |
| [A Contact-Safe Reinforcement Learning Framework for Contact-Rich Robot Manipulation](https://doi.org/10.1109/IROS47612.2022.9981185) | Zhu, X. and Kang, S.C. and Chen, J.Y. | Lack of Monitoring | Real-World Testing |
| [A Deep Reinforcement Learning Framework With Formal Verification](https://doi.org/10.1145/3577204) | Boudi, Z. and Wakrime, A.A. and Toub, M. and Haloua, M. | Lack of Monitoring | Simulated Agents |
| [A Deep Safe Reinforcement Learning Approach for Mapless Navigation](https://doi.org/10.1109/ROBIO54168.2021.9739251) | Lv, S.H. and Li, Y.J. and Liu, Q. and Gao, J.Q. and Pang, X.Z. and Chen, M.L. | Lack of Monitoring | Simulated Agents |
| [A Deployment Model to Extend Ethically Aligned AI Implementation Method ECCOLA](https://doi.org/10.1109/REW53955.2021.00043) | Antikainen, J. and Agbese, M. and Alanen, H.K. and Halme, E. and Isomäki, H. and Jantunen, M. and Kemell, K.K. and Rousi, R. and Vainio-Pekka, H. and Vakkuri, V. | Lack of Monitoring | Design Framework |
| [A Dirichlet Process Mixture of Robust Task Models for Scalable Lifelong Reinforcement Learning](https://doi.org/10.1109/TCYB.2022.3170485) | Wang, Z. and Chen, C. and Dong, D. | System Misspecification, Non-Stationary Distribution | Simulated Agents |
| [A Discrepancy Aware Framework for Robust Anomaly Detection](https://doi.org/10.1109/TII.2023.3318302) | Cai, Y. and Liang, D. and Luo, D. and He, X. and Yang, X. and Bai, X. | Adversarial Attack, Noise and Outliers | Applied Algorithm |
| [A Framework for Building Uncertainty Wrappers for AI/ML-Based Data-Driven Components](https://doi.org/10.1007/978-3-030-55583-2_23) | Kläs, M. and Jöckel, L. | Non-Stationary Distribution | Analysis Framework |
| [A Hierarchical HAZOP-Like Safety Analysis for Learning-Enabled Systems](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139381061&partnerID=40&md5=80164f44c1c28764025cd0045f7ff701) | Qi, Y. and Conmy, P.R. and Huang, W. and Zhao, X. and Huang, X. | System Misspecification | Analysis Framework |
| [A Maturity Model for Trustworthy AI Software Development](https://doi.org/10.3390/app13084771) | Cho, S. and Kim, I. and Kim, J. and Woo, H. and Shin, W. | System Misspecification | Analysis Framework |
| [A Methodology for Evaluating the Robustness of Anomaly Detectors to Adversarial Attacks in Industrial Scenarios](https://doi.org/10.1109/ACCESS.2022.3224930) | Perales Gomez, A.L. and Maimo, L.F. and Clemente, F.J.G. and Morales, J.A.M. and Celdran, A.H. and Bovet, G. | Adversarial Attack | Dataset |
| [A Model Selection Approach for Corruption Robust Reinforcement Learning](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163731154&partnerID=40&md5=be608eb3822f8bbe3c87072cb3e0d7fd) | Wei, C.-.Y. and Dann, C. and Zimmert, J. | Undesirable Behaviour, Adversarial Attack | Theoretical Algorithm |
| [A Multi-Layered Collaborative Framework for Evidence-Driven Data Requirements Engineering for Machine Learning-Based Safety-Critical Systems](https://doi.org/10.1145/3555776.3577647) | Dey, S. and Lee, S.-.W. | System Misspecification | Design Framework |
| [A Multi-Level Semantic Web for Hard-to-Specify Domain Concept, Pedestrian, in ML-based Software](https://doi.org/10.1007/s00766-021-00366-0) | Barzamini, H. and Shahzad, M. and Alhoori, H. and Rahimi, M. | Lack of Monitoring, System Misspecification | Applied Algorithm |
| [A Near-Optimal Algorithm for Safe Reinforcement Learning Under Instantaneous Hard Constraints](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174407935&partnerID=40&md5=e136ba097637fe4a30217c735b53fb7b) | Shi, M. and Liang, Y. and Shroff, N. | Unsafe Exploration | Theoretical Algorithm |
| [A Novel Composite Graph Neural Network](https://doi.org/10.1109/TNNLS.2023.3268766) | Liu, Z. and Yang, J. and Zhong, X. and Wang, W. and Chen, H. and Chang, Y. | Noise and Outliers | Applied Algorithm |
| [A Principal Component Analysis Approach for Embedding Local Symmetries Into Deep Learning Algorithms](https://doi.org/10.1007/978-3-030-55583-2_22) | Lagrave, P.-.Y. | Noise and Outliers | Theoretical Algorithm |
| [A Real-World Reinforcement Learning Framework for Safe and Human-Like Tactical Decision-Making](https://doi.org/10.1109/TITS.2023.3292981) | Yavas, M.U. and Kumbasar, T. and Ure, N.K. | Non-Stationary Distribution | Simulated Agents |
| [A Reinforcement Learning Architecture That Transfers Knowledge Between Skills When Solving Multiple Tasks](https://doi.org/10.1109/TCDS.2016.2607018) | Tommasino, P. and Caligiore, D. and Mirolli, M. and Baldassarre, G. | Non-Stationary Distribution | Simulated Agents |
| [A Risk Degree-Based Safe Semi-Supervised Learning Algorithm](https://doi.org/10.1007/s13042-015-0416-8) | Gan, H.T. and Luo, Z.Z. and Meng, M. and Ma, Y.L. and She, Q.S. | Adversarial Attack | Theoretical Algorithm |
| [A Robust Approach to Model-Based Classification Based on Trimming and Constraints: Semi-Supervised Learning in Presence of Outliers and Label Noise](https://doi.org/10.1007/s11634-019-00371-w) | Cappozzo, A. and Greselin, F. and Murphy, T.B. | Noise and Outliers | Applied Algorithm |
| [A Robust Automated Machine Learning System With Pseudoinverse Learning](https://doi.org/10.1007/s12559-021-09853-6) | Wang, K. and Guo, P. | Noise and Outliers | Simulated Agents |
| [A Robust Double-Parallel Extreme Learning Machine Based on an Improved M-Estimation Algorithm](https://doi.org/10.1016/j.aei.2022.101606) | Zha, L. and Ma, K. and Li, G. and Fang, Q. and Hu, X. | Noise and Outliers | Applied Algorithm |
| [A Robust Extreme Learning Machine Based on Adaptive Loss Function for Regression Modeling](https://doi.org/10.1007/s11063-023-11340-y) | Zhang, F. and Chen, S. and Hong, Z. and Shan, B. and Xu, Q. | Noise and Outliers | Applied Algorithm |
| [A Robust Extreme Learning Machine Framework for Uncertain Data Classification](https://doi.org/10.1007/s11227-018-2430-6) | Jing, S. and Yang, L. | System Misspecification, Noise and Outliers | Applied Algorithm |
| [A Robust Learning Methodology for Uncertainty-Aware Scientific Machine Learning Models](https://doi.org/10.3390/math11010074) | Costa, E.A. and Rebello, C.D.M. and Fontana, M. and Schnitman, L. and Nogueira, I.B.D.R. | Non-Stationary Distribution | Design Framework |
| [A Robust Mean-Field Actor-Critic Reinforcement Learning Against Adversarial Perturbations on Agent States](https://doi.org/10.1109/TNNLS.2023.3278715) | Zhou, Z. and Liu, G. and Zhou, M. | Adversarial Attack | Simulated Agents |
| [A Robust Method to Measure the Global Feature Importance of Complex Prediction Models](https://doi.org/10.1109/ACCESS.2021.3049412) | Zhang, X. and Wu, L. and Li, Z. and Liu, H. | Noise and Outliers | Applied Algorithm |
| [A Robust Offline Reinforcement Learning Algorithm Based on Behavior Regularization Methods](https://doi.org/10.1109/IAICT55358.2022.9887435) | Zhang, Y. and Gao, T. and Mi, Q. | System Misspecification | Simulated Agents |
| [A Robust Policy Bootstrapping Algorithm for Multi-Objective Reinforcement Learning in Non-Stationary Environments](https://doi.org/10.1177/1059712319869313) | Abdelfattah, S. and Kasmarik, K. and Hu, J. | Non-Stationary Distribution | Simulated Agents |
| [A Robust Supervised Subspace Learning Approach for Output-Relevant Prediction and Detection Against Outliers](https://doi.org/10.1016/j.jprocont.2021.09.007) | Li, W. and Wang, Y. | Noise and Outliers | Applied Algorithm |
| [A Robust Unsupervised Feature Learning Framework Using Spatial Boosting Networks](https://doi.org/10.1109/ICMLA.2013.168) | Le, N.D.-.H. and Tran, M.-.T. | Noise and Outliers | Applied Algorithm |
| [A Safe and Self-Recoverable Reinforcement Learning Framework for Autonomous Robots](https://doi.org/10.23919/CCC55666.2022.9901669) | Wang, W.Q. and Zhou, X. and Xu, B.L. and Lu, M.L. and Zhang, Y.X. and Gu, Y.H. | Unsafe Exploration | Simulated Agents |
| [A Safety Case Pattern for Systems With Machine Learning Components](https://doi.org/10.1007/978-3-030-55583-2_28) | Wozniak, E. and Cârlan, C. and Acar-Celik, E. and Putzer, H.J. | Lack of Control Enforcement, System Misspecification | Analysis Framework |
| [A Safety-Critical Decision-Making and Control Framework Combining Machine-Learning-Based and Rule-Based Algorithms](https://doi.org/10.4271/10-07-03-0018) | Aksjonov, A. and Kyrki, V. | Lack of Control Enforcement | Design Framework |
| [A Study on Real-Time Artificial Intelligence](https://www.sciencedirect.com/science/article/pii/S1474667017413097) | Tay, E.B. and Gan, O.P. and Ho, W.K. | System Misspecification | Philosophical |
| [A Validity Perspective on Evaluating the Justified Use of Data-Driven Decision-Making Algorithms](https://doi.org/10.1109/SaTML54575.2023.00050) | Coston, A. and Kawakami, A. and Zhu, H.Y. and Holstein, K. and Heidari, H. | Lack of Control Enforcement, System Misspecification | Theoretical Framework |
| [Adaptive Robust Learning Framework for Twin Support Vector Machine Classification](https://doi.org/10.1016/j.knosys.2020.106536) | Ma, J. and Yang, L. and Sun, Q. | Noise and Outliers | Applied Algorithm |
| [Addressing Uncertainty in the Safety Assurance of Machine-Learning](https://doi.org/10.3389/fcomp.2023.1132580) | Burton, S. and Herd, B. | System Misspecification, Non-Stationary Distribution | Analysis Framework |
| [Adversarial Robustness of Deep Reinforcement Learning Based Dynamic Recommender Systems](https://doi.org/10.3389/fdata.2022.822783) | Wang, S. and Cao, Y. and Chen, X. and Yao, L. and Wang, X. and Sheng, Q.Z. | Adversarial Attack | Applied Algorithm |
| [Adversarial Robustness of Phishing Email Detection Models](https://doi.org/10.1145/3579987.3586567) | Gholampour, P.M. and Verma, R.M. | Lack of Control Enforcement, Adversarial Attack | Applied Algorithm |
| [Algorithmic Robustness for Semi-Supervised (Ε, Y, Τ)-Good Metric Learning](https://doi.org/10.1007/978-3-319-26532-2_28) | Nicolae, M.-.I. and Sebban, M. and Habrard, A. and Gaussier, E. and Amini, M.-.R. | Noise and Outliers | Theoretical Algorithm |
| [Aligning Individual and Collective Welfare in Complex Socio-Technical Systems by Combining Metaheuristics and Reinforcement Learning](https://doi.org/10.1016/j.engappai.2018.12.003) | Bazzan, A.L.C. | System Misspecification | Simulated Agents |
| [Alignment for Advanced Machine Learning Systems](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111881274&doi=10.1093%2foso%2f9780190905033.003.0013&partnerID=40&md5=f1ab76cab567dada4f844447213bb6fa) | Taylor, J. and Yudkowsky, E. and LaVictoire, P. and Critch, A. | Lack of Control Enforcement, System Misspecification | Philosophical |
| [An Adversarial Perspective on Accuracy, Robustness, Fairness, and Privacy: Multilateral-Tradeoffs in Trustworthy ML](https://doi.org/10.1109/ACCESS.2022.3218715) | Gittens, A. and Yener, B. and Yung, M. | System Misspecification, Lack of Monitoring, Noise and Outliers | Literature Review |
| [An Adversarial Reinforcement Learning Framework for Robust Machine Learning-Based Malware Detection](https://doi.org/10.1109/ICDMW58026.2022.00079) | Ebrahimi, M.R. and Li, W. and Chai, Y. and Pacheco, J. and Chen, H. | Lack of Control Enforcement, Adversarial Attack | Simulated Agents |
| [An Assurance Case Pattern for the Interpretability of Machine Learning in Safety-Critical Systems](https://doi.org/10.1007/978-3-030-55583-2_30) | Ward, F.R. and Habli, I. | System Misspecification | Analysis Framework |
| [An Improved Robust Fuzzy Algorithm for Unsupervised Learning](https://doi.org/10.1515/jisys-2018-0030) | Dik, A. and Jebari, K. and Ettouhami, A. | Noise and Outliers | Applied Algorithm |
| [Architectural Patterns for Handling Runtime Uncertainty of Data-Driven Models in Safety-Critical Perception](https://doi.org/10.1007/978-3-031-14835-4_19) | Gross, J. and Adler, R. and Kläs, M. and Reich, J. and Jöckel, L. and Gansch, R. | System Misspecification, Non-Stationary Distribution | Design Framework |
| [Architectural Patterns for Integrating AI Technology Into Safety-Critical Systems](https://doi.org/10.1145/3489449.3490014) | Dzambic, M. and Dobaj, J. and Seidl, M. and Macher, G. | System Misspecification | Design Framework |
| [Assessing and Enhancing Adversarial Robustness of Predictive Analytics: An Empirically Tested Design Framework](https://doi.org/10.1080/07421222.2022.2063549) | Li, W. and Chai, Y. | Adversarial Attack | Design Framework |
| [Assurance Argument Patterns and Processes for Machine Learning in Safety-Related Systems](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081604916&partnerID=40&md5=aeaccb7806b23dc5dcf079764a13b75b) | Picardi, C. and Paterson, C. and Hawkins, R. and Calinescu, R. and Habli, I. | System Misspecification | Analysis Framework |
| [Assured Deep Multi-Agent Reinforcement Learning for Safe Robotic Systems](https://doi.org/10.1007/978-3-031-10161-8_8) | Riley, J. and Calinescu, R. and Paterson, C. and Kudenko, D. and Banks, A. | Lack of Control Enforcement | Simulated Agents |
| [Auditing and Testing AI - A Holistic Framework](https://doi.org/10.1007/978-3-031-06018-2_20) | Becker, N. and Waltl, B. | System Misspecification | Analysis Framework |
| [Autoencoder-Based Semantic Novelty Detection: Towards Dependable AI-Based Systems](https://doi.org/10.3390/app11219881) | Rausch, A. and Sedeh, A.M. and Zhang, M. | Noise and Outliers | Applied Algorithm |
| [Automatic Exploration Process Adjustment for Safe Reinforcement Learning With Joint Chance Constraint Satisfaction](https://doi.org/10.1016/j.ifacol.2020.12.2198) | Okawa, Y. and Sasaki, T. and Iwane, H. | Unsafe Exploration | Theoretical Algorithm |
| [Automatically Learning Fallback Strategies With Model-Free Reinforcement Learning in Safety-Critical Driving Scenarios](https://doi.org/10.1145/3529399.3529432) | Lecerf, U.U.L. and Yemdji-Tchassi, C.C.Y. and Aubert, S.S.A. and Michiardi, P.P.M. | Noise and Outliers, Unsafe Exploration | Simulated Agents |
| [Automating Safety Argument Change Impact Analysis for Machine Learning Components](https://doi.org/10.1109/PRDC55274.2022.00019) | Carlan, C. and Gauerhof, L. and Gallina, B. and Burton, S. | Noise and Outliers, System Misspecification | Analysis Framework |
| [Adaptive Critic Designs for Event-Triggered Robust Control of Nonlinear Systems With Unknown Dynamics](https://doi.org/10.1109/TCYB.2018.2823199) | Yang, X. and He, H. | Unsafe Exploration | Theoretical Algorithm |
| [Adaptive Interleaved Reinforcement Learning: Robust Stability of Affine Nonlinear Systems With Unknown Uncertainty](https://doi.org/10.1109/TNNLS.2020.3027653) | Li, J. and Ding, J. and Chai, T. and Lewis, F.L. and Jagannathan, S. | Noise and Outliers, Non-Stationary Distribution | Theoretical Algorithm |
| [Barrier Lyapunov Function-Based Safe Reinforcement Learning for Autonomous Vehicles With Optimized Backstepping](https://doi.org/10.1109/TNNLS.2022.3186528) | Zhang, Y.X. and Liang, X.L. and Li, D.Y. and Ge, S.Z.S. and Gao, B.Z. and Chen, H. and Lee, T.H. | Unsafe Exploration | Theoretical Algorithm |
| [BDI-Dojo: Developing Robust BDI Agents in Evolving Adversarial Environments](https://doi.org/10.1109/ACSOS-C52956.2021.00066) | Pulawski, S. and Dam, H.K. and Ghose, A. | Noise and Outliers, Adversarial Attack | Analysis Framework |
| [Beneficial Artificial Intelligence Coordination by Means of a Value Sensitive Design Approach](https://doi.org/10.3390/bdcc3010005) | Umbrello, S. | Lack of Control Enforcement | Philosophical |
| [Beyond Generalization: A Theory of Robustness in Machine Learning](https://doi.org/10.1007/s11229-023-04334-9) | Freiesleben, T. and Grote, T. | System Misspecification, Noise and Outliers | Theoretical Framework |
| [BinFI: An Efficient Fault Injector for Safety-Critical Machine Learning Systems](https://doi.org/10.1145/3295500.3356177) | Chen, Z.T. and Li, G.P. and Pattabiraman, K. and DeBardeleben, N. | Noise and Outliers | Applied Algorithm |
| [Bridging Model-Based Safety and Model-Free Reinforcement Learning Through System Identification of Low Dimensional Linear Models](https://arxiv.org/abs/2205.05787) | Li, Z.Y. and Zeng, J. and Thirugnanam, A. and Sreenath, K. | System Misspecification | Real-World Testing |
| [Building Multi-Agent Environments With Theoretical Guarantees on the Learning of Ethical Policies](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173584282&partnerID=40&md5=b592333309eaa9bbf060db65eaa1e353) | Rodriguez-Soto, M. and Rodriguez-Aguilar, J.A. and Lopez-Sanchez, M. | Lack of Monitoring | Simulated Agents |
| [Censored Markov Decision Processes: A Framework for Safe Reinforcement Learning in Collaboration With External Systems](https://doi.org/10.1109/CDC42340.2020.9304411) | Kohjima, M. and Takahashi, M. and Toda, H. | Unsafe Exploration | Theoretical Algorithm |
| [Combining Pessimism With Optimism for Robust and Efficient Model-Based Deep Reinforcement Learning](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127231156&partnerID=40&md5=0803f12a48c9ee753414ffb80af7e9d2) | Curi, S. and Bogunovic, I. and Krause, A. | Noise and Outliers | Simulated Agents |
| [Conflict-Aware Safe Reinforcement Learning: A Meta-Cognitive Learning Framework](https://doi.org/10.1109/JAS.2021.1004353) | Mazouchi, M. and Nageshrao, S. and Modares, H. | Noise and Outliers, Non-Stationary Distribution | Theoretical Algorithm |
| [Conservative and Adaptive Penalty for Model-Based Safe Reinforcement Learning](https://arxiv.org/abs/2112.07701) | Ma, Y.J. and Shen, A. and Bastani, O. and Jayaraman, D. | System Misspecification, Unsafe Exploration | Theoretical Algorithm |
| [Constrained Cross-Entropy Method for Safe Reinforcement Learning](https://doi.org/10.1109/TAC.2020.3015931) | Wen, M. and Topcu, U. | Unsafe Exploration | Theoretical Algorithm |
| [Continuous Safety Assessment of Updated Supervised Learning Models in Shadow Mode](https://doi.org/10.1109/ICSA-C57050.2023.00069) | Guissouma, H. and Zink, M. and Sax, E. | System Misspecification | Analysis Framework |
| [Counterfactual Learning in Enhancing Resilience in Autonomous Agent Systems](https://doi.org/10.3389/frai.2023.1212336) | Samarasinghe, D. | System Misspecification | Theoretical Framework |
| [Curricular Robust Reinforcement Learning via GAN-Based Perturbation Through Continuously Scheduled Task Sequence](https://doi.org/10.26599/TST.2021.9010076) | Li, Y. and Tian, Y. and Tong, E. and Niu, W. and Xiang, Y. and Chen, T. and Wu, Y. and Liu, J. | Noise and Outliers, Unsafe Exploration | Simulated Agents |
| [Curriculum Learning-Based Fuzzy Support Vector Machine](https://doi.org/10.1109/TFUZZ.2023.3319170) | Chen, B. and Gao, Y. and Liu, J. and Weng, W. and Huang, J. and Fan, Y. and Lan, W. | Noise and Outliers | Applied Algorithm |
| [Data Banzhaf: A Robust Data Valuation Framework for Machine Learning](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165146592&partnerID=40&md5=a821d12295cd52facd652e9c94031f07) | Wang, J.T. and Jia, R. | Noise and Outliers, System Misspecification | Theoretical Framework |
| [Data Efficient Safe Reinforcement Learning Algorithm](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173586002&partnerID=40&md5=360c0b6fe4e7ad5c29d0651567b520d8) | Padakandla, S. | Unsafe Exploration | Simulated Agents |
| [Decentralized Stochastic Optimization and Machine Learning: A Unified Variance-Reduction Framework for Robust Performance and Fast Convergence](https://doi.org/10.1109/MSP.2020.2974267) | Xin, R. and Kar, S. and Khan, U.A. | Lack of Control Enforcement, Lack of Monitoring | Literature Review |
| [Deep Robust Reinforcement Learning for Practical Algorithmic Trading](https://doi.org/10.1109/ACCESS.2019.2932789) | Li, Y. and Zheng, W. and Zheng, Z. | Noise and Outliers, Non-Stationary Distribution | Applied Algorithm |
| [Deep Unsupervised Convolutional Domain Adaptation](https://doi.org/10.1145/3123266.3123292) | Zhuo, J.B. and Wang, S.H. and Zhang, W.G. and Huang, Q.M. | Noise and Outliers | Applied Algorithm |
| [Developing Safer AI-Concepts From Economics to the Rescue](https://doi.org/10.1007/s00146-023-01778) | Maskara, P.K. | Lack of Control Enforcement, System Misspecification | Theoretical Framework |
| [Distantly-Supervised Named Entity Recognition With Noise-Robust Learning and Language Model Augmented Self-Training](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121748854&partnerID=40&md5=0fb4ea97baff35f25aec98ee2a1ae5de) | Meng, Y. and Zhang, Y. and Huang, J. and Wang, X. and Zhang, Y. and Ji, H. and Han, J. | Noise and Outliers | Applied Algorithm |
| [Dual Learning-Based Safe Semi-Supervised Learning](https://doi.org/10.1109/ACCESS.2017.2784406) | Gan, H.T. and Li, Z.H. and Fan, Y.L. and Luo, Z.Z. | Noise and Outliers | Applied Algorithm |
| [ECCOLA - A Method for Implementing Ethically Aligned AI Systems](https://doi.org/10.1016/j.jss.2021.111067) | Vakkuri, V. and Kemell, K.K. and Jantunen, M. and Halme, E. and Abrahamsson, P. | Lack of Monitoring, System Misspecification | Design Framework |
| [Establishing Verification and Validation Objectives for Safety-Critical Bayesian Networks](https://doi.org/10.1109/ISSREW.2017.60) | Douthwaite, M. and Kelly, T. | System Misspecification | Analysis Framework |
| [Evaluating Correctness of Reinforcement Learning Based on Actor-Critic Algorithm](https://doi.org/10.1109/ICUFN55119.2022.9829571) | Kim, Y. and Hussain, M. and Suh, J.W. and Hong, J.E. | Lack of Control Enforcement | Analysis Framework |
| [Evaluating Model-Free Reinforcement Learning Toward Safety-Critical Tasks](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167964284&partnerID=40&md5=792e1228a61ef0eed74f6cb077dba25d) | Zhang, L. and Zhang, Q. and Shen, L. and Yuan, B. and Wang, X. and Tao, D. | Unsafe Exploration | Simulated Agents |
| [eXplainable and Reliable Against Adversarial Machine Learning in Data Analytics](https://doi.org/10.1109/ACCESS.2022.3197299) | Vaccari, I. and Carlevaro, A. and Narteni, S. and Cambiaso, E. and Mongelli, M. | Adversarial Attack | Applied Algorithm |
| [Explicit Explore, Exploit, or Escape (E4): Near-Optimal Safety-Constrained Reinforcement Learning in Polynomial Time](https://doi.org/10.1007/s10994-022-06201-z) | Bossens, D.M. and Bishop, N. | Unsafe Exploration | Theoretical Algorithm |
| [Exploring Fault Parameter Space Using Reinforcement Learning-Based Fault Injection](https://doi.org/10.1109/DSN-W50199.2020.00028) | Moradi, M. and Oakes, B.J. and Saraoglu, M. and Morozov, A. and Janschek, K. and Denil, J. | System Misspecification | Applied Algorithm |
| [Extreme Learning Machine: A Robust Modeling Technique? Yes!](https://doi.org/10.1007/978-3-642-38679-4_2) | Lendasse, A. and Akusok, A. and Simula, O. and Corona, F. and Van Heeswijk, M. and Eirola, E. and Miche, Y. | Noise and Outliers | Literature Review |
| [FedEqual: Defending Model Poisoning Attacks in Heterogeneous Federated Learning](https://doi.org/10.1109/GLOBECOM46510.2021.9685082) | Chen, L.-.Y. and Chiu, T.-.C. and Pang, A.-.C. and Cheng, L.-.C. | Adversarial Attack | Applied Algorithm |
| [Flash Crashes in Multi-Agent Systems Using Minority Games and Reinforcement Learning to Test AI Safety](https://doi.org/10.1109/WSC40007.2019.9004675) | Canonico, L.B. and McNeese, N. | Undesirable Behaviour, System Misspecification | Theoretical Algorithm |
| [From Explainable AI to Explainable Simulation: Using Machine Learning and XAI to Understand System Robustness](https://doi.org/10.1145/3573900.3591114) | Feldkamp, N. and Strassburger, S. | Noise and Outliers | Analysis Framework |
| [Gray-Box Adversarial Training](https://doi.org/10.1007/978-3-030-01267-0_13) | Vivek, B.S. and Mopuri, K.R. and Babu, R.V. | Noise and Outliers, Adversarial Attack | Applied Algorithm |
| [Hard Choices in Artificial Intelligence](https://doi.org/10.1016/j.artint.2021.103555) | Dobbe, R. and Gilbert, T.K. and Mintz, Y. | Lack of Monitoring, System Misspecification | Philosophical |
| [Have a Break From Making Decisions, Have a MARS: The Multi-Valued Action Reasoning System](https://doi.org/10.1007/978-3-031-21441-7_31) | Badea, C. | Lack of Monitoring | Design Framework |
| [HiSaRL: A Hierarchical Framework for Safe Reinforcement Learning](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125382338&partnerID=40&md5=ac7bb4dc501b17e0342bcc2d1c031aca) | Xiong, Z. and Agarwal, I. and Jagannathan, S. | Adversarial Attack, Noise and Outliers, Unsafe Exploration | Theoretical Algorithm |
| [Human-Guided Reinforcement Learning With Sim-to-Real Transfer for Autonomous Navigation](https://doi.org/10.1109/TPAMI.2023.3314762) | Wu, J. and Zhou, Y. and Yang, H. and Huang, Z. and Lv, C. | System Misspecification, Noise and Outliers | Simulated Agents |
| [Impact-Learning: A Robust Machine Learning Algorithm](https://doi.org/10.1145/3411174.3411185) | Kowsher, M. and Tahabilder, A. and Murad, S.A. | Noise and Outliers | Design Framework |
| [Imperative Action Masking for Safe Exploration in Reinforcement Learning](https://doi.org/10.1007/978-3-031-40878-6_8) | Dey, S. and Bhat, S. and Dasgupta, P. and Dey, S. | Unsafe Exploration | Simulated Agents |
| [Improving Safety in Reinforcement Learning Using Model-Based Architectures and Human Intervention](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094005967&partnerID=40&md5=77fcdf372b36d2d963ff0e048ff1e361) | Prakash, B. and Khatwani, M. and Waytowich, N. and Mohsenin, T. | Unsafe Exploration | Simulated Agents |
| [Improving the Robustness of Instance-Based Reinforcement Learning Robots by Metalearning](https://doi.org/10.20965/jaciii.2011.p1065) | Yasuda, T. and Araki, K. and Ohkura, K. | Noise and Outliers | Simulated Agents |
| [Induction of Fault Trees Through Bayesian Networks](https://doi.org/10.3850/978-981-11-2724-3_0596-cd) | Linard, A. and Bueno, M.L.P. and Bucur, D. and Stoelinga, M. | System Misspecification | Analysis Framework |
| [Inferring Human Values for Safe AGI Design](https://doi.org/10.1007/978-3-319-21365-1_16) | Sezener, C.E. | Lack of Control Enforcement | Philosophical |
| [Interpretable Semi-Parametric Regression Models With Defined Error Bounds](https://doi.org/10.1016/j.neucom.2013.11.042) | Otte, C. | Non-Stationary Distribution | Theoretical Algorithm |
| [Joint Adversarial Domain Adaptation With Structural Graph Alignment](https://doi.org/10.1109/TNSE.2023.3302574) | Wang, M. and Chen, J. and Wang, Y. and Wang, S. and li, L. and Su, H. and Gong, Z. and Wu, K. and Chen, Z. | Non-Stationary Distribution | Applied Algorithm |
| [Learning From Noisy Labels via Dynamic Loss Thresholding](https://doi.org/10.1109/TKDE.2023.3313604) | Yang, H. and Jin, Y. and Li, Z. and Wang, D. and Geng, X. and Zhang, M. | Noise and Outliers | Applied Algorithm |
| [Joint Domain Adaptation Based on Adversarial Dynamic Parameter Learning](https://doi.org/10.1109/TETCI.2021.3055873) | Yuan, Y.M. and Li, Y.H. and Zhu, Z.L. and Li, R.X. and Gu, X.W. | Non-Stationary Distribution | Applied Algorithm |
| [Learning to Safely Approve Updates to Machine Learning Algorithms](https://doi.org/10.1145/3450439.3451864) | Feng, J. | System Misspecification, Non-Stationary Distribution | Analysis Framework |
| [Look Before You Leap: Safe Model-Based Reinforcement Learning With Human Intervention](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146865238&partnerID=40&md5=b9693007970623e55035eef698de6e42) | Xu, Y. and Liu, Z. and Duan, G. and Zhu, J. and Bai, X. and Tan, J. | Unsafe Exploration | Simulated Agents |
| [Lyapunov Design for Safe Reinforcement Learning](https://doi.org/10.1162/jmlr.2003.3.4-5.803) | Perkins, T.J. and Barto, A.G. | Unsafe Exploration | Simulated Agents |
| [Meaningful Machine Learning Robustness Evaluation in Real-World Machine Learning Enabled System Contexts](https://doi.org/10.1117/12.2638492) | Hiett, B. and Boyd, P. and Fletcher, C. and Gowland, S. and Sharp, J.H. and Sloggett, D. and Banks, A. | Noise and Outliers | Analysis Framework |
| [Mechanisms and Constraints Underpinning Ethically Aligned Artificial Intelligence Systems: An Exploration of Key Performance Areas](https://doi.org/10.34190/EAIR.21.005) | Treacy, S. | Lack of Monitoring | Theoretical Framework |
| [Metacognition for Artificial Intelligence System Safety-an Approach to Safe and Desired Behavior](https://doi.org/10.1016/j.ssci.2022.105743) | Johnson, B. | System Misspecification | Theoretical Framework |
| [Morality, Machines, and the Interpretation Problem: A Value-Based, Wittgensteinian Approach to Building Moral Agents](https://doi.org/10.1007/978-3-031-21441-7_9) | Badea, C. and Artus, G. | Lack of Control Enforcement | Philosophical |
| [MultiDIAL: Domain Alignment Layers for (Multisource) Unsupervised Domain Adaptation](https://doi.org/10.1109/TPAMI.2020.3001338) | Carlucci, F.M. and Porzi, L. and Caputo, B. and Ricci, E. and Bulo, S.R. | Non-Stationary Distribution | Applied Algorithm |
| [Multilayered Review of Safety Approaches for Machine Learning-Based Systems in the Days of AI](https://doi.org/10.1016/j.jss.2021.110941) | Dey, S. and Lee, S.W. | Lack of Control Enforcement, System Misspecification | Literature Review |
| [N-Version Machine Learning Models for Safety Critical Systems](https://doi.org/10.1109/DSN-W.2019.00017) | Machida, F. | Noise and Outliers | Theoretical Algorithm |
| [Neural Simplex Architecture](https://doi.org/10.1007/978-3-030-55754-6_6) | Phan, D.T. and Grosu, R. and Jansen, N. and Paoletti, N. and Smolka, S.A. and Stoller, S.D. | Unsafe Exploration | Simulated Agents |
| [On Assessing the Safety of Reinforcement Learning Algorithms Using Formal Methods](https://doi.org/10.1109/QRS54544.2021.00037) | Mindom, P.S.N. and Nikanjam, A. and Khomh, F. and Mullins, J. | Adversarial Attack, Unsafe Exploration | Analysis Framework |
| [On Conflicts Between Ethical and Logical Principles in Artificial Intelligence](https://doi.org/10.1007/s00146-019-00927-6) | D'Acquisto, G. | Lack of Monitoring | Philosophical |
| [Out-of-Distribution Detection as Support for Autonomous Driving Safety Lifecycle](https://doi.org/10.1007/978-3-031-29786-1_16) | Henriksson, J. and Ursing, S. and Erdogan, M. and Warg, F. and Thorsén, A. and Jaxing, J. and Örsmark, O. and Toftås, M.Ö. | System Misspecification, Noise and Outliers | Theoretical Framework |
| [Parallel Reward and Punishment Control in Humans and Robots: Safe Reinforcement Learning Using the MaxPain Algorithm](https://doi.org/10.1109/DEVLRN.2017.8329799) | Elfwing, S. and Seymour, B. | Unsafe Exploration | Simulated Agents |
| [Probabilistic Counterexample Guidance for Safer Reinforcement Learning](https://doi.org/10.1007/978-3-031-43835-6_22) | Ji, X. and Filieri, A. | Unsafe Exploration | Simulated Agents |
| [Probabilistic Model Predictive Safety Certification for Learning-Based Control](https://doi.org/10.1109/TAC.2021.3049335) | Wabersich, K.J. and Hewing, L. and Carron, A. and Zeilinger, M.N. | Unsafe Exploration | Simulated Agents |
| [Probabilistic Safeguard for Reinforcement Learning Using Safety Index Guided Gaussian Process Models](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164508271&partnerID=40&md5=52100e0b70a3b6e78be9f039cde62fc6) | Zhao, W. and He, T. and Liu, C. | Unsafe Exploration | Theoretical Algorithm |
| [Process Knowledge-Infused AI: Toward User-Level Explainability, Interpretability, and Safety](https://doi.org/10.1109/MIC.2022.3182349) | Sheth, A. and Gaur, M. and Roy, K. and Venkataraman, R. and Khandelwal, V. | Lack of Monitoring | Design Framework |
| [Proposed v-Model for Verification, Validation, and Safety Activities for Artificial Intelligence](https://doi.org/10.1109/ICAA58325.2023.00017) | Schumeg, B. and Marotta, F. and Werner, B. | Lack of Control Enforcement | Design Framework |
| [Provably Safe Artificial General Intelligence via Interactive Proofs](https://doi.org/10.3390/philosophies6040083) | Carlson, K. | Lack of Control Enforcement | Philosophical |
| [Reduce the Handicap: Performance Estimation for AI Systems Safety Certification](https://doi.org/10.1109/INDIN51400.2023.10218017) | Pfrommer, J. and Poyer, M. and Kiroriwal, S. | Lack of Control Enforcement, Adversarial Attack | Analysis Framework |
| [Responsibility as Answerability](https://doi.org/10.1080/0020174X.2015.986851) | Smith, A.M. | Lack of Monitoring | Philosophical |
| [Retrain AI Systems Responsibly! Use Sustainable Concept Drift Adaptation Techniques](https://doi.org/10.1109/GREENS59328.2023.00009) | Poenaru-Olaru, L. and Sallou, J. and Cruz, L. and Rellermeyer, J.S. and Deursen, A. | Non-Stationary Distribution | Theoretical Framework |
| [Reward Tampering and Evolutionary Computation: A Study of Concrete AI-safety Problems Using Evolutionary Algorithms](https://doi.org/10.1007/s10710-023-09456-0) | Nilsen, M.K. and Nygaard, T.F. and Ellefsen, K.O. | Undesirable Behaviour | Analysis Framework |
| [Robust and Verifiable Privacy Federated Learning](https://doi.org/10.1109/TAI.2023.3309273) | Lu, Z. and Lu, S. and Tang, X. and Wu, J. | Lack of Monitoring | Applied Algorithm |
| [Robust Data Sampling in Machine Learning: A Game-Theoretic Framework for Training and Validation Data Selection](https://doi.org/10.3390/g14010013) | Mo, Z. and Di, X. and Shi, R. | Non-Stationary Distribution | Applied Algorithm |
| [Robust ML Model Ensembles via Risk-Driven Anti-Clustering of Training Data](https://doi.org/10.1016/j.ins.2023.03.085) | Mauri, L. and Apolloni, B. and Damiani, E. | Adversarial Attack, Noise and Outliers | Applied Algorithm |
| [Robust Transparency Against Model Inversion Attacks](https://doi.org/10.1109/TDSC.2020.3019508) | Alufaisan, Y. and Kantarcioglu, M. and Zhou, Y. | Lack of Monitoring | Applied Algorithm |
| [Safe Policy Search Using Gaussian Process Models](https://arxiv.org/abs/1712.05556) | Polymenakos, K. and Abate, A. and Roberts, S. | Unsafe Exploration | Theoretical Algorithm |
| [Safe Semi-Supervised Classification Algorithm Combined With Active Learning Sampling Strategy](https://doi.org/10.3233/JIFS-169722) | Zhao, J.H. and Liu, N. and Malov, A. | Noise and Outliers | Applied Algorithm |
| [Safe Trajectory Sampling in Model-Based Reinforcement Learning](https://doi.org/10.1109/CASE56687.2023.10260496) | Zwane, S. and Hadjivelichkov, D. and Luo, Y. and Bekiroglu, Y. and Kanoulas, D. and Deisenroth, M.P. | Unsafe Exploration | Real-World Testing |
| [Safe Value Functions](https://doi.org/10.1109/TAC.2022.3200948) | Massiani, P.F. and Heim, S. and Solowjow, F. and Trimpe, S. | Unsafe Exploration | Theoretical Algorithm |
| [SafeOps: A Concept of Continuous Safety](https://doi.org/10.1109/EDCC51268.2020.00020) | Fayollas, C. and Bonnin, H. and Flebus, O. | Lack of Control Enforcement, System Misspecification | Theoretical Framework |
| [SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness](https://doi.org/10.1109/TPAMI.2019.2932415) | Kocak, M.A. and Ramirez, D. and Erkip, E. and Shasha, D.E. | Lack of Control Enforcement | Applied Algorithm |
| [Safety AI: A Novel Approach to Update Safety Models Using Artificial Intelligence](https://doi.org/10.1109/ACCESS.2019.2941566) | Gheraibia, Y. and Kabir, S. and Aslansefat, K. and Sorokos, I. and Papadopoulos, Y. | System Misspecification | Theoretical Algorithm |
| [Safety and Performance, Why Not Both? Bi-Objective Optimized Model Compression Toward AI Software Deployment](https://doi.org/10.1145/3551349.3556906) | Zhu, J. and Wang, L.Y. and Han, X. | Adversarial Attack | Applied Algorithm |
| [Safety and Robustness of Deep Neural Networks Object Recognition Under Generic Attacks](https://doi.org/10.1007/978-3-030-36808-1_30) | Sallami, M.M. and Ibn Khedher, M. and Trabelsi, A. and Kerboua-Benlarbi, S. and Bettebghor, D. | Lack of Control Enforcement, Adversarial Attack | Applied Algorithm |
| [Safety in the Face of Unknown Unknowns: Algorithm Fusion in Data-Driven Engineering Systems](https://doi.org/10.1109/ICASSP.2019.8683392) | Kshetry, N. and Varshney, L.R. | Noise and Outliers | Theoretical Framework |
| [Safety Integrity Levels for Artificial Intelligence](https://doi.org/10.1007/978-3-031-40953-0_34) | Diemert, S. and Millet, L. and Groves, J. and Joyce, J. | Lack of Control Enforcement | Theoretical Framework |
| [Safety vs. Efficiency: AI-Based Risk Mitigation in Collaborative Robotics](https://doi.org/10.1109/iccar49639.2020.9108037) | Terra, A. and Riaz, H. and Raizer, K. and Hata, A. and Inam, R. | Unsafe Exploration | Real-World Testing |
| [SAFEXPLAIN: Safe and Explainable Critical Embedded Systems Based on AI](https://doi.org/10.23919/DATE56975.2023.10137128) | Abella, J. and Perez, J. and Englund, C. and Zonooz, B. and Giordana, G. and Donzella, C. and Cazorla, F.J. and Mezzetti, E. and Serra, I. and Brando, A. and Agirre, I. and Eizaguirre, F. and Bui, T.H. and Arani, E. and Sarfraz, F. and Balasubramaniam, A. and Badar, A. and Bloise, I. and Feruglio, L. and Cinelli, I. and Brighenti, D. and Cunial, D. | Lack of Control Enforcement, Lack of Monitoring | Theoretical Framework |
| [SAMBA: Safe Model-Based & Active Reinforcement Learning](https://doi.org/10.1007/s10994-021-06103-6) | Cowen-Rivers, A.I. and Palenicek, D. and Moens, V. and Abdullah, M.A. and Sootla, A. and Wang, J. and Bou-Ammar, H. | Unsafe Exploration | Applied Algorithm |
| [Sampling-Based Inverse Reinforcement Learning Algorithms With Safety Constraints](https://doi.org/10.1109/IROS51168.2021.9636672) | Fischer, J. and Eyberg, C. and Werling, M. and Lauer, M. | Unsafe Exploration | Applied Algorithm |
| [Security Analysis of Safe and Seldonian Reinforcement Learning Algorithms](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108455515&partnerID=40&md5=dfff89b401122e01f74e471cb746b1f9) | Pinar Ozisik, A. and Thomas, P.S. | Adversarial Attack, System Misspecification | Applied Algorithm |
| [Self-Improving Safety Performance of Reinforcement Learning Based Driving With Black-Box Verification Algorithms](https://doi.org/10.1109/ICRA48891.2023.10160883) | Dagdanov, R. and Durmus, H. and Ure, N.K. | Non-Stationary Distribution | Applied Algorithm |
| [Self-Preserving Genetic Algorithms for Safe Learning in Discrete Action Spaces](https://doi.org/10.1145/3576841.3585936) | Robinette, P.K. and Hamilton, N.P. and Johnson, T.T. | Unsafe Exploration | Simulated Agents |
| [Shared Interest: Measuring Human-Ai Alignment to Identify Recurring Patterns in Model Behavior](https://doi.org/10.1145/3491102.3501965) | Boggust, A. and Hoover, B. and Satyanarayan, A. and Strobelt, H. | Lack of Monitoring | Analysis Framework |
| [Stability-Certified Reinforcement Learning: A Control-Theoretic Perspective](https://doi.org/10.1109/ACCESS.2020.3045114) | Jin, M. and Lavaei, J. | Noise and Outliers | Applied Algorithm |
| [TENET: A New Hybrid Network Architecture for Adversarial Defense](https://doi.org/10.1007/s10207-023-00675-1) | Tuna, O.F. and Catak, F.O. and Eskil, M.T. | Adversarial Attack | Applied Algorithm |
| [Towards a Robust and Trustworthy Machine Learning System Development: An Engineering Perspective](https://doi.org/10.1016/j.jisa.2022.103121) | Xiong, P. and Buffett, S. and Iqbal, S. and Lamontagne, P. and Mamun, M. and Molyneaux, H. | Lack of Monitoring | Literature Review |
| [Towards Deep Anomaly Detection With Structured Knowledge Representations](https://doi.org/10.1007/978-3-031-40953-0_32) | Kirchheim, K. | Noise and Outliers | Theoretical Algorithm |
| [Towards Deployment of Robust Cooperative AI Agents: An Algorithmic Framework for Learning Adaptive Policies](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093460105&partnerID=40&md5=9647e6eca81c01f303cba645d35a90aa) | Ghosh, A. and Mahdavi, H. and Tschiatschek, S. and Singla, A. | Lack of Control Enforcement | Theoretical Algorithm |
| [Towards Fair and Robust Classification](https://doi.org/10.1109/EuroSP53844.2022.00030) | Sun, H. and Wu, K. and Wang, T. and Wang, W.H. | Lack of Monitoring | Applied Algorithm |
| [Towards Functional Safety Compliance of Matrix-Matrix Multiplication for Machine Learning-Based Autonomous Systems](https://doi.org/10.1016/j.sysarc.2021.102298) | Fernández, J. and Perez, J. and Agirre, I. and Allende, I. and Abella, J. and Cazorla, F.J. | Lack of Control Enforcement | Real-World Testing |
| [Understanding Adversarial Training: Increasing Local Stability of Supervised Models Through Robust Optimization](https://doi.org/10.1016/j.neucom.2018.04.027) | Shaham, U. and Yamada, Y. and Negahban, S. | Adversarial Attack | Applied Algorithm |
| [User Tampering in Reinforcement Learning Recommender Systems](https://doi.org/10.1145/3600211.3604669) | Kasirzadeh, A. and Evans, C. | Undesirable Behaviour | Theoretical Algorithm |
| [Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089513686&partnerID=40&md5=c3851d370937fd50d52dcc25e03e3bd5) | Hendrycks, D. and Mazeika, M. and Kadavath, S. and Song, D. | Noise and Outliers | Analysis Framework |
| [Verifiably Safe Exploration for End-to-End Reinforcement Learning](https://doi.org/10.1145/3447928.3456653) | Hunt, N. and Fulton, N. and Magliacane, S. and Hoang, T.N. and Das, S. and Solar-Lezama, A. | Lack of Control Enforcement, Unsafe Exploration | Simulated Agents |
| [Verifiably Safe Off-Model Reinforcement Learning](https://doi.org/10.1007/978-3-030-17462-0_28) | Fulton, N. and Platzer, A. | Lack of Control Enforcement, Unsafe Exploration | Theoretical Algorithm |
| [When Neurons Fail](https://doi.org/10.1109/IPDPS.2017.66) | El Mhamdi, E.M. and Guerraoui, R. | System Misspecification | Theoretical Framework |
| [A Data-Driven Robust Optimization Algorithm for Black-Box Cases: An Application to Hyper-Parameter Optimization of Machine Learning Algorithms](https://doi.org/10.1016/j.cie.2021.107581) | Seifi, F. and Azizi, M.J. and Akhavan Niaki, S.T. | System Misspecification, Noise and Outliers | Applied Algorithm |
| [A Robust Framework for Fixing the Vulnerability of Compressed Distributed Learning](https://doi.org/10.1109/CSCWD57460.2023.10152781) | Chen, Y. and Wang, B. and Zhang, Y. and Kuang, J. | System Misspecification, Noise and Outliers, Adversarial Attack | Design Framework |
| [A Robust Self-Organizing Approach to Effectively Clustering Incomplete Data](https://doi.org/10.1109/KSE.2015.11) | Chau, V.T.N. | System Misspecification | Applied Algorithm |
| [A Semi-Supervised Deep Learning Model With Consistency Regularization of Augmented Samples for Imbalanced Fault Detection](https://doi.org/10.1109/ICRMS55680.2022.9944609) | Chen, H. and Han, J. and Lv, X. and Wu, Z. and Guo, H. and Zhan, Z. | Noise and Outliers, System Misspecification | Applied Algorithm |
| [Adversarial Feature Distribution Alignment for Semi-Supervised Learning](https://doi.org/10.1016/j.cviu.2020.103109) | Mayer, C. and Paul, M. and Timofte, R. | Non-Stationary Distribution | Applied Algorithm |
| [Adversarial Robustness via Fisher-Rao Regularization](https://doi.org/10.1109/TPAMI.2022.3174724) | Picot, M. and Messina, F. and Boudiaf, M. and Labeau, F. and Ayed, I.B. and Piantanida, P. | Adversarial Attack | Applied Algorithm |
| [Adversarial Training on Joint Energy Based Model for Robust Classification and Out-of-Distribution Detection](https://doi.org/10.23919/iccas50221.2020.9268406) | Lee, K. and Yang, H. and Oh, S.Y. | Noise and Outliers | Applied Algorithm |
| [Adversarial Training With Channel Attention Regularization](https://doi.org/10.1109/ICIP46576.2022.9897754) | Cho, S. and Byun, J. and Kwon, M.-.J. and Kim, Y. and Kim, C. | Adversarial Attack, Noise and Outliers | Applied Algorithm |
| [AlwaysSafe: Reinforcement Learning Without Safety Constraint Violations During Training](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103674313&partnerID=40&md5=359b14bfd08a91ba735c0f4b935475f4) | Simão, T.D. and Jansen, N. and Spaan, M.T.J. | Unsafe Exploration | Simulated Agents |
| [An Actor-Critic Framework for Online Control With Environment Stability Guarantee](https://doi.org/10.1109/ACCESS.2023.3306070) | Osinenko, P. and Yaremenko, G. and Malaniya, G. and Bolychev, A. | Unsafe Exploration | Simulated Agents |
| [An Adversarial Training Method for Improving Model Robustness in Unsupervised Domain Adaptation](https://doi.org/10.1007/978-3-030-82153-1_1) | Nie, Z. and Lin, Y. and Yan, M. and Cao, Y. and Ning, S. | Adversarial Attack | Applied Algorithm |
| [Anomaly Detection for Insider Threats Using Unsupervised Ensembles](https://doi.org/10.1109/TNSM.2021.3071928) | Le, D.C. and Zincir-Heywood, N. | Noise and Outliers | Applied Algorithm |
| [Bayesian Robust Multi-Extreme Learning Machine](https://doi.org/10.1016/j.knosys.2020.106468) | Li, Y. and Wang, Y. and Chen, Z. and Zou, R. | Noise and Outliers | Applied Algorithm |
| [Bidirectional Adaptation for Robust Semi-Supervised Learning With Inconsistent Data Distributions](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174426186&partnerID=40&md5=83bb3938b25b176e9237be2cf5d0084a) | Jia, L.-.H. and Guo, L.-.Z. and Zhou, Z. and Shao, J.-.J. and Xiang, Y.-.K. and Li, Y.-.F. | Noise and Outliers | Theoretical Framework |
| [Bullseye Polytope: A Scalable Clean-Label Poisoning Attack With Improved Transferability](https://doi.org/10.1109/EuroSP51992.2021.00021) | Aghakhani, H. and Meng, D. and Wang, Y.-.X. and Kruegel, C. and Vigna, G. | Adversarial Attack | Applied Algorithm |
| [Byzantine-Robust Online and Offline Distributed Reinforcement Learning](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164381268&partnerID=40&md5=f0f263cd3d72d542c157a405a6e611d3) | Chen, Y. and Zhang, X. and Zhang, K. and Wang, M. and Zhu, X. | Adversarial Attack | Simulated Agents |
| [Conformal Predictive Safety Filter for RL Controllers in Dynamic Environments](https://doi.org/10.1109/LRA.2023.3322644) | Strawn, K.J. and Ayanian, N. and Lindemann, L. | Unsafe Exploration | Simulated Agents |
| [CROP: Towards Distributional-Shift Robust Reinforcement Learning Using Compact Reshaped Observation Processing](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170359176&partnerID=40&md5=324523ba832f4699dfb4c359f306a724) | Altmann, P. and Ritz, F. and Feuchtinger, L. and Nüßlein, J. and Linnhoff-Popien, C. and Phan, T. | Non-Stationary Distribution | Simulated Agents |
| [Data-Driven Robust Multi-Agent Reinforcement Learning](https://doi.org/10.1109/MLSP55214.2022.9943500) | Wang, Y. and Wang, Y. and Zhou, Y. and Velasquez, A. and Zou, S. | Non-Stationary Distribution | Simulated Agents |
| [Decoupled Adversarial Contrastive Learning for Self-Supervised Adversarial Robustness](https://doi.org/10.1007/978-3-031-20056-4_42) | Zhang, C. and Zhang, K. and Zhang, C. and Niu, A. and Feng, J. and Yoo, C.D. and Kweon, I.S. | Noise and Outliers | Applied Algorithm |
| [DiscrimLoss: A Universal Loss for Hard Samples and Incorrect Samples Discrimination](https://doi.org/10.1109/TMM.2023.3290477) | Wu, T. and Ding, X. and Zhang, H. and Gao, J. and Tang, M. and Du, L. and Qin, B. and Liu, T. | Noise and Outliers | Applied Algorithm |
| [Distributed Multi-Agent Deep Reinforcement Learning for Robust Coordination Against Noise](https://doi.org/10.1109/IJCNN55064.2022.9892253) | Motokawa, Y. and Sugawara, T. | Noise and Outliers | Simulated Agents |
| [Distributionally Robust Semi-Supervised Learning for People-Centric Sensing](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090808244&partnerID=40&md5=68a7c553d7f705a73dd28cb3ab222b33) | Chen, K. and Yao, L. and Zhang, D. and Chang, X. and Long, G. and Wang, S. | Non-Stationary Distribution | Applied Algorithm |
| [Does Distributionally Robust Supervised Learning Give Robust Classifiers?](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057279674&partnerID=40&md5=290395557bb261b75f67b472cfddea3d) | Hu, W. and Niu, G. and Sato, I. and Sugiyama, M. | Non-Stationary Distribution | Theoretical Algorithm |
| [Efficient Adversarial Training Without Attacking: Worst-Case-Aware Robust Reinforcement Learning](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150009921&partnerID=40&md5=f780a63268f2da04aa1b18bf9fc5d208) | Liang, Y. and Sun, Y. and Zheng, R. and Huang, F. | Adversarial Attack | Simulated Agents |
| [Embracing Risk in Reinforcement Learning: The Connection Between Risk-Sensitive Exponential and Distributionally Robust Criteria](https://doi.org/10.23919/ACC53348.2022.9867841) | Noorani, E. and Baras, J.S. | Unsafe Exploration | Theoretical Algorithm |
| [FairMixRep: Self-Supervised Robust Representation Learning for Heterogeneous Data With Fairness Constraints](https://doi.org/10.1109/ICDMW51313.2020.00069) | Chakraborty, S. and Verma, E. and Sahoo, S. and Datta, J. | Lack of Monitoring, Noise and Outliers | Applied Algorithm |
| [Formal Methods Assisted Training of Safe Reinforcement Learning Agents](https://doi.org/10.1007/978-3-030-20652-9_22) | Murugesan, A. and Moghadamfalahi, M. and Chattopadhyay, A. | Lack of Control Enforcement | Simulated Agents |
| [Group Distributionally Robust Reinforcement Learning With Hierarchical Latent Variables](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165176997&partnerID=40&md5=78b2ceb3e076387d4b6e631d9b475265) | Xu, M. and Huang, P. and Niu, Y. and Kumar, V. and Qiu, J. and Fang, C. and Lee, K.-.H. and Qi, X. and Lam, H. and Li, B. and Zhao, D. | Noise and Outliers | Simulated Agents |
| [Improving Fairness Generalization Through a Sample-Robust Optimization Method](https://doi.org/10.1007/s10994-022-06191-y) | Ferry, J. and Aïvodji, U. and Gambs, S. and Huguet, M.-.J. and Siala, M. | Lack of Monitoring, Noise and Outliers, Non-Stationary Distribution | Applied Algorithm |
| [Improving Machine Learning Robustness via Adversarial Training](https://doi.org/10.1109/ICCCN58024.2023.10230138) | Dang, L. and Hapuarachchi, T. and Xiong, K. and Lin, J. | Noise and Outliers | Applied Algorithm |
| [Improving Robustness of Deep Reinforcement Learning Agents: Environment Attack Based on the Critic Network](https://doi.org/10.1109/IJCNN55064.2022.9892901) | Schott, L. and Hajri, H. and Lamprier, S. | Noise and Outliers | Simulated Agents |
| [Improving Robustness via Risk Averse Distributional Reinforcement Learning](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160153359&partnerID=40&md5=819ea330ab2feb455bbe62ea271c4e74) | Singh, R. and Zhang, Q. and Chen, Y. | Noise and Outliers, Non-Stationary Distribution | Simulated Agents |
| [Integrating Safety Constraints Into Adversarial Training for Robust Deep Reinforcement Learning](https://doi.org/10.1016/j.ins.2022.11.051) | Meng, J.L. and Zhu, F. and Ge, Y.Y. and Zhao, P.Y. | Adversarial Attack, Noise and Outliers | Simulated Agents |
| [JSMix: A Holistic Algorithm for Learning With Label Noise](https://doi.org/10.1007/s00521-022-07770-9) | Wen, Z. and Xu, H. and Ying, S. | Noise and Outliers | Applied Algorithm |
| [Learning a Domain-Invariant Embedding for Unsupervised Domain Adaptation Using Class-Conditioned Distribution Alignment](https://doi.org/10.1109/allerton.2019.8919960) | Gabourie, A.J. and Rostami, M. and Pope, P.E. and Kolouri, S. and Kim, K. | Non-Stationary Distribution | Applied Algorithm |
| [Learning a Low-Dimensional Representation of a Safe Region for Safe Reinforcement Learning on Dynamical Systems](https://doi.org/10.1109/TNNLS.2021.3106818) | Zhou, Z.H. and Oguz, O.S. and Leibold, M. and Buss, M. | Unsafe Exploration | Simulated Agents |
| [Learning Barrier Certificates: Towards Safe Reinforcement Learning With Zero Training-Time Violations](https://arxiv.org/abs/2108.01846) | Luo, Y.P. and Ma, T.Y. | Unsafe Exploration | Simulated Agents |
| [Model-Free Safe Reinforcement Learning Through Neural Barrier Certificate](https://doi.org/10.1109/LRA.2023.3238656) | Yang, Y.J. and Jiang, Y.X. and Liu, Y.C. and Chen, J.Y. and Li, S.E. | Unsafe Exploration | Simulated Agents |
| [Multi-Modal Mutual Information (MuMMI) Training for Robust Self-Supervised Deep Reinforcement Learning](https://doi.org/10.1109/ICRA48506.2021.9561187) | Chen, K. and Lee, Y. and Soh, H. | Noise and Outliers | Simulated Agents |
| [Multiexpert Adversarial Regularization for Robust and Data-Efficient Deep Supervised Learning](https://doi.org/10.1109/ACCESS.2022.3196780) | Gholami, B. and Liu, Q. and El-Khamy, M. and Lee, J. | Noise and Outliers | Applied Algorithm |
| [Not All Parameters Should Be Treated Equally: Deep Safe Semi-Supervised Learning Under Class Distribution Mismatch](https://doi.org/10.1609/aaai.v36i6.20644) | He, R.D. and Han, Z.Y. and Yang, Y. and Yin, Y.L. | Adversarial Attack | Applied Algorithm |
| [RDA: Reciprocal Distribution Alignment for Robust Semi-Supervised Learning](https://doi.org/10.1007/978-3-031-20056-4_31) | Duan, Y. and Qi, L. and Wang, L. and Zhou, L.P. and Shi, Y.H. | Noise and Outliers, Non-Stationary Distribution | Theoretical Algorithm |
| [Risk-Averse Distributional Reinforcement Learning: A CVaR Optimization Approach](https://doi.org/10.5220/0008175604120423) | Stanko, S. and Macek, K. | Noise and Outliers | Simulated Agents |
| [Robust Federated Learning With Noisy Labeled Data Through Loss Function Correction](https://doi.org/10.1109/TNSE.2022.3227287) | Chen, L. and Ang, F. and Chen, Y. and Wang, W. | Noise and Outliers | Applied Algorithm |
| [Robust Fusion of Unreliable Data Sources Using Error-Correcting Output Codes](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117998598&doi=10.1049%2fPBCE117E_ch12&partnerID=40&md5=c7334508b4b3a8a50b7d4bf13b57694b) | Vempaty, A. and Kailkhura, B. and Varshney, P.K. | System Misspecification | Applied Algorithm |
| [Robust Multi-Agent Reinforcement Learning via Bayesian Distributional Value Estimation](https://doi.org/10.1016/j.patcog.2023.109917) | Du, X. and Chen, H. and Wang, C. and Xing, Y. and Yang, J. and Yu, P.S. and Chang, Y. and He, L. | Noise and Outliers | Simulated Agents |
| [Robust Reinforcement Learning Using Offline Data](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143683086&partnerID=40&md5=12385a40ddb4c145a75af7e617b994dc) | Panaganti, K. and Xu, Z. and Kalathil, D. and Ghavamzadeh, M. | Noise and Outliers | Simulated Agents |
| [Robust Reinforcement Learning via Adversarial Training With Langevin Dynamics](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104088882&partnerID=40&md5=c4d1f8bdcf47f337e3c90c03ad3e14c6) | Kamalaruban, P. and Huang, Y.-.T. and Hsieh, Y.-.P. and Rolland, P. and Shi, C. and Cevher, V. | Noise and Outliers, Non-Stationary Distribution | Simulated Agents |
| [Robust Reinforcement Learning: A Constrained Game-Theoretic Approach](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129673722&partnerID=40&md5=924f503e07b0d4790f2ca10f68012f4f) | Yu, J. and Gehring, C. and Schäfer, F. and Anandkumar, A. | Noise and Outliers | Simulated Agents |
| [Robust Semi-Supervised Representation Learning for Graph-Structured Data](https://doi.org/10.1007/978-3-030-16142-2_11) | Guo, L.-.Z. and Han, T. and Li, Y.-.F. | Noise and Outliers | Applied Algorithm |
| [Robust Supervised Classification With Mixture Models: Learning From Data With Uncertain Labels](https://doi.org/10.1016/j.patcog.2009.03.027) | Bouveyron, C. and Girard, S. | Noise and Outliers | Applied Algorithm |
| [Robust Support Vector Machine Using Least Median Loss Penalty](https://doi.org/10.3182/20110828-6-IT-1002.03467) | Ma, Y. and Li, L. and Huang, X. and Wang, S. | Noise and Outliers | Applied Algorithm |
| [Robust Unsupervised Domain Adaptation From a Corrupted Source](https://doi.org/10.1109/ICDM54844.2022.00171) | Yu, S. and Zhu, Z. and Liu, B. and Jain, A.K. and Zhou, J. | Non-Stationary Distribution | Applied Algorithm |
| [Robust Weighted Gaussian Processes](https://doi.org/10.1007/s00180-020-01011-0) | Ramirez-Padron, R. and Mederos, B. and Gonzalez, A.J. | Noise and Outliers | Theoretical Algorithm |
| [Robustifying Reinforcement Learning Agents via Action Space Adversarial Training](https://doi.org/10.23919/ACC45564.2020.9147846) | Tan, K.L. and Esfandiari, Y. and Lee, X.Y. and Sarkar, S. | Noise and Outliers | Simulated Agents |
| [Safe Artificial General Intelligence via Distributed Ledger Technology](https://doi.org/10.3390/bdcc3030040) | Carlson, K.W. | Lack of Control Enforcement | Theoretical Framework |
| [Safe Batch Constrained Deep Reinforcement Learning With Generative Adversarial Network](https://doi.org/10.1016/j.ins.2023.03.108) | Dong, W.B. and Liu, S.F. and Sun, S.L. | Noise and Outliers | Simulated Agents |
| [Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data](https://proceedings.mlr.press/v119/guo20i.html) | Guo, L.Z. and Zhang, Z.Y. and Jiang, Y. and Li, Y.F. and Zhou, Z.H. | Noise and Outliers | Applied Algorithm |
| [Safe Incomplete Label Distribution Learning](https://doi.org/10.1016/j.patcog.2021.108518) | Zhang, J. and Tao, H. and Luo, T.J. and Hou, C.P. | Noise and Outliers | Applied Algorithm |
| [Safe Multi-View Co-Training for Semi-Supervised Regression](https://doi.org/10.1109/DSAA54385.2022.10032437) | Liu, L.Y. and Huang, P. and Min, F. | Noise and Outliers | Applied Algorithm |
| [Safe Offline Reinforcement Learning Through Hierarchical Policies](https://doi.org/10.1007/978-3-031-05936-0_30) | Liu, S.F. and Sun, S.L. | Unsafe Exploration | Simulated Agents |
| [Safe Reinforcement Learning Using Data-Driven Predictive Control](https://doi.org/10.1109/ICCSPA55860.2022.10018994) | Selim, M. and Alanwar, A. and El-Kharashi, M.W. and Abbas, H.M. and Johansson, K.H. | Unsafe Exploration | Simulated Agents |
| [Safe Reinforcement Learning via a Model-Free Safety Certifier](https://doi.org/10.1109/TNNLS.2023.3264815) | Modares, A. and Sadati, N. and Esmaeili, B. and Yaghmaie, F.A. and Modares, H. | Unsafe Exploration | Simulated Agents |
| [Safe Semi-Supervised Learning Using a Bayesian Neural Network](https://doi.org/10.1016/j.ins.2022.08.094) | Bae, J. and Lee, M.J. and Kim, S.B. | Noise and Outliers, Non-Stationary Distribution | Applied Algorithm |
| [Safe-Ds: A Domain Specific Language to Make Data Science Safe](https://doi.org/10.1109/ICSE-NIER58687.2023.00019) | Reimann, L. and Kniesel-Wünsche, G. | System Misspecification | Design Framework |
| [Safe-Student for Safe Deep Semi-Supervised Learning With Unseen-Class Unlabeled Data](https://doi.org/10.1109/CVPR52688.2022.01418) | He, R.D. and Han, Z.Y. and Lu, X.K. and Yin, Y.L. | Noise and Outliers | Applied Algorithm |
| [Safety Assurance With Ensemble-Based Uncertainty Estimation and Overlapping Alternative Predictions in Reinforcement Learning](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159259475&partnerID=40&md5=2c11248b544075497117f0dc81f91729) | Eilers, D. and Burton, S. and Roza, F.S. and Roscher, K. | Noise and Outliers, Unsafe Exploration | Simulated Agents |
| [Safety Monitoring of Neural Networks Using Unsupervised Feature Learning and Novelty Estimation](https://doi.org/10.1109/TIV.2022.3152084) | Ranjbar, A. and Hornauer, S. and Fredriksson, J. and Yu, S.X. and Chan, C.Y. | Noise and Outliers | Simulated Agents |
| [Safety-Constrained Reinforcement Learning With a Distributional Safety Critic](https://doi.org/10.1007/s10994-022-06187-8) | Yang, Q.S. and Simao, T.D. and Tindemans, S.H. and Spaan, M.T.J. | Unsafe Exploration | Simulated Agents |
| [Supervised Contrastive Learning for Robust Text Adversarial Training](https://doi.org/10.1007/s00521-022-07871-5) | Li, W. and Zhao, B. and An, Y. and Shangguan, C. and Ji, M. and Yuan, A. | Adversarial Attack, Noise and Outliers | Applied Algorithm |
| [Toward Improved Reliability of Deep Learning Based Systems Through Online Relabeling of Potential Adversarial Attacks](https://doi.org/10.1109/TR.2023.3298685) | Al-Maliki, S. and Bouanani, F.E. and Ahmad, K. and Abdallah, M. and Hoang, D.T. and Niyato, D. and Al-Fuqaha, A. | Adversarial Attack | Applied Algorithm |
| [Toward Learning Robust and Invariant Representations With Alignment Regularization and Data Augmentation](https://doi.org/10.1145/3534678.3539438) | Wang, H. and Huang, Z. and Wu, X. and Xing, E. | Noise and Outliers | Applied Algorithm |
| [Towards Robust and Safe Reinforcement Learning With Benign Off-Policy Data](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174408788&partnerID=40&md5=6463a867d47c3f19ff2fbca9a27362f9) | Liu, Z. and Guo, Z. and Cen, Z. and Zhang, H. and Yao, Y. and Hu, H. and Zhao, D. | Noise and Outliers, Unsafe Exploration | Simulated Agents |
| [Towards Robust Off-Policy Evaluation via Human Inputs](https://doi.org/10.1145/3514094.3534198) | Singh, H. and Joshi, S. and Doshi-Velez, F. and Lakkaraju, H. | Non-Stationary Distribution | Design Framework |
| [Towards Robust Production Machine Learning Systems: Managing Dataset Shift](https://doi.org/10.1145/3324884.3415281) | Abdelkader, H. | Non-Stationary Distribution | Analysis Framework |
| [Train Small, Deploy Big: Do Relative World Views Permit Swarm-Safety During Policy Transplantation for Multi-Agent Reinforcement Learning Problems?](https://doi.org/10.1007/978-3-030-64984-5_21) | Fraser, B. and Laurito, G. | Non-Stationary Distribution | Simulated Agents |
| [Training and Transferring Safe Policies in Reinforcement Learning](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168254751&partnerID=40&md5=69ee98d03db848dbd74b02d5ad5311a6) | Yang, Q. and Simão, T.D. and Jansen, N. and Tindemans, S.H. and Spaan, M.T.J. | Non-Stationary Distribution | Simulated Agents |
| [A Family of Robust Stochastic Operators for Reinforcement Learning](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090177881&partnerID=40&md5=a53a7206f40717efe60cd4f5353df697) | Lu, Y. and Squillante, M.S. and Wu, C.W. | Noise and Outliers | Simulated Agents |
| [A Robust Approach for Continuous Interactive Actor-Critic Algorithms](https://doi.org/10.1109/ACCESS.2021.3099071) | Millan-Arias, C.C. and Fernandes, B.J.T. and Cruz, F. and Dazeley, R. and Fernandes, S. | Non-Stationary Distribution | Simulated Agents |
| [AI Apology: Interactive Multi-Objective Reinforcement Learning for Human-Aligned AI](https://doi.org/10.1007/s00521-023-08586-x) | Harland, H. and Dazeley, R. and Nakisa, B. and Cruz, F. and Vamplew, P. | Lack of Control Enforcement | Simulated Agents |
| [Assured Multi-Agent Reinforcement Learning With Robust Agent-Interaction Adaptability](https://doi.org/10.1007/978-981-19-3444-5_8) | Riley, J. and Calinescu, R. and Paterson, C. and Kudenko, D. and Banks, A. | Lack of Control Enforcement | Simulated Agents |
| [Batch-Like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104622356&partnerID=40&md5=04edd84470671c7e18a70da9f07e9d78) | Schmid, T. | Noise and Outliers | Theoretical Framework |
| [Detecting Functional Safety Violations in Online AI Accelerators](https://doi.org/10.1109/IOLTS56730.2022.9897702) | Kundu, S. and Basu, K. | System Misspecification | Analysis Framework |
| [Detecting Operational Adversarial Examples for Reliable Deep Learning](https://doi.org/10.1109/DSN-S52858.2021.00013) | Zhao, X.Y. and Huang, W. and Schewe, S. and Dong, Y. and Huang, X.W. | Adversarial Attack | Applied Algorithm |
| [Discovering Blind Spots in Reinforcement Learning](https://doi.org/10.5555/3237383.3237849) | Ramakrishnan, R. and Kamar, E. and Dey, D. and Shah, J. and Horvitz, E. | Noise and Outliers | Simulated Agents |
| [How to Train Your Agent: Active Learning From Human Preferences and Justifications in Safety-Critical Environments](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134306740&partnerID=40&md5=76a61115b19501a21f7673129a6cff87) | Kazantzidis, I. and Norman, T.J. and Du, Y. and Freeman, C.T. | Noise and Outliers, Non-Stationary Distribution | Simulated Agents |
| [Online Robust Reinforcement Learning With Model Uncertainty](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130775690&partnerID=40&md5=ac00e2eb7a717c085e21d9699fe68881) | Wang, Y. and Zou, S. | Noise and Outliers | Simulated Agents |
| [Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172894839&partnerID=40&md5=cdac5fce5f7bd75455c6185abb95092b) | Ding, D. and Wei, X. and Yang, Z. and Wang, Z. and Jovanović, M.R. | Unsafe Exploration | Simulated Agents |
| [Reflection Machines: Increasing Meaningful Human Control Over Decision Support Systems](https://doi.org/10.1007/s10676-022-09645-y) | Cornelissen, N.A.J. and Eerdt, R.J.M. and Schraffenberger, H.K. and Haselager, W.F.G. | Lack of Monitoring | Theoretical Framework |
| [Robust Automatic Target Recognition Using Learning Classifier Systems](https://doi.org/10.1016/j.inffus.2006.03.001) | Ravichandran, B. and Gandhe, A. and Smith, R. and Mehra, R. | Noise and Outliers | Applied Algorithm |
| [Robust Label Prediction via Label Propagation and Geodesic K-Nearest Neighbor in Online Semi-Supervised Learning](https://doi.org/10.1587/transinf.2018EDP7424) | Wada, Y. and Su, S. and Kumagai, W. and Kanamori, T. | Noise and Outliers | Applied Algorithm |
| [Safe Exploration for Interactive Machine Learning](https://doi.org/10.5555/3454287.3454547) | Turchetta, M. and Berkenkamp, F. and Krause, A. | Unsafe Exploration | Simulated Agents |
| [Safe Reinforcement Learning of Dynamic High-Dimensional Robotic Tasks: Navigation, Manipulation, Interaction](https://doi.org/10.1109/ICRA48891.2023.10161548) | Liu, P.Z. and Zhang, K. and Tateo, D. and Jauhri, S. and Hu, Z.Y. and Peters, J. and Chalvatzaki, G. | Unsafe Exploration | Real-World Testing |
| [A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks](https://doi.org/10.48550/arXiv.1610.02136) | Hendrycks, D. and Gimpel, K. | System Misspecification, Noise and Outliers | Dataset |
| [A Hazard Analysis Framework for Code Synthesis Large Language Models](https://doi.org/10.48550/arXiv.2207.14157) | Khlaaf, H. and Mishkin, P. and Achiam, J. and Krueger, G. and Brundage, M. | Lack of Control Enforcement, Adversarial Attack | Analysis Framework |
| [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) | Elhage, N. and Nanda, N. and Olsson, C. and Henighan, T. and Joseph, N. and Mann, B. and Askell, A. and Bai, Y. and Chen, A. and Conerly, T. and DasSarma, N. and Drain, D. and Ganguli, D. and Hatfield-Dodds, Z. and Hernandez, D. and Jones, A. and Kernion, J. and Lovitt, L. and Ndousse, K. and Amodei, D. and Brown, T. and Clark, J. and Kaplan, J. and McCandlish, S. and Olah, C. | Lack of Monitoring | Mechanistic Interpretability |
| [Active Reward Learning](https://doi.org/10.15607/RSS.2014.X.031) | Daniel, C. and Viering, M. and Metz, J. and Kroemer, O. and Peters, J. | Undesirable Behaviour, System Misspecification | Real-World Testing |
| [Adversarial Examples Are Not Bugs, They Are Features](https://papers.nips.cc/paper_files/paper/2019/hash/e2c420d928d4bf8ce0ff2ec19b371514-Abstract.html) | Ilyas, A. and Santurkar, S. and Tsipras, D. and Engstrom, L. and Tran, B. and Madry, A. | Adversarial Attack | Applied Algorithm |
| [Adversarial Filters of Dataset Biases](https://proceedings.mlr.press/v119/bras20a.html) | Bras, R.L. and Swayamdipta, S. and Bhagavatula, C. and Zellers, R. and Peters, M. and Sabharwal, A. and Choi, Y. | Lack of Monitoring, Noise and Outliers, Non-Stationary Distribution | Applied Algorithm |
| [Adversarial Robustness as a Prior for Learned Representations](https://doi.org/10.48550/arXiv.1906.00945) | Engstrom, L. and Ilyas, A. and Santurkar, S. and Tsipras, D. and Tran, B. and Madry, A. | Lack of Monitoring, Adversarial Attack | Mechanistic Interpretability |
| [Agreeing to Disagree: Active Learning With Noisy Labels Without Crowdsourcing](https://doi.org/10.1007/s13042-017-0645-0) | Bouguelia, M. and Nowaczyk, S. and Santosh, K. .C. and Verikas, A. | Noise and Outliers | Applied Algorithm |
| [Aligning AI With Shared Human Values](https://doi.org/10.48550/arXiv.2008.02275) | Hendrycks, D. and Burns, C. and Basart, S. and Critch, A. and Li, J. and Song, D. and Steinhardt, J. | Lack of Monitoring | Dataset |
| [Aligning to Social Norms and Values in Interactive Narratives](https://doi.org/10.18653/v1/2022.naacl-main.439) | Ammanabrolu, P. and Jiang, L. and Sap, M. and Hajishirzi, H. and Choi, Y. | Lack of Monitoring | Applied Algorithm |
| [Analyzing Information Leakage of Updates to Natural Language Models](https://doi.org/10.1145/3372297.3417880) | Zanella-Béguelin, S. and Wutschitz, L. and Tople, S. and Rühle, V. and Paverd, A. and Ohrimenko, O. and Köpf, B. and Brockschmidt, M. | Lack of Monitoring, System Misspecification | Analysis Framework |
| [Apprenticeship Learning via Inverse Reinforcement Learning](https://doi.org/10.1145/1015330.1015430) | Abbeel, P. and Ng, A.Y. | Lack of Control Enforcement | Simulated Agents |
| [Approximate Data Deletion From Machine Learning Models](https://proceedings.mlr.press/v130/izzo21a.html) | Izzo, Z. and Smart, M.A. and Chaudhuri, K. and Zou, J. | Lack of Monitoring, System Misspecification | Applied Algorithm |
| [Artificial Intelligence Safety Engineering: Why Machine Ethics Is a Wrong Approach](https://doi.org/10.1007/978-3-642-31674-6_29) | Yampolskiy, R.V. | Lack of Control Enforcement, Lack of Monitoring, System Misspecification | Philosophical |
| [Assuring the Behavior of Adaptive Agents](https://doi.org/10.1007/1-84628-271-3_8) | Spears, D.F. | Undesirable Behaviour, Unsafe Exploration | Analysis Framework |
| [ATOM: Robustifying Out-of-Distribution Detection Using Outlier Mining](https://doi.org/10.48550/arXiv.2006.15207) | Chen, J. and Li, Y. and Wu, X. and Liang, Y. and Jha, S. | Noise and Outliers, Adversarial Attack | Applied Algorithm |
| [Avoiding Unintended AI Behaviors](https://doi.org/10.1007/978-3-642-35506-6_12) | Hibbard, B. | Undesirable Behaviour, Lack of Control Enforcement, System Misspecification | Theoretical Framework |
| [Avoiding Wireheading With Value Reinforcement Learning](https://doi.org/10.48550/arXiv.1605.03143) | Everitt, T. and Hutter, M. | Undesirable Behaviour | Design Framework |
| [Backdoor Defense With Machine Unlearning](https://doi.org/10.1109/INFOCOM48880.2022.9796974) | Liu, Y. and Fan, M. and Chen, C. and Liu, X. and Ma, Z. and Wang, L. and Ma, J. | Lack of Monitoring, Adversarial Attack | Applied Algorithm |
| [Batch Learning From Logged Bandit Feedback Through Counterfactual Risk Minimization](http://jmlr.org/papers/v16/swaminathan15a.html) | Swaminathan, A. and Joachims, T. | Non-Stationary Distribution | Applied Algorithm |
| [Bayesian Optimization With Safety Constraints: Safe and Automatic Parameter Tuning in Robotics](https://doi.org/10.1007/s10994-021-06019-1) | Berkenkamp, F. and Krause, A. and Schoellig, A.P. | Unsafe Exploration | Theoretical Algorithm |
| [Benchmarking Neural Network Robustness to Common Corruptions and Perturbations](https://doi.org/10.48550/arXiv.1903.12261) | Hendrycks, D. and Dietterich, T. | Noise and Outliers | Dataset |
| [Benchmarking Safe Exploration in Deep Reinforcement Learning](https://cdn.openai.com/safexp-short.pdf) | Ray, A. and Achiam, J. and Amodei, D. | Unsafe Exploration | Analysis Framework |
| [Corrigibility](https://cdn.aaai.org/ocs/ws/ws0067/10124-45900-1-PB.pdf) | Soares, N. and Fallenstein, B. and Yudkowsky, E. and Armstrong, S. | Undesirable Behaviour | Theoretical Framework |
| [Curve Detectors](https://doi.org/10.23915/distill.00024.003) | Cammarata, N. and Goh, G. and Carter, S. and Schubert, L. and Petrov, M. and Olah, C. | Lack of Monitoring | Mechanistic Interpretability |
| [Deep Neural Networks Are Easily Fooled: High Confidence Predictions for Unrecognizable Images](https://doi.org/10.1109/CVPR.2015.7298640) | Nguyen, A. and Yosinski, J. and Clune, J. | Adversarial Attack | Analysis Framework |
| [Deep Reinforcement Learning From Human Preferences](http://arxiv.org/abs/1706.03741) | Christiano, P. and Leike, J. and Brown, T.B. and Martic, M. and Legg, S. and Amodei, D. | Lack of Control Enforcement, Unsafe Exploration | Simulated Agents |
| [DeltaGrad: Rapid Retraining of Machine Learning Models](https://proceedings.mlr.press/v119/wu20b.html) | Wu, Y. and Dobriban, E. and Davidson, S. | System Misspecification | Applied Algorithm |
| [Delusion, Survival, and Intelligent Agents](https://doi.org/10.1007/978-3-642-22887-2_2) | Ring, M. and Orseau, L. | Undesirable Behaviour | Theoretical Framework |
| [Delving Into Transferable Adversarial Examples and Black-Box Attacks](https://doi.org/10.48550/arXiv.1611.02770) | Liu, Y. and Chen, X. and Liu, C. and Song, D. | Adversarial Attack | Analysis Framework |
| [Discovering Robust Convolutional Architecture at Targeted Capacity: A Multi-Shot Approach](https://doi.org/10.48550/arXiv.2012.11835) | Ning, X. and Zhao, J. and Li, W. and Zhao, T. and Zheng, Y. and Yang, H. and Wang, Y. | System Misspecification, Adversarial Attack | Applied Algorithm |
| [Disentangling by Factorising](https://doi.org/10.48550/arXiv.1802.05983) | Kim, H. and Mnih, A. | Lack of Monitoring | Applied Algorithm |
| [Domain-Adversarial Training of Neural Networks](http://link.springer.com/10.1007/978-3-319-58347-1_10) | Ganin, Y. and Ustinova, E. and Ajakan, H. and Germain, P. and Larochelle, H. and Laviolette, F. and Marchand, M. and Lempitsky, V. | System Misspecification, Non-Stationary Distribution | Applied Algorithm |
| [Establishing Safety Criteria for Artificial Neural Networks](https://doi.org/10.1007/978-3-540-45224-9_24) | Kurd, Z. and Kelly, T. | System Misspecification | Design Framework |
| [Evaluating Models' Local Decision Boundaries via Contrast Sets](https://doi.org/10.18653/v1/2020.findings-emnlp.117) | Gardner, M. and Artzi, Y. and Basmov, V. and Berant, J. and Bogin, B. and Chen, S. and Dasigi, P. and Dua, D. and Elazar, Y. and Gottumukkala, A. and Gupta, N. and Hajishirzi, H. and Ilharco, G. and Khashabi, D. and Lin, K. and Liu, J. and Liu, N.F. and Mulcaire, P. and Ning, Q. and Singh, S. and Smith, N.A. and Subramanian, S. and Tsarfaty, R. and Wallace, E. and Zhang, A. and Zhou, B. | System Misspecification | Mechanistic Interpretability |
| [Explaining and Harnessing Adversarial Examples](https://doi.org/10.48550/arXiv.1412.6572) | Goodfellow, I.J. and Shlens, J. and Szegedy, C. | Adversarial Attack | Mechanistic Interpretability |
| [Exploration-Exploitation in Multi-Agent Learning: Catastrophe Theory Meets Game Theory](https://doi.org/10.1016/j.artint.2021.103653) | Leonardos, S. and Piliouras, G. | Non-Stationary Distribution | Theoretical Algorithm |
| [Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible From Input-Output Observations](https://doi.org/10.1007/978-3-030-58526-6_23) | Golatkar, A. and Achille, A. and Soatto, S. | Lack of Monitoring | Applied Algorithm |
| [Formalizing Data Deletion in the Context of the Right to Be Forgotten](https://doi.org/10.1007/978-3-030-45724-2_13) | Garg, S. and Goldwasser, S. and Vasudevan, P.N. | Lack of Monitoring | Theoretical Framework |
| [Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL](https://doi.org/10.5555/3545946.3598670) | Christoffersen, P.J.K. and Haupt, A.A. and Hadfield-Menell, D. | Undesirable Behaviour, Lack of Control Enforcement | Simulated Agents |
| [Hardening of Artificial Neural Networks for Use in Safety-Critical Applications -- A Mapping Study](https://doi.org/10.48550/arXiv.1909.03036) | Adler, R. and Akram, M.N. and Bauer, P. and Feth, P. and Gerber, P. and Jedlitschka, A. and Jöckel, L. and Kläs, M. and Schneider, D. | System Misspecification | Literature Review |
| [Have You Forgotten? A Method to Assess if Machine Learning Models Have Forgotten Data](https://doi.org/10.1007/978-3-030-59710-8_10) | Liu, X. and Tsaftaris, S.A. | Lack of Monitoring, System Misspecification | Analysis Framework |
| [High-Low Frequency Detectors](https://doi.org/10.23915/distill.00024.005) | Schubert, L. and Voss, C. and Cammarata, N. and Goh, G. and Olah, C. | Lack of Monitoring | Mechanistic Interpretability |
| [How to Talk So AI Will Learn: Instructions, Descriptions, and Autonomy](http://arxiv.org/abs/2206.07870) | Sumers, T.R. and Hawkins, R.D. and Ho, M.K. and Griffiths, T.L. and Hadfield-Menell, D. | Lack of Control Enforcement | Simulated Agents |
| [Human-in-the-Loop Interpretability Prior](http://arxiv.org/abs/1805.11571) | Lage, I. and Ross, A.S. and Kim, B. and Gershman, S.J. and Doshi-Velez, F. | Lack of Monitoring | Applied Algorithm |
| [Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration](http://arxiv.org/abs/1610.09064) | Lakkaraju, H. and Kamar, E. and Caruana, R. and Horvitz, E. | Lack of Monitoring, Noise and Outliers | Analysis Framework |
| [In-Context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) | Olsson, C. and Elhage, N. and Nanda, N. and Joseph, N. and DasSarma, N. and Henighan, T. and Mann, B. and Askell, A. and Bai, Y. and Chen, A. and Conerly, T. and Drain, D. and Ganguli, D. and Hatfield-Dodds, Z. and Hernandez, D. and Johnston, S. and Jones, A. and Kernion, J. and Lovitt, L. and Ndousse, K. and Amodei, D. and Brown, T. and Clark, J. and Kaplan, J. and McCandlish, S. and Olah, C. | Lack of Monitoring | Mechanistic Interpretability |
| [Inference-Time Intervention: Eliciting Truthful Answers From a Language Model](https://doi.org/10.48550/arXiv.2306.03341) | Li, K. and Patel, O. and Viégas, F. and Pfister, H. and Wattenberg, M. | Undesirable Behaviour | Design Framework |
| [Interpretability Beyond Feature Attribution: Quantitative Testing With Concept Activation Vectors (TCAV)](http://arxiv.org/abs/1711.11279) | Kim, B. and Wattenberg, M. and Gilmer, J. and Cai, C. and Wexler, J. and Viegas, F. and Sayres, R. | Lack of Monitoring | Mechanistic Interpretability |
| [Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 Small](https://doi.org/10.48550/arXiv.2211.00593) | Wang, K. and Variengien, A. and Conmy, A. and Shlegeris, B. and Steinhardt, J. | Lack of Monitoring | Mechanistic Interpretability |
| [Inverse Game Theory: Learning Utilities in Succinct Games](https://doi.org/10.1007/978-3-662-48995-6_30) | Kuleshov, V. and Schrijvers, O. | Lack of Control Enforcement, Noise and Outliers | Analysis Framework |
| [Is Attention Interpretable?](http://arxiv.org/abs/1906.03731) | Serrano, S. and Smith, N.A. | Lack of Monitoring | Analysis Framework |
| [Language Models Can Explain Neurons in Language Models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html) | Bills, S. and Cammarata, N. and Mossing, D. and Tillman, H. and Gao, L. and Goh, G. and Sutskever, I. and Leike, J. and Wu, J. and Saunders, W. | Lack of Monitoring | Applied Algorithm |
| [Leakproofing the Singularity: Artificial Intelligence Confinement Problem](http://cecs.louisville.edu/ry/LeakproofingtheSingularity.pdf) | Yampolskiy, R.V. | Lack of Control Enforcement | Philosophical |
| [Learning Effective and Interpretable Semantic Models Using Non-Negative Sparse Embedding](https://aclanthology.org/C12-1118) | Murphy, B. and Talukdar, P. and Mitchell, T. | Lack of Monitoring | Applied Algorithm |
| [Learning Fair Representations](https://proceedings.mlr.press/v28/zemel13.html) | Zemel, R. and Wu, Y. and Swersky, K. and Pitassi, T. and Dwork, C. | Lack of Monitoring | Applied Algorithm |
| [Learning Rewards From Linguistic Feedback](https://doi.org/10.48550/arXiv.2009.14715) | Sumers, T.R. and Ho, M.K. and Hawkins, R.D. and Narasimhan, K. and Griffiths, T.L. | Undesirable Behaviour | Simulated Agents |
| [Learning Robust Representations by Projecting Superficial Statistics Out](http://arxiv.org/abs/1903.06256) | Wang, H. and He, Z. and Lipton, Z.C. and Xing, E.P. | Non-Stationary Distribution | Applied Algorithm |
| [Learning the Difference That Makes a Difference With Counterfactually-Augmented Data](https://doi.org/10.48550/arXiv.1909.12434) | Kaushik, D. and Hovy, E. and Lipton, Z.C. | Non-Stationary Distribution | Dataset |
| [Learning the Preferences of Ignorant, Inconsistent Agents](https://doi.org/10.5555/3015812.3015860) | Evans, O. and Stuhlmüller, A. and Goodman, N.D. | Lack of Control Enforcement | Theoretical Algorithm |
| [Machine Unlearning for Random Forests](https://proceedings.mlr.press/v139/brophy21a.html) | Brophy, J. and Lowd, D. | Lack of Monitoring | Applied Algorithm |
| [MetaReg: Towards Domain Generalization Using Meta-Regularization](https://papers.nips.cc/paper_files/paper/2018/hash/647bba344396e7c8170902bcf2e15551-Abstract.html) | Balaji, Y. and Sankaranarayanan, S. and Chellappa, R. | Non-Stationary Distribution | Applied Algorithm |
| [MORAL: Aligning AI With Human Norms Through Multi-Objective Reinforced Active Learning](http://arxiv.org/abs/2201.00012) | Peschl, M. and Zgonnikov, A. and Oliehoek, F.A. and Siebert, L.C. | Lack of Control Enforcement | Simulated Agents |
| [Multimodal Neurons in Artificial Neural Networks](https://doi.org/10.23915/distill.00030) | Goh, G. and †, N.C. and †, C.V. and Carter, S. and Petrov, M. and Schubert, L. and Radford, A. and Olah, C. | Lack of Monitoring | Mechanistic Interpretability |
| [On Interpretability and Feature Representations: An Analysis of the Sentiment Neuron](https://doi.org/10.1007/978-3-030-15712-8_55) | Donnelly, J. and Roegiest, A. | Lack of Monitoring | Mechanistic Interpretability |
| [On the Importance of Single Directions for Generalization](http://arxiv.org/abs/1803.06959) | Morcos, A.S. and Barrett, D.G.T. and Rabinowitz, N.C. and Botvinick, M. | Lack of Monitoring | Mechanistic Interpretability |
| [On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning](https://www.usenix.org/conference/usenixsecurity22/presentation/thudi) | Thudi, A. and Jia, H. and Shumailov, I. and Papernot, N. | Lack of Monitoring | Analysis Framework |
| [Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf) | Ng, A.Y. and Harada, D. and Russell, S.J. | Undesirable Behaviour | Simulated Agents |
| [Privacy-Preserving Prediction](https://proceedings.mlr.press/v75/dwork18a.html) | Dwork, C. and Feldman, V. | Lack of Monitoring | Theoretical Algorithm |
| [Process for Adapting Language Models to Society (PALMS) With Values-Targeted Datasets](https://doi.org/10.5555/3540261.3540709) | Solaiman, I. and Dennison, C. | Lack of Control Enforcement | Design Framework |
| [Progress Measures for Grokking via Mechanistic Interpretability](https://doi.org/10.48550/arXiv.2301.05217) | Nanda, N. and Chan, L. and Lieberum, T. and Smith, J. and Steinhardt, J. | Undesirable Behaviour, Lack of Monitoring | Mechanistic Interpretability |
| [Provable Defenses Against Adversarial Examples via the Convex Outer Adversarial Polytope](https://proceedings.mlr.press/v80/wong18a.html) | Wong, E. and Kolter, Z. | Noise and Outliers, Adversarial Attack | Applied Algorithm |
| [PUMA: Performance Unchanged Model Augmentation for Training Data Removal](https://doi.org/10.48550/arXiv.2203.00846) | Wu, G. and Hashemi, M. and Srinivasa, C. | Lack of Monitoring | Applied Algorithm |
| [Responses to Catastrophic AGI Risk: A Survey](https://doi.org/10.1088/0031-8949/90/1/018001) | Sotala, K. and Yampolskiy, R.V. | Lack of Control Enforcement | Literature Review |
| [Safety Engineering for Artificial General Intelligence](https://doi.org/10.1007/s11245-012-9128-9) | Yampolskiy, R. and Fox, J. | Lack of Control Enforcement | Philosophical |
| [Sanity Checks for Saliency Maps](https://doi.org/10.48550/arXiv.1810.03292) | Adebayo, J. and Gilmer, J. and Muelly, M. and Goodfellow, I. and Hardt, M. and Kim, B. | Lack of Monitoring | Analysis Framework |
| [Scalable Agent Alignment via Reward Modeling: A Research Direction](http://arxiv.org/abs/1811.07871) | Leike, J. and Krueger, D. and Everitt, T. and Martic, M. and Maini, V. and Legg, S. | Lack of Control Enforcement | Literature Review |
| [Self-Critiquing Models for Assisting Human Evaluators](https://doi.org/10.48550/arXiv.2206.05802) | Saunders, W. and Yeh, C. and Wu, J. and Bills, S. and Ouyang, L. and Ward, J. and Leike, J. | Undesirable Behaviour | Analysis Framework |
| [Self-Modification of Policy and Utility Function in Rational Agents](https://doi.org/10.48550/arXiv.1605.03142) | Everitt, T. and Filan, D. and Daswani, M. and Hutter, M. | Lack of Control Enforcement | Theoretical Framework |
| [Social Norms-Grounded Machine Ethics in Complex Narrative Situation](https://aclanthology.org/2022.coling-1.114) | Shen, T. and Geng, X. and Jiang, D. | Lack of Monitoring | Applied Algorithm |
| [Softmax Linear Units](https://transformer-circuits.pub/2022/solu/index.html) | Elhage, N. and Hume, T. and Olsson, C. and Nanda, N. and Henighan, T. and Johnston, S. and ElShowk, S. and Joseph, N. and DasSarma, N. and Mann, B. and Hernandez, D. and Askell, A. and Ndousse, K. and Jones, A. and Drain, D. and Chen, A. and Bai, Y. and Ganguli, D. and Lovitt, L. and Hatfield-Dodds, Z. and Kernion, J. and Conerly, T. and Kravec, S. and Fort, S. and Kadavath, S. and Jacobson, J. and Tran-Johnson, E. and Kaplan, J. and Clark, J. and Brown, T. and McCandlish, S. and Amodei, D. and Olah, C. | Lack of Monitoring | Mechanistic Interpretability |
| [Sparse Attention With Linear Units](https://doi.org/10.48550/arXiv.2104.07012) | Zhang, B. and Titov, I. and Sennrich, R. | Lack of Monitoring | Mechanistic Interpretability |
| [SPINE: SParse Interpretable Neural Embeddings](https://doi.org/10.48550/arXiv.1711.08792) | Subramanian, A. and Pruthi, D. and Jhamtani, H. and Berg-Kirkpatrick, T. and Hovy, E. | Lack of Monitoring | Applied Algorithm |
| [SPLASH: Learnable Activation Functions for Improving Accuracy and Adversarial Robustness](https://doi.org/10.48550/arXiv.2006.08947) | Tavakoli, M. and Agostinelli, F. and Baldi, P. | Adversarial Attack, Noise and Outliers | Applied Algorithm |
| [State Abstraction for Programmable Reinforcement Learning Agents](https://people.eecs.berkeley.edu/~russell/classes/cs294/f05/papers/andre+russell-2002.pdf) | Andre, D. and Russell, S.J. | Unsafe Exploration | Simulated Agents |
| [Supervising Strong Learners by Amplifying Weak Experts](https://doi.org/10.48550/arXiv.1810.08575) | Christiano, P. and Shlegeris, B. and Amodei, D. | Undesirable Behaviour | Simulated Agents |
| [SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability](https://proceedings.neurips.cc/paper_files/paper/2017/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html) | Raghu, M. and Gilmer, J. and Yosinski, J. and Sohl-Dickstein, J. | Lack of Monitoring | Mechanistic Interpretability |
| [Task-Guided Inverse Reinforcement Learning Under Partial Information](http://arxiv.org/abs/2105.14073) | Djeumou, F. and Cubuktepe, M. and Lennon, C. and Topcu, U. | Non-Stationary Distribution | Simulated Agents |
| [Taxonomy of Pathways to Dangerous AI](https://doi.org/10.48550/arXiv.1511.03246) | Yampolskiy, R.V. | Lack of Control Enforcement | Literature Review |
| [Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning](http://arxiv.org/abs/2207.05480) | Dunion, M. and McInroe, T. and Luck, K.S. and Hanna, J.P. and Albrecht, S.V. | Non-Stationary Distribution | Applied Algorithm |
| [Testing Robustness Against Unforeseen Adversaries](https://doi.org/10.48550/arXiv.1908.08016) | Kaufmann, M. and Kang, D. and Sun, Y. and Basart, S. and Yin, X. and Mazeika, M. and Arora, A. and Dziedzic, A. and Boenisch, F. and Brown, T. and Steinhardt, J. and Hendrycks, D. | System Misspecification, Adversarial Attack | Analysis Framework |
| [The Building Blocks of Interpretability](https://doi.org/10.23915/distill.00010) | Olah, C. and Satyanarayan, A. and Johnson, I. and Carter, S. and Schubert, L. and Ye, K. and Mordvintsev, A. | Lack of Monitoring | Mechanistic Interpretability |
| [The First Law of Robotics](https://doi.org/10.1007/978-3-642-04879-1_7) | Weld, D. and Etzioni, O. | Lack of Control Enforcement | Philosophical |
| [The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks](https://doi.org/10.5555/3361338.3361358) | Carlini, N. and Liu, C. and Erlingsson, Ú. and Kos, J. and Song, D. | Lack of Monitoring | Analysis Framework |
| [Towards Deep Learning Models Resistant to Adversarial Attacks](https://doi.org/10.48550/arXiv.1706.06083) | Madry, A. and Makelov, A. and Schmidt, L. and Tsipras, D. and Vladu, A. | Adversarial Attack | Analysis Framework |
| [Towards Deployment of Robust Cooperative AI Agents: An Algorithmic Framework for Learning Adaptive Policies](https://doi.org/10.5555/3398761.3398817) | Ghosh, A. and Tschiatschek, S. and Mahdavi, H. and Singla, A. | Non-Stationary Distribution | Simulated Agents |
| [Towards Evaluating the Robustness of Neural Networks](https://doi.org/10.48550/arXiv.1608.04644) | Carlini, N. and Wagner, D. | Adversarial Attack | Analysis Framework |
| [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features) | Bricken, T. and Templeton, A. and Batson, J. and Chen, B. and Jermyn, A. and Conerly, T. and Turner, N. and Anil, C. and Denison, C. and Askell, A. and Lasenby, R. and Wu, Y. and Kravec, S. and Schiefer, N. and Maxwell, T. and Joseph, N. and Hatfield-Dodds, Z. and Tamkin, A. and Nguyen, K. and McLean, B. and Burke, J.E. and Hume, T. and Carter, S. and Henighan, T. and Olah, C. | Lack of Monitoring | Mechanistic Interpretability |
| [Towards Neural Networks That Provably Know When They Don't Know](https://doi.org/10.48550/arXiv.1909.12180) | Meinke, A. and Hein, M. | Noise and Outliers | Applied Algorithm |
| [Training Language Models to Follow Instructions With Human Feedback](https://doi.org/10.48550/arXiv.2203.02155) | Ouyang, L. and Wu, J. and Jiang, X. and Almeida, D. and Wainwright, C.L. and Mishkin, P. and Zhang, C. and Agarwal, S. and Slama, K. and Ray, A. and Schulman, J. and Hilton, J. and Kelton, F. and Miller, L. and Simens, M. and Askell, A. and Welinder, P. and Christiano, P. and Leike, J. and Lowe, R. | Lack of Control Enforcement | Applied Algorithm |
| [Transparent Value Alignment](https://doi.org/10.1145/3568294.3580147) | Sanneman, L. and Shah, J. | Lack of Control Enforcement | Theoretical Framework |
| [Unbiased Look at Dataset Bias](https://doi.org/10.1109/CVPR.2011.5995347) | Torralba, A. and Efros, A.A. | System Misspecification | Literature Review |
| [Understanding the Role of Individual Units in a Deep Neural Network](https://doi.org/10.1073/pnas.1907375117) | Bau, D. and Zhu, J. and Strobelt, H. and Lapedriza, A. and Zhou, B. and Torralba, A. | Lack of Monitoring | Mechanistic Interpretability |
| [Unethical Research: How to Create a Malevolent Artificial Intelligence](https://doi.org/10.48550/arXiv.1605.02817) | Pistono, F. and Yampolskiy, R.V. | Lack of Control Enforcement | Philosophical |
| [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://doi.org/10.48550/arXiv.2307.15043) | Zou, A. and Wang, Z. and Carlini, N. and Nasr, M. and Kolter, J.Z. and Fredrikson, M. | Lack of Control Enforcement, Adversarial Attack | Applied Algorithm |
| [Unsolved Problems in ML Safety](http://arxiv.org/abs/2109.13916) | Hendrycks, D. and Carlini, N. and Schulman, J. and Steinhardt, J. | Lack of Control Enforcement, Noise and Outliers | Literature Review |
| [Utility Function Security in Artificially Intelligent Agents](https://doi.org/10.1080/0952813X.2014.895114) | Yampolskiy, R.V. | Undesirable Behaviour | Philosophical |
| [Visualizing and Understanding Recurrent Networks](http://arxiv.org/abs/1506.02078) | Karpathy, A. and Johnson, J. and Fei-Fei, L. | Lack of Monitoring | Mechanistic Interpretability |
| [AI Safety via Debate](https://doi.org/10.48550/arXiv.1805.00899) | Irving, G. and Christiano, P. and Amodei, D. | Lack of Control Enforcement | Applied Algorithm |
| [Algorithms for Inverse Reinforcement Learning](https://ai.stanford.edu/~ang/papers/icml00-irl.pdf) | Ng, A.Y. and Russell, S.J. | Undesirable Behaviour, Lack of Control Enforcement | Simulated Agents |
| [Cooperative Inverse Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html) | Hadfield-Menell, D. and Russell, S.J. and Abbeel, P. and Dragan, A. | Undesirable Behaviour, Lack of Control Enforcement | Simulated Agents |
| [Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models](https://doi.org/10.48550/arXiv.2311.04378) | Zhang, H. and Edelman, B.L. and Francati, D. and Venturi, D. and Ateniese, G. and Barak, B. | Lack of Control Enforcement | Applied Algorithm |
| [What Would Jiminy Cricket Do? Towards Agents That Behave Morally](https://openreview.net/forum?id=G1muTb5zuO7) | Hendrycks, D. and Mazeika, M. and Zou, A. and Patel, S. and Zhu, C. and Navarro, J. and Song, D. and Li, B. and Steinhardt, J. | Lack of Monitoring | Analysis Framework |
| [When Machine Unlearning Jeopardizes Privacy](https://doi.org/10.1145/3460120.3484756) | Chen, M. and Zhang, Z. and Wang, T. and Backes, M. and Humbert, M. and Zhang, Y. | Lack of Monitoring | Analysis Framework |
| [Zero-Shot Machine Unlearning](https://doi.org/10.1109/TIFS.2023.3265506) | Chundawat, V.S. and Tarun, A.K. and Mandal, M. and Kankanhalli, M. | Lack of Monitoring | Applied Algorithm |
| [Toy Models of Superposition](https://doi.org/10.48550/arXiv.2209.10652) | Elhage, N. and Hume, T. and Olsson, C. and Schiefer, N. and Henighan, T. and Kravec, S. and Hatfield-Dodds, Z. and Lasenby, R. and Drain, D. and Chen, C. and Grosse, R. and McCandlish, S. and Kaplan, J. and Amodei, D. and Wattenberg, M. and Olah, C. | Lack of Monitoring | Mechanistic Interpretability |
| [Concrete Problems in AI Safety](https://doi.org/10.48550/arXiv.1606.06565) | Amodei, D. and Olah, C. and Steinhardt, J. and Christiano, P. and Schulman, J. and Mané, D. | System Misspecification | Literature Review |


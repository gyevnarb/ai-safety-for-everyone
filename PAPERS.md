# AI Safety Paper Survey

This is a collection of all papers retrieved either through our systematic survey on 1 November, 2023, or through snowballing from 12 seed papers (shown below).

### Seed Papers (N: 12)

| Seed Papers |
| --- |
| Amodei, D. <i>et al.</i> Concrete Problems in AI Safety. Preprint at <a href="https://doi.org/10.48550/arXiv.1606.06565">https://doi.org/10.48550/arXiv.1606.06565</a> (2016). |
| Elhage, N. <i>et al.</i> Toy Models of Superposition. Preprint at <a href="https://doi.org/10.48550/arXiv.2209.10652">https://doi.org/10.48550/arXiv.2209.10652</a> (2022). |
| Hadfield-Menell, D., Russell, S. J., Abbeel, P. & Dragan, A. Cooperative Inverse Reinforcement Learning. in <i>Advances in Neural Information Processing Systems</i> vol. 29 (Curran Associates, Inc., 2016). |
| Hendrycks, D. <i>et al.</i> Aligning AI With Shared Human Values. Preprint at <a href="https://doi.org/10.48550/arXiv.2008.02275">https://doi.org/10.48550/arXiv.2008.02275</a> (2023). |
| Hendrycks, D., Carlini, N., Schulman, J. & Steinhardt, J. Unsolved Problems in ML Safety. Preprint at <a href="https://doi.org/10.48550/arXiv.2109.13916">https://doi.org/10.48550/arXiv.2109.13916</a> (2022). |
| Irving, G., Christiano, P. & Amodei, D. AI safety via debate. Preprint at <a href="https://doi.org/10.48550/arXiv.1805.00899">https://doi.org/10.48550/arXiv.1805.00899</a> (2018). |
| Mohseni, S. <i>et al.</i> Taxonomy of Machine Learning Safety: A Survey and Primer. Preprint at <a href="http://arxiv.org/abs/2106.04823">http://arxiv.org/abs/2106.04823</a> (2022). |
| Ng, A. Y. & Russell, S. J. Algorithms for Inverse Reinforcement Learning. in <i>Proceedings of the Seventeenth International Conference on Machine Learning</i> 663–670 (Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2000). |
| Russell, S., Dewey, D. & Tegmark, M. Research Priorities for Robust and Beneficial Artificial Intelligence. Preprint at <a href="https://doi.org/10.48550/arXiv.1602.03506">https://doi.org/10.48550/arXiv.1602.03506</a> (2016). |
| Willers, O., Sudholt, S., Raafatnia, S. & Abrecht, S. Safety Concerns and Mitigation Approaches Regarding the Use of Deep Learning in Safety-Critical Perception Tasks. in <i>Computer Safety, Reliability, and Security. SAFECOMP 2020 Workshops</i> (eds. Casimiro, A., Ortmeier, F., Schoitsch, E., Bitsch, F. & Ferreira, P.) 336–350 (Springer International Publishing, Cham, 2020). doi:<a href="https://doi.org/10.1007/978-3-030-55583-2_25">10.1007/978-3-030-55583-2_25</a>. |
| Xu, H., Zhu, T., Zhang, L., Zhou, W. & Yu, P. S. Machine Unlearning: A Survey. <i>ACM Comput. Surv.</i> <b>56</b>, 9:1-9:36 (2023). |
| Yampolskiy, R. V. Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures. Preprint at <a href="https://doi.org/10.48550/arXiv.1610.07997">https://doi.org/10.48550/arXiv.1610.07997</a> (2016). |


### Papers (N: 383)

| Paper |
| --- |
| Abbeel, P. & Ng, A. Y. Apprenticeship learning via inverse reinforcement learning. in <i>Proceedings of the twenty-first international conference on Machine learning</i> 1 (Association for Computing Machinery, New York, NY, USA, 2004). doi:<a href="https://doi.org/10.1145/1015330.1015430">10.1145/1015330.1015430</a>. |
| Abdelfattah, S., Kasmarik, K. & Hu, J. A robust policy bootstrapping algorithm for multi-objective reinforcement learning in non-stationary environments. <i>Adaptive Behavior</i> <b>28</b>, 273–292 (2020). |
| Abdelkader, H. Towards Robust Production Machine Learning Systems: Managing Dataset Shift. in 1164–1166 (2020). doi:<a href="https://doi.org/10.1145/3324884.3415281">10.1145/3324884.3415281</a>. |
| Abella, J. <i>et al.</i> SAFEXPLAIN: Safe and Explainable Critical Embedded Systems Based on AI. in (2023). |
| Adebayo, J. <i>et al.</i> Sanity Checks for Saliency Maps. in <i>Advances in Neural Information Processing Systems</i> vol. 31 (Curran Associates, Inc., 2018). |
| Adler, R. <i>et al.</i> Hardening of Artificial Neural Networks for Use in Safety-Critical Applications -- A Mapping Study. Preprint at <a href="https://doi.org/10.48550/arXiv.1909.03036">https://doi.org/10.48550/arXiv.1909.03036</a> (2019). |
| Aghakhani, H., Meng, D., Wang, Y.-X., Kruegel, C. & Vigna, G. Bullseye polytope: A scalable clean-label poisoning attack with improved transferability. in 159–178 (2021). doi:<a href="https://doi.org/10.1109/EuroSP51992.2021.00021">10.1109/EuroSP51992.2021.00021</a>. |
| Aksjonov, A. & Kyrki, V. A Safety-Critical Decision-Making and Control Framework Combining Machine-Learning-Based and Rule-Based Algorithms. <i>SAE INTERNATIONAL JOURNAL OF VEHICLE DYNAMICS STABILITY AND NVH</i> <b>7</b>, 287–299 (2023). |
| Al-Maliki, S. <i>et al.</i> Toward Improved Reliability of Deep Learning Based Systems Through Online Relabeling of Potential Adversarial Attacks. <i>IEEE Transactions on Reliability</i> 1–16 (2023) doi:<a href="https://doi.org/10.1109/TR.2023.3298685">10.1109/TR.2023.3298685</a>. |
| Altmann, P. <i>et al.</i> CROP: Towards Distributional-Shift Robust Reinforcement Learning Using Compact Reshaped Observation Processing. in vols 2023-August 3414–3422 (2023). |
| Alufaisan, Y., Kantarcioglu, M. & Zhou, Y. Robust Transparency Against Model Inversion Attacks. <i>IEEE Transactions on Dependable and Secure Computing</i> (2020) doi:<a href="https://doi.org/10.1109/TDSC.2020.3019508">10.1109/TDSC.2020.3019508</a>. |
| Ammanabrolu, P., Jiang, L., Sap, M., Hajishirzi, H. & Choi, Y. Aligning to Social Norms and Values in Interactive Narratives. in <i>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</i> (eds. Carpuat, M., de Marneffe, M.-C. & Meza Ruiz, I. V.) 5994–6017 (Association for Computational Linguistics, Seattle, United States, 2022). doi:<a href="https://doi.org/10.18653/v1/2022.naacl-main.439">10.18653/v1/2022.naacl-main.439</a>. |
| Amodei, D. <i>et al.</i> Concrete Problems in AI Safety. Preprint at <a href="https://doi.org/10.48550/arXiv.1606.06565">https://doi.org/10.48550/arXiv.1606.06565</a> (2016). |
| Andre, D. & Russell, S. J. State abstraction for programmable reinforcement learning agents. in <i>Eighteenth national conference on Artificial intelligence</i> 119–125 (American Association for Artificial Intelligence, USA, 2002). |
| Antikainen, J. <i>et al.</i> A Deployment Model to Extend Ethically Aligned AI Implementation Method ECCOLA. in (eds. Yue, T. & Mirakhorli, M.) 230–235 (2021). doi:<a href="https://doi.org/10.1109/REW53955.2021.00043">10.1109/REW53955.2021.00043</a>. |
| Badea, C. Have a Break from Making Decisions, Have a MARS: The Multi-valued Action Reasoning System. in (eds. Bramer, M. & Stahl, F.) vol. 13652 359–366 (2022). |
| Badea, C. & Artus, G. Morality, Machines, and the Interpretation Problem: A Value-based, Wittgensteinian Approach to Building Moral Agents. in (eds. Bramer, M. & Stahl, F.) vol. 13652 124–137 (2022). |
| Bae, J., Lee, M. & Kim, S. Safe semi-supervised learning using a bayesian neural network. <i>INFORMATION SCIENCES</i> <b>612</b>, 453–464 (2022). |
| Balaji, Y., Sankaranarayanan, S. & Chellappa, R. MetaReg: Towards Domain Generalization using Meta-Regularization. in <i>Advances in Neural Information Processing Systems</i> vol. 31 (Curran Associates, Inc., 2018). |
| Barzamini, H., Shahzad, M., Alhoori, H. & Rahimi, M. A multi-level semantic web for hard-to-specify domain concept, Pedestrian, in ML-based software. <i>REQUIREMENTS ENGINEERING</i> <b>27</b>, 161–182 (2022). |
| Bau, D. <i>et al.</i> Understanding the Role of Individual Units in a Deep Neural Network. <i>Proc. Natl. Acad. Sci. U.S.A.</i> <b>117</b>, 30071–30078 (2020). |
| Bazzan, A. Aligning individual and collective welfare in complex socio-technical systems by combining metaheuristics and reinforcement learning. <i>ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE</i> <b>79</b>, 23–33 (2019). |
| Becker, N. & Waltl, B. Auditing and Testing AI - A Holistic Framework. in (ed. Duffy, V.) vol. 13320 283–292 (2022). |
| Berkenkamp, F., Krause, A. & Schoellig, A. P. Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics. <i>Mach Learn</i> <b>112</b>, 3713–3747 (2023). |
| Bills, S. <i>et al.</i> Language models can explain neurons in language models. Preprint at <a href="https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html">https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html</a> (2023). |
| Boggust, A., Hoover, B., Satyanarayan, A. & Strobelt, H. Shared Interest: Measuring Human-AI Alignment to Identify Recurring Patterns in Model Behavior. in (2022). doi:<a href="https://doi.org/10.1145/3491102.3501965">10.1145/3491102.3501965</a>. |
| Bossens, D. & Bishop, N. Explicit Explore, Exploit, or Escape (E4): near-optimal safety-constrained reinforcement learning in polynomial time. <i>MACHINE LEARNING</i> <b>112</b>, 817–858 (2023). |
| Boudi, Z., Wakrime, A., Toub, M. & Haloua, M. A Deep Reinforcement Learning Framework with Formal Verification. <i>FORMAL ASPECTS OF COMPUTING</i> <b>35</b>, (2023). |
| Bouguelia, M.-R., Nowaczyk, S., Santosh, K. C. & Verikas, A. Agreeing to disagree: active learning with noisy labels without crowdsourcing. <i>Int. J. Mach. Learn. &amp; Cyber.</i> <b>9</b>, 1307–1319 (2018). |
| Bouveyron, C. & Girard, S. Robust supervised classification with mixture models: Learning from data with uncertain labels. <i>Pattern Recognition</i> <b>42</b>, 2649–2658 (2009). |
| Bras, R. L. <i>et al.</i> Adversarial Filters of Dataset Biases. in <i>Proceedings of the 37th International Conference on Machine Learning</i> 1078–1088 (PMLR, 2020). |
| Bricken, T. <i>et al.</i> Towards monosemanticity: Decomposing language models with dictionary learning. <i>Transformer Circuits Thread</i> (2023). |
| Brophy, J. & Lowd, D. Machine Unlearning for Random Forests. in <i>Proceedings of the 38th International Conference on Machine Learning</i> 1092–1104 (PMLR, 2021). |
| Burton, S. & Herd, B. Addressing uncertainty in the safety assurance of machine-learning. <i>FRONTIERS IN COMPUTER SCIENCE</i> <b>5</b>, (2023). |
| Cai, Y. <i>et al.</i> A Discrepancy Aware Framework for Robust Anomaly Detection. <i>IEEE Transactions on Industrial Informatics</i> 1–10 (2023) doi:<a href="https://doi.org/10.1109/TII.2023.3318302">10.1109/TII.2023.3318302</a>. |
| Cammarata, N. <i>et al.</i> Curve detectors. <i>Distill</i> (2020) doi:<a href="https://doi.org/10.23915/distill.00024.003">10.23915/distill.00024.003</a>. |
| Canonico, L. B. & McNeese, N. Flash Crashes in Multi-Agent Systems Using Minority Games and Reinforcement Learning to Test AI Safety. in vols 2019-December 193–204 (2019). |
| Cappozzo, A., Greselin, F. & Murphy, T. B. A robust approach to model-based classification based on trimming and constraints: Semi-supervised learning in presence of outliers and label noise. <i>Advances in Data Analysis and Classification</i> <b>14</b>, 327–354 (2020). |
| Carlan, C., Gauerhof, L., Gallina, B., Burton, S., & IEEE. Automating Safety Argument Change Impact Analysis for Machine Learning Components. in 43–53 (2022). doi:<a href="https://doi.org/10.1109/PRDC55274.2022.00019">10.1109/PRDC55274.2022.00019</a>. |
| Carlini, N., Liu, C., Erlingsson, Ú., Kos, J. & Song, D. The secret sharer: evaluating and testing unintended memorization in neural networks. in <i>Proceedings of the 28th USENIX Conference on Security Symposium</i> 267–284 (USENIX Association, USA, 2019). |
| Carlini, N. & Wagner, D. Towards Evaluating the Robustness of Neural Networks. in 39–57 (IEEE Computer Society, 2017). doi:<a href="https://doi.org/10.1109/SP.2017.49">10.1109/SP.2017.49</a>. |
| Carlson, K. Provably Safe Artificial General Intelligence via Interactive Proofs. <i>PHILOSOPHIES</i> <b>6</b>, (2021). |
| Carlson, K. Safe Artificial General Intelligence via Distributed Ledger Technology. <i>BIG DATA AND COGNITIVE COMPUTING</i> <b>3</b>, (2019). |
| Carlucci, F., Porzi, L., Caputo, B., Ricci, E. & Bulo, S. MultiDIAL: Domain Alignment Layers for (Multisource) Unsupervised Domain Adaptation. <i>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</i> <b>43</b>, 4441–4452 (2021). |
| Chakraborty, S., Verma, E., Sahoo, S. & Datta, J. FairMixRep: Self-supervised Robust Representation Learning for Heterogeneous Data with Fairness constraints. in vols 2020-November 458–463 (2020). |
| Chau, V. T. N. A Robust Self-Organizing Approach to Effectively Clustering Incomplete Data. in 150–155 (2016). doi:<a href="https://doi.org/10.1109/KSE.2015.11">10.1109/KSE.2015.11</a>. |
| Chen, B. <i>et al.</i> Curriculum learning-based fuzzy support vector machine. <i>IEEE Transactions on Fuzzy Systems</i> 1–15 (2023) doi:<a href="https://doi.org/10.1109/TFUZZ.2023.3319170">10.1109/TFUZZ.2023.3319170</a>. |
| Chen, H. <i>et al.</i> A Semi-supervised Deep Learning Model with Consistency Regularization of Augmented Samples for Imbalanced Fault Detection. in 290–295 (2022). doi:<a href="https://doi.org/10.1109/ICRMS55680.2022.9944609">10.1109/ICRMS55680.2022.9944609</a>. |
| Chen, J., Li, Y., Wu, X., Liang, Y. & Jha, S. ATOM: Robustifying Out-of-Distribution Detection Using Outlier Mining. in <i>Machine Learning and Knowledge Discovery in Databases. Research Track</i> (eds. Oliver, N., Pérez-Cruz, F., Kramer, S., Read, J. & Lozano, J. A.) 430–445 (Springer International Publishing, Cham, 2021). doi:<a href="https://doi.org/10.1007/978-3-030-86523-8_26">10.1007/978-3-030-86523-8_26</a>. |
| Chen, K., Lee, Y. & Soh, H. Multi-Modal Mutual Information (MuMMI) Training for Robust Self-Supervised Deep Reinforcement Learning. in vols 2021-May 4274–4280 (2021). |
| Chen, K. <i>et al.</i> Distributionally robust semi-supervised learning for people-centric sensing. in 3321–3328 (2019). |
| Chen, L., Ang, F., Chen, Y. & Wang, W. Robust Federated Learning With Noisy Labeled Data Through Loss Function Correction. <i>IEEE Transactions on Network Science and Engineering</i> <b>10</b>, 1501–1511 (2023). |
| Chen, L.-Y., Chiu, T.-C., Pang, A.-C. & Cheng, L.-C. FedEqual: Defending Model Poisoning Attacks in Heterogeneous Federated Learning. in (2021). doi:<a href="https://doi.org/10.1109/GLOBECOM46510.2021.9685082">10.1109/GLOBECOM46510.2021.9685082</a>. |
| Chen, M. <i>et al.</i> When Machine Unlearning Jeopardizes Privacy. in <i>Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security</i> 896–911 (Association for Computing Machinery, New York, NY, USA, 2021). doi:<a href="https://doi.org/10.1145/3460120.3484756">10.1145/3460120.3484756</a>. |
| Chen, Y., Wang, B., Zhang, Y. & Kuang, J. A Robust Framework for Fixing The Vulnerability of Compressed Distributed Learning. in 733–738 (2023). doi:<a href="https://doi.org/10.1109/CSCWD57460.2023.10152781">10.1109/CSCWD57460.2023.10152781</a>. |
| Chen, Y., Zhang, X., Zhang, K., Wang, M. & Zhu, X. Byzantine-Robust Online and Offline Distributed Reinforcement Learning. in vol. 206 3230–3269 (2023). |
| Chen, Z., Li, G., Pattabiraman, K., DeBardeleben, N., & Assoc Comp Machinery. BinFI: An Efficient Fault Injector for Safety-Critical Machine Learning Systems. in (2019). doi:<a href="https://doi.org/10.1145/3295500.3356177">10.1145/3295500.3356177</a>. |
| Cho, S., Kim, I., Kim, J., Woo, H. & Shin, W. A Maturity Model for Trustworthy AI Software Development. <i>APPLIED SCIENCES-BASEL</i> <b>13</b>, (2023). |
| Cho, S., Byun, J., Kwon, M.-J., Kim, Y. & Kim, C. Adversarial Training With Channel Attention Regularization. in 2996–3000 (2022). doi:<a href="https://doi.org/10.1109/ICIP46576.2022.9897754">10.1109/ICIP46576.2022.9897754</a>. |
| Christiano, P. F. <i>et al.</i> Deep Reinforcement Learning from Human Preferences. in <i>Advances in Neural Information Processing Systems</i> vol. 30 (Curran Associates, Inc., 2017). |
| Christiano, P., Shlegeris, B. & Amodei, D. Supervising strong learners by amplifying weak experts. Preprint at <a href="https://doi.org/10.48550/arXiv.1810.08575">https://doi.org/10.48550/arXiv.1810.08575</a> (2018). |
| Christoffersen, P. J. K., Haupt, A. A. & Hadfield-Menell, D. Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL. in <i>Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems</i> 448–456 (International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 2023). |
| Chundawat, V. S., Tarun, A. K., Mandal, M. & Kankanhalli, M. Zero-Shot Machine Unlearning. <i>Trans. Info. For. Sec.</i> <b>18</b>, 2345–2354 (2023). |
| Cornelissen, N., van Eerdt, R., Schraffenberger, H. & Haselager, W. Reflection machines: increasing meaningful human control over Decision Support Systems. <i>ETHICS AND INFORMATION TECHNOLOGY</i> <b>24</b>, (2022). |
| Costa, E. A., Rebello, C. D. M., Fontana, M., Schnitman, L. & Nogueira, I. B. D. R. A Robust Learning Methodology for Uncertainty-Aware Scientific Machine Learning Models. <i>Mathematics</i> <b>11</b>, (2023). |
| Coston, A. <i>et al.</i> A Validity Perspective on Evaluating the Justified Use of Data-driven Decision-making Algorithms. in 690–704 (2023). doi:<a href="https://doi.org/10.1109/SaTML54575.2023.00050">10.1109/SaTML54575.2023.00050</a>. |
| Cowen-Rivers, A. <i>et al.</i> SAMBA: safe model-based & active reinforcement learning. <i>MACHINE LEARNING</i> <b>111</b>, 173–203 (2022). |
| Curi, S., Bogunovic, I. & Krause, A. Combining Pessimism with Optimism for Robust and Efficient Model-Based Deep Reinforcement Learning. in vol. 139 2254–2264 (2021). |
| D’Acquisto, G. On conflicts between ethical and logical principles in artificial intelligence. <i>AI &amp; SOCIETY</i> <b>35</b>, 895–900 (2020). |
| Dagdanov, R., Durmus, H. & Ure, N. K. Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms. in vols 2023-May 5631–5637 (2023). |
| Dang, L., Hapuarachchi, T., Xiong, K. & Lin, J. Improving Machine Learning Robustness via Adversarial Training. in vols 2023-July (2023). |
| Daniel, C., Viering, M., Metz, J., Kroemer, O. & Peters, J. Active Reward Learning. in <i>Robotics: Science and Systems X</i> (Robotics: Science and Systems Foundation, 2014). doi:<a href="https://doi.org/10.15607/RSS.2014.X.031">10.15607/RSS.2014.X.031</a>. |
| Dey, S. & Lee, S. Multilayered review of safety approaches for machine learning-based systems in the days of AI. <i>JOURNAL OF SYSTEMS AND SOFTWARE</i> <b>176</b>, (2021). |
| Dey, S., Bhat, S., Dasgupta, P. & Dey, S. Imperative Action Masking for Safe Exploration in Reinforcement Learning. in vol. 14127 LNAI 130–142 (2023). |
| Dey, S. & Lee, S.-W. A Multi-layered Collaborative Framework for Evidence-driven Data Requirements Engineering for Machine Learning-based Safety-critical Systems. in 1404–1413 (2023). doi:<a href="https://doi.org/10.1145/3555776.3577647">10.1145/3555776.3577647</a>. |
| Diemert, S., Millet, L., Groves, J. & Joyce, J. Safety Integrity Levels for Artificial Intelligence. in vol. 14182 LNCS 397–409 (2023). |
| Dik, A., Jebari, K. & Ettouhami, A. An Improved Robust Fuzzy Algorithm for Unsupervised Learning. <i>Journal of Intelligent Systems</i> <b>29</b>, 1028–1042 (2020). |
| Ding, D., Wei, X., Yang, Z., Wang, Z. & Jovanović, M. R. Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning. in vol. 211 315–332 (2023). |
| Djeumou, F., Cubuktepe, M., Lennon, C. & Topcu, U. Task-Guided Inverse Reinforcement Learning Under Partial Information. Preprint at <a href="http://arxiv.org/abs/2105.14073">http://arxiv.org/abs/2105.14073</a> (2021). |
| Dobbe, R., Gilbert, T. & Mintz, Y. Hard choices in artificial intelligence. <i>ARTIFICIAL INTELLIGENCE</i> <b>300</b>, (2021). |
| Dong, W., Liu, S. & Sun, S. Safe batch constrained deep reinforcement learning with generative adversarial network. <i>INFORMATION SCIENCES</i> <b>634</b>, 259–270 (2023). |
| Donnelly, J. & Roegiest, A. On Interpretability and Feature Representations: An Analysis of the Sentiment Neuron. in <i>Advances in Information Retrieval</i> (eds. Azzopardi, L. et al.) 795–802 (Springer International Publishing, Cham, 2019). doi:<a href="https://doi.org/10.1007/978-3-030-15712-8_55">10.1007/978-3-030-15712-8_55</a>. |
| Douthwaite, M. & Kelly, T. Establishing Verification and Validation Objectives for Safety-Critical Bayesian Networks. in 302–309 (2017). doi:<a href="https://doi.org/10.1109/ISSREW.2017.60">10.1109/ISSREW.2017.60</a>. |
| Du, X. <i>et al.</i> Robust multi-agent reinforcement learning via Bayesian distributional value estimation. <i>Pattern Recognition</i> <b>145</b>, (2024). |
| Duan, Y., Qi, L., Wang, L., Zhou, L. & Shi, Y. RDA: Reciprocal Distribution Alignment for Robust Semi-supervised Learning. in (eds. Avidan, S., Brostow, G., Cisse, M., Farinella, G. & Hassner, T.) vol. 13690 533–549 (2022). |
| Dunion, M., McInroe, T., Luck, K. S., Hanna, J. P. & Albrecht, S. V. Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning. Preprint at <a href="http://arxiv.org/abs/2207.05480">http://arxiv.org/abs/2207.05480</a> (2023). |
| Dwork, C. & Feldman, V. Privacy-preserving Prediction. in <i>Proceedings of the 31st  Conference On Learning Theory</i> 1693–1702 (PMLR, 2018). |
| Dzambic, M., Dobaj, J., Seidl, M., Macher, G., & ACM. Architectural Patterns for Integrating AI Technology into Safety-Critical Systems. in (2021). doi:<a href="https://doi.org/10.1145/3489449.3490014">10.1145/3489449.3490014</a>. |
| Ebrahimi, M. R., Li, W., Chai, Y., Pacheco, J. & Chen, H. An Adversarial Reinforcement Learning Framework for Robust Machine Learning-based Malware Detection. in vols 2022-November 567–576 (2022). |
| Eilers, D., Burton, S., Roza, F. S. & Roscher, K. Safety Assurance with Ensemble-based Uncertainty Estimation and overlapping alternative Predictions in Reinforcement Learning. in vol. 3381 (2023). |
| El Mhamdi, E. M. & Guerraoui, R. When Neurons Fail. in 1028–1037 (2017). doi:<a href="https://doi.org/10.1109/IPDPS.2017.66">10.1109/IPDPS.2017.66</a>. |
| Elfwing, S. & Seymour, B. Parallel reward and punishment control in humans and robots: safe reinforcement learning using the MaxPain algorithm. in 140–147 (2017). |
| Elhage, N. <i>et al.</i> Softmax linear units. <i>Transformer Circuits Thread</i> (2022). |
| Elhage, N. <i>et al.</i> Toy Models of Superposition. Preprint at <a href="https://doi.org/10.48550/arXiv.2209.10652">https://doi.org/10.48550/arXiv.2209.10652</a> (2022). |
| Elhage, N. <i>et al.</i> A mathematical framework for transformer circuits. Preprint at <a href="https://transformer-circuits.pub/2021/framework/index.html">https://transformer-circuits.pub/2021/framework/index.html</a> (2021). |
| Engstrom, L. <i>et al.</i> Adversarial Robustness as a Prior for Learned Representations. Preprint at <a href="https://doi.org/10.48550/arXiv.1906.00945">https://doi.org/10.48550/arXiv.1906.00945</a> (2019). |
| Evans, O., Stuhlmüller, A. & Goodman, N. D. Learning the preferences of ignorant, inconsistent agents. in <i>Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</i> 323–329 (AAAI Press, Phoenix, Arizona, 2016). |
| Everitt, T., Filan, D., Daswani, M. & Hutter, M. Self-Modification of Policy and Utility Function in Rational Agents. Preprint at <a href="https://doi.org/10.48550/arXiv.1605.03142">https://doi.org/10.48550/arXiv.1605.03142</a> (2016). |
| Everitt, T. & Hutter, M. Avoiding Wireheading with Value Reinforcement Learning. Preprint at <a href="https://doi.org/10.48550/arXiv.1605.03143">https://doi.org/10.48550/arXiv.1605.03143</a> (2016). |
| Fayollas, C., Bonnin, H. & Flebus, O. SafeOps: a concept of continuous safety. in 65–68 (2020). doi:<a href="https://doi.org/10.1109/EDCC51268.2020.00020">10.1109/EDCC51268.2020.00020</a>. |
| Feldkamp, N. & Strassburger, S. From Explainable AI to Explainable Simulation: Using Machine Learning and XAI to understand System Robustness. in 96–106 (2023). doi:<a href="https://doi.org/10.1145/3573900.3591114">10.1145/3573900.3591114</a>. |
| Feng, J. Learning to safely approve updates to machine learning algorithms. in 164–173 (2021). doi:<a href="https://doi.org/10.1145/3450439.3451864">10.1145/3450439.3451864</a>. |
| Fernández, J. <i>et al.</i> Towards functional safety compliance of matrix-matrix multiplication for machine learning-based autonomous systems. <i>JOURNAL OF SYSTEMS ARCHITECTURE</i> <b>121</b>, (2021). |
| Ferry, J., Aïvodji, U., Gambs, S., Huguet, M.-J. & Siala, M. Improving fairness generalization through a sample-robust optimization method. <i>Machine Learning</i> <b>112</b>, 2131–2192 (2023). |
| Fischer, J., Eyberg, C., Werling, M. & Lauer, M. Sampling-based Inverse Reinforcement Learning Algorithms with Safety Constraints. in 791–798 (2021). doi:<a href="https://doi.org/10.1109/IROS51168.2021.9636672">10.1109/IROS51168.2021.9636672</a>. |
| Fraser, B. & Laurito, G. Train Small, Deploy Big: Do Relative World Views Permit Swarm-Safety During Policy Transplantation for Multi-Agent Reinforcement Learning Problems? in (eds. Gallagher, M., Moustafa, N. & Lakshika, E.) vol. 12576 269–280 (2020). |
| Freiesleben, T. & Grote, T. Beyond generalization: a theory of robustness in machine learning. <i>Synthese</i> <b>202</b>, (2023). |
| Fulton, N. & Platzer, A. Verifiably Safe Off-Model Reinforcement Learning. in (eds. Vojnar, T. & Zhang, L.) vol. 11427 413–430 (2019). |
| Gabourie, A., Rostami, M., Pope, P., Kolouri, S. & Kim, K. Learning a Domain-Invariant Embedding for Unsupervised Domain Adaptation Using Class-Conditioned Distribution Alignment. in 352–359 (2019). doi:<a href="https://doi.org/10.1109/allerton.2019.8919960">10.1109/allerton.2019.8919960</a>. |
| Gan, H., Li, Z., Fan, Y. & Luo, Z. Dual Learning-Based Safe Semi-Supervised Learning. <i>IEEE ACCESS</i> <b>6</b>, 2615–2621 (2018). |
| Gan, H., Luo, Z., Meng, M., Ma, Y. & She, Q. A risk degree-based safe semi-supervised learning algorithm. <i>INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS</i> <b>7</b>, 85–94 (2016). |
| Ganin, Y. <i>et al.</i> Domain-Adversarial Training of Neural Networks. in <i>Domain Adaptation in Computer Vision Applications</i> (ed. Csurka, G.) 189–209 (Springer International Publishing, Cham, 2017). doi:<a href="https://doi.org/10.1007/978-3-319-58347-1_10">10.1007/978-3-319-58347-1_10</a>. |
| Gardner, M. <i>et al.</i> Evaluating Models’ Local Decision Boundaries via Contrast Sets. in <i>Findings of the Association for Computational Linguistics: EMNLP 2020</i> (eds. Cohn, T., He, Y. & Liu, Y.) 1307–1323 (Association for Computational Linguistics, Online, 2020). doi:<a href="https://doi.org/10.18653/v1/2020.findings-emnlp.117">10.18653/v1/2020.findings-emnlp.117</a>. |
| Garg, S., Goldwasser, S. & Vasudevan, P. N. Formalizing Data Deletion in the Context of the Right to Be Forgotten. in <i>Advances in Cryptology – EUROCRYPT 2020: 39th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, May 10–14, 2020, Proceedings, Part II</i> 373–402 (Springer-Verlag, Berlin, Heidelberg, 2020). doi:<a href="https://doi.org/10.1007/978-3-030-45724-2_13">10.1007/978-3-030-45724-2_13</a>. |
| Gheraibia, Y., Kabir, S., Aslansefat, K., Sorokos, I. & Papadopoulos, Y. Safety AI: A Novel Approach to Update Safety Models Using Artificial Intelligence. <i>IEEE ACCESS</i> <b>7</b>, 135855–135869 (2019). |
| Gholami, B., Liu, Q., El-Khamy, M. & Lee, J. Multiexpert Adversarial Regularization for Robust and Data-Efficient Deep Supervised Learning. <i>IEEE Access</i> <b>10</b>, 85080–85094 (2022). |
| Gholampour, P. M. & Verma, R. M. Adversarial Robustness of Phishing Email Detection Models. in 67–76 (2023). doi:<a href="https://doi.org/10.1145/3579987.3586567">10.1145/3579987.3586567</a>. |
| Ghosh, A., Mahdavi, H., Tschiatschek, S. & Singla, A. Towards Deployment of Robust Cooperative AI Agents: An Algorithmic Framework for Learning Adaptive Policies. in vols 2020-May 447–455 (2020). |
| Gittens, A., Yener, B. & Yung, M. An Adversarial Perspective on Accuracy, Robustness, Fairness, and Privacy: Multilateral-Tradeoffs in Trustworthy ML. <i>IEEE Access</i> <b>10</b>, 120850–120865 (2022). |
| Goh, G. <i>et al.</i> Multimodal neurons in artificial neural networks. <i>Distill</i> (2021) doi:<a href="https://doi.org/10.23915/distill.00030">10.23915/distill.00030</a>. |
| Golatkar, A., Achille, A. & Soatto, S. Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations. in <i>Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIX</i> 383–398 (Springer-Verlag, Berlin, Heidelberg, 2020). doi:<a href="https://doi.org/10.1007/978-3-030-58526-6_23">10.1007/978-3-030-58526-6_23</a>. |
| Goodfellow, I. J., Shlens, J. & Szegedy, C. Explaining and Harnessing Adversarial Examples. in <i>International Conference on Learning Representations</i> (2015). doi:<a href="https://doi.org/10.48550/arXiv.1412.6572">10.48550/arXiv.1412.6572</a>. |
| Gross, J. <i>et al.</i> Architectural Patterns for Handling Runtime Uncertainty of Data-Driven Models in Safety-Critical Perception. in (eds. Trapp, M., Saglietti, F., Spislander, M. & Bitsch, F.) vol. 13414 284–297 (2022). |
| Guissouma, H., Zink, M., Sax, E., & IEEE. Continuous Safety Assessment of Updated Supervised Learning Models in Shadow Mode. in 301–308 (2023). doi:<a href="https://doi.org/10.1109/ICSA-C57050.2023.00069">10.1109/ICSA-C57050.2023.00069</a>. |
| Guo, L.-Z., Han, T. & Li, Y.-F. Robust semi-supervised representation learning for graph-structured data. in vol. 11441 LNAI 131–143 (2019). |
| Guo, L., Zhang, Z., Jiang, Y., Li, Y. & Zhou, Z. Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data. in (eds. Daume, H. & Singh, A.) vol. 119 (2020). |
| Gupta, V. <i>et al.</i> Adaptive Machine Unlearning. Preprint at <a href="https://doi.org/10.48550/arXiv.2106.04378">https://doi.org/10.48550/arXiv.2106.04378</a> (2021). |
| Gyevnar, B., Wang, C., Lucas, C. G., Cohen, S. B. & Albrecht, S. V. Causal Explanations for Sequential Decision-Making in Multi-Agent Systems. in <i>Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems</i> 771–779 (International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 2024). |
| Hadfield-Menell, D., Russell, S. J., Abbeel, P. & Dragan, A. Cooperative Inverse Reinforcement Learning. in <i>Advances in Neural Information Processing Systems</i> vol. 29 (Curran Associates, Inc., 2016). |
| Harland, H., Dazeley, R., Nakisa, B., Cruz, F. & Vamplew, P. AI apology: interactive multi-objective reinforcement learning for human-aligned AI. <i>NEURAL COMPUTING &amp; APPLICATIONS</i> <b>35</b>, 16917–16930 (2023). |
| He, R., Han, Z., Lu, X., Yin, Y., & IEEE COMP SOC. Safe-Student for Safe Deep Semi-Supervised Learning with Unseen-Class Unlabeled Data. in 14565–14574 (2022). doi:<a href="https://doi.org/10.1109/CVPR52688.2022.01418">10.1109/CVPR52688.2022.01418</a>. |
| He, R., Han, Z., Yang, Y., Yin, Y., & Assoc Advancement Artificial Intelligence. Not All Parameters Should Be Treated Equally: Deep Safe Semi-supervised Learning under Class Distribution Mismatch. in 6874–6883 (2022). |
| Hendrycks, D., Mazeika, M., Kadavath, S. & Song, D. Using self-supervised learning can improve model robustness and uncertainty. in vol. 32 (2019). |
| Hendrycks, D. <i>et al.</i> Aligning AI With Shared Human Values. in (ICLR, 2021). doi:<a href="https://doi.org/10.48550/arXiv.2008.02275">10.48550/arXiv.2008.02275</a>. |
| Hendrycks, D. & Dietterich, T. Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. in <i>International Conference on Learning Representations</i> (arXiv, 2019). doi:<a href="https://doi.org/10.48550/arXiv.1903.12261">10.48550/arXiv.1903.12261</a>. |
| Hendrycks, D. & Gimpel, K. A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. in (2018). doi:<a href="https://doi.org/10.48550/arXiv.1610.02136">10.48550/arXiv.1610.02136</a>. |
| Hendrycks, D. <i>et al.</i> What Would Jiminy Cricket Do? Towards Agents That Behave Morally. in (2021). |
| Henriksson, J. <i>et al.</i> Out-of-Distribution Detection as Support for Autonomous Driving Safety Lifecycle. in vol. 13975 LNCS 233–242 (2023). |
| Hibbard, B. Avoiding Unintended AI Behaviors. in <i>Artificial General Intelligence</i> (eds. Bach, J., Goertzel, B. & Iklé, M.) 107–116 (Springer, Berlin, Heidelberg, 2012). doi:<a href="https://doi.org/10.1007/978-3-642-35506-6_12">10.1007/978-3-642-35506-6_12</a>. |
| Hiett, B. <i>et al.</i> Meaningful Machine Learning Robustness Evaluation in Real-World Machine Learning Enabled System Contexts. in vol. 12276 (2022). |
| Hu, W., Niu, G., Sato, I. & Sugiyama, M. Does distributionally robust supervised learning give robust classifiers? in vol. 5 3220–3249 (2018). |
| Hunt, N. <i>et al.</i> Verifiably Safe Exploration for End-to-End Reinforcement Learning. in (2021). doi:<a href="https://doi.org/10.1145/3447928.3456653">10.1145/3447928.3456653</a>. |
| Ilyas, A. <i>et al.</i> Adversarial Examples Are Not Bugs, They Are Features. in <i>Advances in Neural Information Processing Systems</i> vol. 32 (Curran Associates, Inc., 2019). |
| Irving, G., Christiano, P. & Amodei, D. AI safety via debate. Preprint at <a href="https://doi.org/10.48550/arXiv.1805.00899">https://doi.org/10.48550/arXiv.1805.00899</a> (2018). |
| Izzo, Z., Smart, M. A., Chaudhuri, K. & Zou, J. Approximate Data Deletion from Machine Learning Models. in <i>Proceedings of The 24th International Conference on Artificial Intelligence and Statistics</i> 2008–2016 (PMLR, 2021). |
| Jeddi, A. B., Dehghani, N. L. & Shafieezadeh, A. Memory-augmented Lyapunov-based safe reinforcement learning: end-to-end safety under uncertainty. <i>IEEE Transactions on Artificial Intelligence</i> 1–10 (2023) doi:<a href="https://doi.org/10.1109/TAI.2023.3238700">10.1109/TAI.2023.3238700</a>. |
| Ji, X. & Filieri, A. Probabilistic Counterexample Guidance for Safer Reinforcement Learning. in vol. 14287 LNCS 311–328 (2023). |
| Jia, L.-H. <i>et al.</i> Bidirectional Adaptation for Robust Semi-Supervised Learning with Inconsistent Data Distributions. in vol. 202 14886–14901 (2023). |
| Jin, M. & Lavaei, J. Stability-Certified Reinforcement Learning: A Control-Theoretic Perspective. <i>IEEE ACCESS</i> <b>8</b>, 229086–229100 (2020). |
| Jing, S. & Yang, L. A robust extreme learning machine framework for uncertain data classification. <i>Journal of Supercomputing</i> <b>76</b>, 2390–2416 (2020). |
| Johnson, B. Metacognition for artificial intelligence system safety-An approach to safe and desired behavior. <i>SAFETY SCIENCE</i> <b>151</b>, (2022). |
| Kamalaruban, P. <i>et al.</i> Robust reinforcement learning via adversarial training with Langevin dynamics. in vols 2020-December (2020). |
| Kamm, S., Sahlab, N., Jazdi, N. & Weyrich, M. A Concept for Dynamic and Robust Machine Learning with Context Modeling for Heterogeneous Manufacturing Data. in vol. 118 354–359 (2023). |
| Karpathy, A., Johnson, J. & Fei-Fei, L. Visualizing and Understanding Recurrent Networks. in <i>International Conference on Learning Representations</i> (arXiv, 2016). |
| Kasirzadeh, A. & Evans, C. User Tampering in Reinforcement Learning Recommender Systems. in 58–69 (2023). doi:<a href="https://doi.org/10.1145/3600211.3604669">10.1145/3600211.3604669</a>. |
| Kaufmann, M. <i>et al.</i> Testing Robustness Against Unforeseen Adversaries. Preprint at <a href="https://doi.org/10.48550/arXiv.1908.08016">https://doi.org/10.48550/arXiv.1908.08016</a> (2019). |
| Kaushik, D., Hovy, E. & Lipton, Z. C. Learning the Difference that Makes a Difference with Counterfactually-Augmented Data. in <i>International Conference on Learning Representations</i> (arXiv, 2020). doi:<a href="https://doi.org/10.48550/arXiv.1909.12434">10.48550/arXiv.1909.12434</a>. |
| Kazantzidis, I., Norman, T. J., Du, Y. & Freeman, C. T. How to Train Your Agent: Active Learning from Human Preferences and Justifications in Safety-Critical Environments. in vol. 3 1654–1656 (2022). |
| Khan, W. & Seto, E. A ‘Do No Harm’ Novel Safety Checklist and Research Approach to Determine Whether to Launch an Artificial Intelligence-Based Medical Technology: Introducing the Biological-Psychological, Economic, and Social (BPES) Framework. <i>JOURNAL OF MEDICAL INTERNET RESEARCH</i> <b>25</b>, (2023). |
| Khlaaf, H., Mishkin, P., Achiam, J., Krueger, G. & Brundage, M. A Hazard Analysis Framework for Code Synthesis Large Language Models. Preprint at <a href="https://doi.org/10.48550/arXiv.2207.14157">https://doi.org/10.48550/arXiv.2207.14157</a> (2022). |
| Kim, B. <i>et al.</i> Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV). Preprint at <a href="http://arxiv.org/abs/1711.11279">http://arxiv.org/abs/1711.11279</a> (2018). |
| Kim, H. & Mnih, A. Disentangling by Factorising. Preprint at <a href="https://doi.org/10.48550/arXiv.1802.05983">https://doi.org/10.48550/arXiv.1802.05983</a> (2019). |
| Kim, Y., Hussain, M., Suh, J., Hong, J., & IEEE. Evaluating Correctness of Reinforcement Learning based on Actor-Critic Algorithm. in 320–325 (2022). doi:<a href="https://doi.org/10.1109/ICUFN55119.2022.9829571">10.1109/ICUFN55119.2022.9829571</a>. |
| Kirchheim, K. Towards Deep Anomaly Detection with Structured Knowledge Representations. in vol. 14182 LNCS 382–389 (2023). |
| Kläs, M. & Jöckel, L. A Framework for Building Uncertainty Wrappers for AI/ML-Based Data-Driven Components. in vol. 12235 LNCS 315–327 (2020). |
| Kocak, M., Ramirez, D., Erkip, E. & Shasha, D. SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness. <i>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</i> <b>43</b>, 663–678 (2021). |
| Kohjima, M., Takahashi, M., Toda, H., & IEEE. Censored Markov Decision Processes: A Framework for Safe Reinforcement Learning in Collaboration with External Systems. in 3623–3630 (2020). |
| Kowsher, M., Tahabilder, A. & Murad, S. A. Impact-learning: A robust machine learning algorithm. in 9–13 (2020). doi:<a href="https://doi.org/10.1145/3411174.3411185">10.1145/3411174.3411185</a>. |
| Kshetry, N. & Varshney, L. Safety in the Face of Unknown Unknowns: Algorithm Fusion in Data-driven Engineering Systems. in 8162–8166 (2019). |
| Kuleshov, V. & Schrijvers, O. Inverse Game Theory: Learning Utilities in Succinct Games. in <i>Web and Internet Economics</i> (eds. Markakis, E. & Schäfer, G.) 413–427 (Springer, Berlin, Heidelberg, 2015). doi:<a href="https://doi.org/10.1007/978-3-662-48995-6_30">10.1007/978-3-662-48995-6_30</a>. |
| Kundu, S. & Basu, K. Detecting Functional Safety Violations in Online AI Accelerators. in (eds. Savino, A., Rech, P., DiCarlo, S. & Gizopoulos, D.) (2022). doi:<a href="https://doi.org/10.1109/IOLTS56730.2022.9897702">10.1109/IOLTS56730.2022.9897702</a>. |
| Kurd, Z. & Kelly, T. Establishing Safety Criteria for Artificial Neural Networks. in <i>Knowledge-Based Intelligent Information and Engineering Systems</i> (eds. Palade, V., Howlett, R. J. & Jain, L.) 163–169 (Springer, Berlin, Heidelberg, 2003). doi:<a href="https://doi.org/10.1007/978-3-540-45224-9_24">10.1007/978-3-540-45224-9_24</a>. |
| Lage, I., Ross, A. S., Kim, B., Gershman, S. J. & Doshi-Velez, F. Human-in-the-Loop Interpretability Prior. Preprint at <a href="http://arxiv.org/abs/1805.11571">http://arxiv.org/abs/1805.11571</a> (2018). |
| Lagrave, P.-Y. A Principal Component Analysis Approach for Embedding Local Symmetries into Deep Learning Algorithms. in vol. 12235 LNCS 302–314 (2020). |
| Lakkaraju, H., Kamar, E., Caruana, R. & Horvitz, E. Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration. Preprint at <a href="http://arxiv.org/abs/1610.09064">http://arxiv.org/abs/1610.09064</a> (2016). |
| Le, D. C. & Zincir-Heywood, N. Anomaly Detection for Insider Threats Using Unsupervised Ensembles. <i>IEEE Transactions on Network and Service Management</i> <b>18</b>, 1152–1164 (2021). |
| Le, N. D.-H. & Tran, M.-T. A robust unsupervised feature learning framework using spatial boosting networks. in vol. 2 507–512 (2013). |
| Lecerf, U., Yemdji-Tchassi, C., Aubert, S., Michiardi, P., & ACM. Automatically Learning Fallback Strategies with Model-Free Reinforcement Learning in Safety-Critical Driving Scenarios. in 209–215 (2022). doi:<a href="https://doi.org/10.1145/3529399.3529432">10.1145/3529399.3529432</a>. |
| Lee, K., Yang, H. & Oh, S. Adversarial Training on Joint Energy Based Model for Robust Classification and Out-of-Distribution Detection. in 17–21 (2020). doi:<a href="https://doi.org/10.23919/iccas50221.2020.9268406">10.23919/iccas50221.2020.9268406</a>. |
| Leike, J. <i>et al.</i> Scalable agent alignment via reward modeling: a research direction. Preprint at <a href="http://arxiv.org/abs/1811.07871">http://arxiv.org/abs/1811.07871</a> (2018). |
| Lendasse, A. <i>et al.</i> Extreme learning machine: A robust modeling technique? yes! in vol. 7902 LNCS 17–35 (2013). |
| Leonardos, S. & Piliouras, G. Exploration-exploitation in multi-agent learning: Catastrophe theory meets game theory. <i>Artificial Intelligence</i> <b>304</b>, 103653 (2022). |
| Li, J., Ding, J., Chai, T., Lewis, F. L. & Jagannathan, S. Adaptive Interleaved Reinforcement Learning: Robust Stability of Affine Nonlinear Systems with Unknown Uncertainty. <i>IEEE Transactions on Neural Networks and Learning Systems</i> <b>33</b>, 270–280 (2022). |
| Li, K., Patel, O., Viégas, F., Pfister, H. & Wattenberg, M. Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. Preprint at <a href="https://doi.org/10.48550/arXiv.2306.03341">https://doi.org/10.48550/arXiv.2306.03341</a> (2023). |
| Li, W. & Chai, Y. Assessing and Enhancing Adversarial Robustness of Predictive Analytics: An Empirically Tested Design Framework. <i>Journal of Management Information Systems</i> <b>39</b>, 542–572 (2022). |
| Li, W. & Wang, Y. A robust supervised subspace learning approach for output-relevant prediction and detection against outliers. <i>Journal of Process Control</i> <b>106</b>, 184–194 (2021). |
| Li, W. <i>et al.</i> Supervised contrastive learning for robust text adversarial training. <i>Neural Computing and Applications</i> <b>35</b>, 7357–7368 (2023). |
| Li, Y. <i>et al.</i> Curricular Robust Reinforcement Learning via GAN-Based Perturbation Through Continuously Scheduled Task Sequence. <i>Tsinghua Science and Technology</i> <b>28</b>, 27–38 (2023). |
| Li, Y., Wang, Y., Chen, Z. & Zou, R. Bayesian robust multi-extreme learning machine. <i>Knowledge-Based Systems</i> <b>210</b>, (2020). |
| Li, Y., Zheng, W. & Zheng, Z. Deep Robust Reinforcement Learning for Practical Algorithmic Trading. <i>IEEE Access</i> <b>7</b>, 108014–108021 (2019). |
| Li, Z., Zeng, J., Thirugnanam, A. & Sreenath, K. Bridging Model-based Safety and Model-free Reinforcement Learning through System Identification of Low Dimensional Linear Models. in (eds. Hauser, K., Shell, D. & Huang, S.) (2022). |
| Liang, Y., Sun, Y., Zheng, R. & Huang, F. Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning. in vol. 35 (2022). |
| Linard, A., Bueno, M. L. P., Bucur, D. & Stoelinga, M. Induction of fault trees through Bayesian networks. in 910–917 (2020). doi:<a href="https://doi.org/10.3850/978-981-11-2724-3_0596-cd">10.3850/978-981-11-2724-3_0596-cd</a>. |
| Liu, L., Huang, P. & Min, F. Safe Multi-view Co-training for Semi-supervised Regression. in (eds. Huang, J. et al.) 56–65 (2022). doi:<a href="https://doi.org/10.1109/DSAA54385.2022.10032437">10.1109/DSAA54385.2022.10032437</a>. |
| Liu, P. <i>et al.</i> Safe Reinforcement Learning of Dynamic High-Dimensional Robotic Tasks: Navigation, Manipulation, Interaction. in 9449–9456 (2023). doi:<a href="https://doi.org/10.1109/ICRA48891.2023.10161548">10.1109/ICRA48891.2023.10161548</a>. |
| Liu, S. & Sun, S. Safe Offline Reinforcement Learning Through Hierarchical Policies. in (eds. Gama, J. et al.) vol. 13281 380–391 (2022). |
| Liu, X. & Tsaftaris, S. A. Have You Forgotten? A Method to Assess if Machine Learning Models Have Forgotten Data. in <i>Medical Image Computing and Computer Assisted Intervention – MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I</i> 95–105 (Springer-Verlag, Berlin, Heidelberg, 2020). doi:<a href="https://doi.org/10.1007/978-3-030-59710-8_10">10.1007/978-3-030-59710-8_10</a>. |
| Liu, Y. <i>et al.</i> Backdoor Defense with Machine Unlearning. in <i>IEEE INFOCOM 2022 - IEEE Conference on Computer Communications</i> 280–289 (IEEE Press, London, United Kingdom, 2022). doi:<a href="https://doi.org/10.1109/INFOCOM48880.2022.9796974">10.1109/INFOCOM48880.2022.9796974</a>. |
| Liu, Y., Chen, X., Liu, C. & Song, D. Delving into Transferable Adversarial Examples and Black-box Attacks. Preprint at <a href="https://doi.org/10.48550/arXiv.1611.02770">https://doi.org/10.48550/arXiv.1611.02770</a> (2017). |
| Liu, Z. <i>et al.</i> Towards Robust and Safe Reinforcement Learning with Benign Off-policy Data. in vol. 202 22249–22265 (2023). |
| Liu, Z. <i>et al.</i> A Novel Composite Graph Neural Network. <i>IEEE Transactions on Neural Networks and Learning Systems</i> 1–15 (2023) doi:<a href="https://doi.org/10.1109/TNNLS.2023.3268766">10.1109/TNNLS.2023.3268766</a>. |
| Lu, Y., Squillante, M. S. & Wu, C. W. A family of robust stochastic operators for reinforcement learning. in vol. 32 (2019). |
| Lu, Z., Lu, S., Tang, X. & Wu, J. Robust and verifiable privacy federated learning. <i>IEEE Transactions on Artificial Intelligence</i> 1–14 (2023) doi:<a href="https://doi.org/10.1109/TAI.2023.3309273">10.1109/TAI.2023.3309273</a>. |
| Luo, Y. & Ma, T. Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations. in (eds. Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P. & Vaughan, J.) vol. 34 (2021). |
| Lv, S. <i>et al.</i> A Deep Safe Reinforcement Learning Approach for Mapless Navigation. in 1520–1525 (2021). doi:<a href="https://doi.org/10.1109/ROBIO54168.2021.9739251">10.1109/ROBIO54168.2021.9739251</a>. |
| Ma, J., Yang, L. & Sun, Q. Adaptive robust learning framework for twin support vector machine classification. <i>Knowledge-Based Systems</i> <b>211</b>, (2021). |
| Ma, Y., Li, L., Huang, X. & Wang, S. Robust Support Vector Machine using Least Median Loss Penalty. in vol. 44 11208–11213 (2011). |
| Ma, Y., Shen, A., Bastani, O., Jayaraman, D., & Assoc Advancement Artificial Intelligence. Conservative and Adaptive Penalty for Model-Based Safe Reinforcement Learning. in 5404–5412 (2022). |
| Machida, F. N-version machine learning models for safety critical systems. in 48–51 (2019). doi:<a href="https://doi.org/10.1109/DSN-W.2019.00017">10.1109/DSN-W.2019.00017</a>. |
| Madry, A., Makelov, A., Schmidt, L., Tsipras, D. & Vladu, A. Towards Deep Learning Models Resistant to Adversarial Attacks. Preprint at <a href="https://doi.org/10.48550/arXiv.1706.06083">https://doi.org/10.48550/arXiv.1706.06083</a> (2019). |
| Marchant, N. G., Rubinstein, B. I. P. & Alfeld, S. Hard to Forget: Poisoning Attacks on Certified Machine Unlearning. Preprint at <a href="https://doi.org/10.48550/arXiv.2109.08266">https://doi.org/10.48550/arXiv.2109.08266</a> (2022). |
| Maskara, P. Developing safer AI-concepts from economics to the rescue. <i>AI &amp; SOCIETY</i> (2023) doi:<a href="https://doi.org/10.1007/s00146-023-01778">10.1007/s00146-023-01778</a>. |
| Massiani, P., Heim, S., Solowjow, F. & Trimpe, S. Safe Value Functions. <i>IEEE TRANSACTIONS ON AUTOMATIC CONTROL</i> <b>68</b>, 2743–2757 (2023). |
| Mauri, L., Apolloni, B. & Damiani, E. Robust ML model ensembles via risk-driven anti-clustering of training data. <i>Information Sciences</i> <b>633</b>, 122–140 (2023). |
| Mayer, C., Paul, M. & Timofte, R. Adversarial feature distribution alignment for semi-supervised learning. <i>COMPUTER VISION AND IMAGE UNDERSTANDING</i> <b>202</b>, (2021). |
| Mazouchi, M., Nageshrao, S. & Modares, H. Conflict-Aware Safe Reinforcement Learning: A Meta-Cognitive Learning Framework. <i>IEEE-CAA JOURNAL OF AUTOMATICA SINICA</i> <b>9</b>, 466–481 (2022). |
| Meinke, A. & Hein, M. Towards neural networks that provably know when they don’t know. Preprint at <a href="https://doi.org/10.48550/arXiv.1909.12180">https://doi.org/10.48550/arXiv.1909.12180</a> (2020). |
| Meng, J., Zhu, F., Ge, Y. & Zhao, P. Integrating safety constraints into adversarial training for robust deep reinforcement learning. <i>INFORMATION SCIENCES</i> <b>619</b>, 310–323 (2023). |
| Meng, Y. <i>et al.</i> Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training. in 10367–10378 (2021). |
| Millan-Arias, C. C., Fernandes, B. J. T., Cruz, F., Dazeley, R. & Fernandes, S. A Robust Approach for Continuous Interactive Actor-Critic Algorithms. <i>IEEE Access</i> <b>9</b>, 104242–104260 (2021). |
| Mindom, P., Nikanjam, A., Khomh, F., Mullins, J., & IEEE COMP SOC. On Assessing The Safety of Reinforcement Learning algorithms Using Formal Methods. in 260–269 (2021). doi:<a href="https://doi.org/10.1109/QRS54544.2021.00037">10.1109/QRS54544.2021.00037</a>. |
| Mo, Z., Di, X. & Shi, R. Robust Data Sampling in Machine Learning: A Game-Theoretic Framework for Training and Validation Data Selection. <i>Games</i> <b>14</b>, (2023). |
| Modares, A., Sadati, N., Esmaeili, B., Yaghmaie, F. & Modares, H. Safe Reinforcement Learning via a Model-Free Safety Certifier. <i>IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</i> (2023) doi:<a href="https://doi.org/10.1109/TNNLS.2023.3264815">10.1109/TNNLS.2023.3264815</a>. |
| Moradi, M. <i>et al.</i> Exploring Fault Parameter Space Using Reinforcement Learning-based Fault Injection. in 102–109 (2020). doi:<a href="https://doi.org/10.1109/DSN-W50199.2020.00028">10.1109/DSN-W50199.2020.00028</a>. |
| Morcos, A. S., Barrett, D. G. T., Rabinowitz, N. C. & Botvinick, M. On the importance of single directions for generalization. Preprint at <a href="http://arxiv.org/abs/1803.06959">http://arxiv.org/abs/1803.06959</a> (2018). |
| Motokawa, Y. & Sugawara, T. Distributed Multi-Agent Deep Reinforcement Learning for Robust Coordination against Noise. in vols 2022-July (2022). |
| Murphy, B., Talukdar, P. & Mitchell, T. Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding. in <i>Proceedings of COLING 2012</i> (eds. Kay, M. & Boitet, C.) 1933–1950 (The COLING 2012 Organizing Committee, Mumbai, India, 2012). |
| Murugesan, A., Moghadamfalahi, M. & Chattopadhyay, A. Formal Methods Assisted Training of Safe Reinforcement Learning Agents. in (eds. Badger, J. & Rozier, K.) vol. 11460 333–340 (2019). |
| Nanda, N., Chan, L., Lieberum, T., Smith, J. & Steinhardt, J. Progress measures for grokking via mechanistic interpretability. Preprint at <a href="https://doi.org/10.48550/arXiv.2301.05217">https://doi.org/10.48550/arXiv.2301.05217</a> (2023). |
| Ng, A. Y., Harada, D. & Russell, S. J. Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping. in <i>Proceedings of the Sixteenth International Conference on Machine Learning</i> 278–287 (Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1999). |
| Ng, A. Y. & Russell, S. J. Algorithms for Inverse Reinforcement Learning. in <i>Proceedings of the Seventeenth International Conference on Machine Learning</i> 663–670 (Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2000). |
| Nguyen, A., Yosinski, J. & Clune, J. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. in <i>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i> 427–436 (IEEE, Boston, MA, USA, 2015). doi:<a href="https://doi.org/10.1109/CVPR.2015.7298640">10.1109/CVPR.2015.7298640</a>. |
| Nicolae, M.-I., Sebban, M., Habrard, A., Gaussier, E. & Amini, M.-R. Algorithmic robustness for semi-supervised (ε, γ, τ)-good metric learning. in vol. 9489 253–263 (2015). |
| Nie, Z., Lin, Y., Yan, M., Cao, Y. & Ning, S. An Adversarial Training Method for Improving Model Robustness in Unsupervised Domain Adaptation. in vol. 12817 LNAI 3–13 (2021). |
| Nilsen, M., Nygaard, T. & Ellefsen, K. Reward tampering and evolutionary computation: a study of concrete AI-safety problems using evolutionary algorithms. <i>GENETIC PROGRAMMING AND EVOLVABLE MACHINES</i> <b>24</b>, (2023). |
| Ning, X. <i>et al.</i> Discovering Robust Convolutional Architecture at Targeted Capacity: A Multi-Shot Approach. Preprint at <a href="https://doi.org/10.48550/arXiv.2012.11835">https://doi.org/10.48550/arXiv.2012.11835</a> (2021). |
| Noorani, E. & Baras, J. S. Embracing Risk in Reinforcement Learning: The Connection between Risk-Sensitive Exponential and Distributionally Robust Criteria. in vols 2022-June 2703–2708 (2022). |
| Okawa, Y., Sasaki, T. & Iwane, H. Automatic Exploration Process Adjustment for Safe Reinforcement Learning with Joint Chance Constraint Satisfaction. in vol. 53 1588–1595 (2020). |
| Olah, C. <i>et al.</i> The building blocks of interpretability. <i>Distill</i> (2018) doi:<a href="https://doi.org/10.23915/distill.00010">10.23915/distill.00010</a>. |
| Olsson, C. <i>et al.</i> In-context learning and induction heads. Preprint at <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html</a> (2022). |
| Osinenko, P., Yaremenko, G., Malaniya, G. & Bolychev, A. An Actor-Critic Framework for Online Control With Environment Stability Guarantee. <i>IEEE ACCESS</i> <b>11</b>, 89188–89204 (2023). |
| Otte, C. Interpretable semi-parametric regression models with defined error bounds. <i>NEUROCOMPUTING</i> <b>143</b>, 1–6 (2014). |
| Ouyang, L. <i>et al.</i> Training language models to follow instructions with human feedback. Preprint at <a href="https://doi.org/10.48550/arXiv.2203.02155">https://doi.org/10.48550/arXiv.2203.02155</a> (2022). |
| Padakandla, S. Data Efficient Safe Reinforcement Learning Algorithm. in (2022). |
| Panaganti, K., Xu, Z., Kalathil, D. & Ghavamzadeh, M. Robust Reinforcement Learning using Offline Data. in vol. 35 (2022). |
| Perales Gomez, A. L. <i>et al.</i> A Methodology for Evaluating the Robustness of Anomaly Detectors to Adversarial Attacks in Industrial Scenarios. <i>IEEE Access</i> <b>10</b>, 124582–124594 (2022). |
| Perkins, T. & Barto, A. Lyapunov design for safe reinforcement learning. <i>JOURNAL OF MACHINE LEARNING RESEARCH</i> <b>3</b>, 803–832 (2003). |
| Peschl, M., Zgonnikov, A., Oliehoek, F. A. & Siebert, L. C. MORAL: Aligning AI with Human Norms through Multi-Objective Reinforced Active Learning. Preprint at <a href="http://arxiv.org/abs/2201.00012">http://arxiv.org/abs/2201.00012</a> (2021). |
| Pfrommer, J., Poyer, M. & Kiroriwal, S. Reduce the Handicap: Performance Estimation for AI Systems Safety Certification. in (eds. Dorksen, H. et al.) (2023). doi:<a href="https://doi.org/10.1109/INDIN51400.2023.10218017">10.1109/INDIN51400.2023.10218017</a>. |
| Phan, D. <i>et al.</i> Neural Simplex Architecture. in (eds. Lee, R., Jha, S., Mavridou, A. & Giannakopoulou, D.) vol. 12229 97–114 (2020). |
| Picardi, C., Paterson, C., Hawkins, R., Calinescu, R. & Habli, I. Assurance argument patterns and processes for machine learning in safety-related systems. in vol. 2560 23–30 (2020). |
| Picot, M. <i>et al.</i> Adversarial Robustness Via Fisher-Rao Regularization. <i>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</i> <b>45</b>, 2698–2710 (2023). |
| Pinar Ozisik, A. & Thomas, P. S. Security analysis of safe and seldonian reinforcement learning algorithms. in vols 2020-December (2020). |
| Pistono, F. & Yampolskiy, R. V. Unethical Research: How to Create a Malevolent Artificial Intelligence. Preprint at <a href="https://doi.org/10.48550/arXiv.1605.02817">https://doi.org/10.48550/arXiv.1605.02817</a> (2016). |
| Poenaru-Olaru, L. <i>et al.</i> Retrain AI Systems Responsibly! Use Sustainable Concept Drift Adaptation Techniques. in 17–18 (2023). doi:<a href="https://doi.org/10.1109/GREENS59328.2023.00009">10.1109/GREENS59328.2023.00009</a>. |
| Polymenakos, K., Abate, A., Roberts, S., & Assoc Comp Machinery. Safe Policy Search Using Gaussian Process Models. in 1565–1573 (2019). |
| Prakash, B., Khatwani, M., Waytowich, N. & Mohsenin, T. Improving safety in reinforcement learning using model-based architectures and human intervention. in 50–55 (2019). |
| Pulawski, S., Dam, H. K. & Ghose, A. BDI-Dojo: Developing robust BDI agents in evolving adversarial environments. in 257–262 (2021). doi:<a href="https://doi.org/10.1109/ACSOS-C52956.2021.00066">10.1109/ACSOS-C52956.2021.00066</a>. |
| Qi, Y., Conmy, P. R., Huang, W., Zhao, X. & Huang, X. A Hierarchical HAZOP-Like Safety Analysis for Learning-Enabled Systems. in vol. 3215 (2022). |
| Raghu, M., Gilmer, J., Yosinski, J. & Sohl-Dickstein, J. SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability. in <i>Advances in Neural Information Processing Systems</i> vol. 30 (Curran Associates, Inc., 2017). |
| Ramakrishnan, R., Kamar, E., Dey, D., Shah, J. & Horvitz, E. Discovering Blind Spots in Reinforcement Learning. in 1017–1025 (2018). |
| Ramirez-Padron, R., Mederos, B. & Gonzalez, A. J. Robust weighted Gaussian processes. <i>Computational Statistics</i> <b>36</b>, 347–373 (2021). |
| Ranjbar, A., Hornauer, S., Fredriksson, J., Yu, S. & Chan, C. Safety Monitoring of Neural Networks Using Unsupervised Feature Learning and Novelty Estimation. <i>IEEE TRANSACTIONS ON INTELLIGENT VEHICLES</i> <b>7</b>, 711–721 (2022). |
| Rausch, A., Sedeh, A. & Zhang, M. Autoencoder-Based Semantic Novelty Detection: Towards Dependable AI-Based Systems. <i>APPLIED SCIENCES-BASEL</i> <b>11</b>, (2021). |
| Ravichandran, B., Gandhe, A., Smith, R. & Mehra, R. Robust automatic target recognition using learning classifier systems. <i>Information Fusion</i> <b>8</b>, 252–265 (2007). |
| Ray, A., Achiam, J. & Amodei, D. Benchmarking Safe Exploration in Deep Reinforcement Learning. (2023). |
| Reimann, L. & Kniesel-Wünsche, G. Safe-DS: A Domain Specific Language to Make Data Science Safe. in 72–77 (2023). doi:<a href="https://doi.org/10.1109/ICSE-NIER58687.2023.00019">10.1109/ICSE-NIER58687.2023.00019</a>. |
| Riley, J., Calinescu, R., Paterson, C., Kudenko, D. & Banks, A. Assured Multi-agent Reinforcement Learning with Robust Agent-Interaction Adaptability. in vol. 309 87–97 (2022). |
| Ring, M. & Orseau, L. Delusion, Survival, and Intelligent Agents. in <i>Artificial General Intelligence</i> (eds. Schmidhuber, J., Thórisson, K. R. & Looks, M.) 11–20 (Springer, Berlin, Heidelberg, 2011). doi:<a href="https://doi.org/10.1007/978-3-642-22887-2_2">10.1007/978-3-642-22887-2_2</a>. |
| Robinette, P. K., Hamilton, N. P. & Johnson, T. T. Self-Preserving Genetic Algorithms for Safe Learning in Discrete Action Spaces. in 110–119 (2023). doi:<a href="https://doi.org/10.1145/3576841.3585936">10.1145/3576841.3585936</a>. |
| Rodriguez-Soto, M., Rodriguez-Aguilar, J. A. & Lopez-Sanchez, M. Building Multi-Agent Environments with Theoretical Guarantees on the Learning of Ethical Policies. in (2022). |
| Sallami, M., Ibn Khedher, M., Trabelsi, A., Kerboua-Benlarbi, S. & Bettebghor, D. Safety and Robustness of Deep Neural Networks Object Recognition Under Generic Attacks. in (eds. Gedeon, T., Wong, K. & Lee, M.) vol. 1142 274–286 (2019). |
| Samarasinghe, D. Counterfactual learning in enhancing resilience in autonomous agent systems. <i>Frontiers in Artificial Intelligence</i> <b>6</b>, (2023). |
| Sanneman, L. & Shah, J. Transparent Value Alignment. in <i>Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction</i> 557–560 (ACM, Stockholm Sweden, 2023). doi:<a href="https://doi.org/10.1145/3568294.3580147">10.1145/3568294.3580147</a>. |
| Saunders, W. <i>et al.</i> Self-critiquing models for assisting human evaluators. Preprint at <a href="https://doi.org/10.48550/arXiv.2206.05802">https://doi.org/10.48550/arXiv.2206.05802</a> (2022). |
| Schmid, T. Batch-like online learning for more robust hybrid artificial intelligence: Deconstruction as a machine learning process. in vol. 2846 (2021). |
| Schott, L., Hajri, H. & Lamprier, S. Improving Robustness of Deep Reinforcement Learning Agents: Environment Attack based on the Critic Network. in vols 2022-July (2022). |
| Schubert, L., Voss, C., Cammarata, N., Goh, G. & Olah, C. High-low frequency detectors. <i>Distill</i> (2021) doi:<a href="https://doi.org/10.23915/distill.00024.005">10.23915/distill.00024.005</a>. |
| Schumeg, B., Marotta, F. & Werner, B. Proposed V-Model for Verification, Validation, and Safety Activities for Artificial Intelligence. in 61–66 (2023). doi:<a href="https://doi.org/10.1109/ICAA58325.2023.00017">10.1109/ICAA58325.2023.00017</a>. |
| Seifi, F., Azizi, M. J. & Akhavan Niaki, S. T. A data-driven robust optimization algorithm for black-box cases: An application to hyper-parameter optimization of machine learning algorithms. <i>Computers and Industrial Engineering</i> <b>160</b>, (2021). |
| Selim, M., Alanwar, A., El-Kharashi, M., Abbas, H. & Johansson, K. Safe Reinforcement Learning using Data-Driven Predictive Control. in (2022). doi:<a href="https://doi.org/10.1109/ICCSPA55860.2022.10018994">10.1109/ICCSPA55860.2022.10018994</a>. |
| Serrano, S. & Smith, N. A. Is Attention Interpretable? Preprint at <a href="http://arxiv.org/abs/1906.03731">http://arxiv.org/abs/1906.03731</a> (2019). |
| Sezener, C. Inferring Human Values for Safe AGI Design. in (eds. Bieger, J., Goertzel, B. & Potapov, A.) vol. 9205 152–155 (2015). |
| Shaham, U., Yamada, Y. & Negahban, S. Understanding adversarial training: Increasing local stability of supervised models through robust optimization. <i>Neurocomputing</i> <b>307</b>, 195–204 (2018). |
| Shen, T., Geng, X. & Jiang, D. Social Norms-Grounded Machine Ethics in Complex Narrative Situation. in <i>Proceedings of the 29th International Conference on Computational Linguistics</i> (eds. Calzolari, N. et al.) 1333–1343 (International Committee on Computational Linguistics, Gyeongju, Republic of Korea, 2022). |
| Sheth, A., Gaur, M., Roy, K., Venkataraman, R. & Khandelwal, V. Process Knowledge-Infused AI: Toward User-Level Explainability, Interpretability, and Safety. <i>IEEE INTERNET COMPUTING</i> <b>26</b>, 76–84 (2022). |
| Shi, M., Liang, Y. & Shroff, N. A Near-Optimal Algorithm for Safe Reinforcement Learning Under Instantaneous Hard Constraints. in vol. 202 31243–31268 (2023). |
| Simão, T. D., Jansen, N. & Spaan, M. T. J. AlwaysSafe: Reinforcement learning without safety constraint violations during training. in vol. 2 1214–1223 (2021). |
| Singh, H., Joshi, S., Doshi-Velez, F. & Lakkaraju, H. Towards Robust Off-Policy Evaluation via Human Inputs. in 686–699 (2022). doi:<a href="https://doi.org/10.1145/3514094.3534198">10.1145/3514094.3534198</a>. |
| Singh, R., Zhang, Q. & Chen, Y. Improving Robustness via Risk Averse Distributional Reinforcement Learning. in vol. 120 958–968 (2020). |
| Soares, N., Fallenstein, B., Yudkowsky, E. & Armstrong, S. Corrigibility. in <i>Artificial Intelligence and Ethics: Papers from the 2015 AAAI Workshop</i> (Austin, Texas, USA, 2015). |
| Solaiman, I. & Dennison, C. Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets. Preprint at (2023). |
| Soremekun, E., Udeshi, S. & Chattopadhyay, S. Towards Backdoor Attacks and Defense in Robust Machine Learning Models. <i>Computers and Security</i> <b>127</b>, (2023). |
| Sotala, K. & Yampolskiy, R. V. Responses to catastrophic AGI risk: a survey. <i>Phys. Scr.</i> <b>90</b>, 018001 (2014). |
| Spears, D. F. Assuring the Behavior of Adaptive Agents. in <i>Agent Technology from a Formal Perspective</i> (eds. Rouff, C. A., Hinchey, M., Rash, J., Truszkowski, W. & Gordon-Spears, D.) 227–257 (Springer, London, 2006). doi:<a href="https://doi.org/10.1007/1-84628-271-3_8">10.1007/1-84628-271-3_8</a>. |
| Stanko, S. & Macek, K. Risk-averse Distributional Reinforcement Learning: A CVaR Optimization Approach. in (eds. Merelo, J., Garibaldi, J., Barranco, A., Madani, K. & Warwick, K.) 412–423 (2019). doi:<a href="https://doi.org/10.5220/0008175604120423">10.5220/0008175604120423</a>. |
| Strawn, K. J., Ayanian, N. & Lindemann, L. Conformal Predictive Safety Filter for RL Controllers in Dynamic Environments. <i>IEEE Robotics and Automation Letters</i> 1–8 (2023) doi:<a href="https://doi.org/10.1109/LRA.2023.3322644">10.1109/LRA.2023.3322644</a>. |
| Subramanian, A., Pruthi, D., Jhamtani, H., Berg-Kirkpatrick, T. & Hovy, E. SPINE: SParse Interpretable Neural Embeddings. Preprint at <a href="https://doi.org/10.48550/arXiv.1711.08792">https://doi.org/10.48550/arXiv.1711.08792</a> (2017). |
| Sumers, T. R., Hawkins, R. D., Ho, M. K., Griffiths, T. L. & Hadfield-Menell, D. How to talk so AI will learn: Instructions, descriptions, and autonomy. Preprint at <a href="http://arxiv.org/abs/2206.07870">http://arxiv.org/abs/2206.07870</a> (2022). |
| Sumers, T. R., Ho, M. K., Hawkins, R. D., Narasimhan, K. & Griffiths, T. L. Learning Rewards from Linguistic Feedback. Preprint at <a href="https://doi.org/10.48550/arXiv.2009.14715">https://doi.org/10.48550/arXiv.2009.14715</a> (2021). |
| Sun, H., Wu, K., Wang, T. & Wang, W. H. Towards Fair and Robust Classification. in 356–376 (2022). doi:<a href="https://doi.org/10.1109/EuroSP53844.2022.00030">10.1109/EuroSP53844.2022.00030</a>. |
| Swaminathan, A. & Joachims, T. Batch Learning from Logged Bandit Feedback through Counterfactual Risk Minimization. <i>Journal of Machine Learning Research</i> <b>16</b>, 1731–1755 (2015). |
| Tan, K. L., Esfandiari, Y., Lee, X. Y. & Sarkar, S. Robustifying Reinforcement Learning Agents via Action Space Adversarial Training. in vols 2020-July 3959–3964 (2020). |
| Tavakoli, M., Agostinelli, F. & Baldi, P. SPLASH: Learnable Activation Functions for Improving Accuracy and Adversarial Robustness. Preprint at <a href="https://doi.org/10.48550/arXiv.2006.08947">https://doi.org/10.48550/arXiv.2006.08947</a> (2020). |
| Tay, E., Gan, O. & Ho, W. A study on real-time artificial intelligence. in (ed. Rauch, H.) 109–114 (1998). |
| Taylor, J., Yudkowsky, E., LaVictoire, P. & Critch, A. Alignment for advanced machine learning systems. in <i>Ethics of Artificial Intelligence</i> 342–382 (2020). doi:<a href="https://doi.org/10.1093/oso/9780190905033.003.0013">10.1093/oso/9780190905033.003.0013</a>. |
| Terra, A., Riaz, H., Raizer, K., Hata, A. & Inam, R. Safety vs. Efficiency: AI-Based Risk Mitigation in Collaborative Robotics. in 151–160 (2020). doi:<a href="https://doi.org/10.1109/iccar49639.2020.9108037">10.1109/iccar49639.2020.9108037</a>. |
| Thudi, A., Jia, H., Shumailov, I. & Papernot, N. On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning. in 4007–4022 (2022). |
| Torralba, A. & Efros, A. A. Unbiased look at dataset bias. in <i>CVPR 2011</i> 1521–1528 (2011). doi:<a href="https://doi.org/10.1109/CVPR.2011.5995347">10.1109/CVPR.2011.5995347</a>. |
| Treacy, S. Mechanisms and Constraints Underpinning Ethically Aligned Artificial Intelligence Systems: An Exploration of key Performance Areas. in (eds. Matos, F., Salavisa, I. & Serrao, C.) 183–191 (2021). doi:<a href="https://doi.org/10.34190/EAIR.21.005">10.34190/EAIR.21.005</a>. |
| Tuna, O. F., Catak, F. O. & Eskil, M. T. TENET: a new hybrid network architecture for adversarial defense. <i>International Journal of Information Security</i> <b>22</b>, 987–1004 (2023). |
| Turchetta, M., Berkenkamp, F. & Krause, A. Safe Exploration for Interactive Machine Learning. in (eds. Wallach, H. et al.) vol. 32 (2019). |
| Umbrello, S. Beneficial Artificial Intelligence Coordination by Means of a Value Sensitive Design Approach. <i>BIG DATA AND COGNITIVE COMPUTING</i> <b>3</b>, (2019). |
| Vaccari, I., Carlevaro, A., Narteni, S., Cambiaso, E. & Mongelli, M. eXplainable and Reliable Against Adversarial Machine Learning in Data Analytics. <i>IEEE ACCESS</i> <b>10</b>, 83949–83970 (2022). |
| Vakkuri, V., Kemell, K., Jantunen, M., Halme, E. & Abrahamsson, P. ECCOLA - A method for implementing ethically aligned AI systems. <i>JOURNAL OF SYSTEMS AND SOFTWARE</i> <b>182</b>, (2021). |
| Vempaty, A., Kailkhura, B. & Varshney, P. K. Robust fusion of unreliable data sources using error-correcting output codes. in <i>Data Fusion in Wireless Sensor Networks</i> 291–311 (2019). doi:<a href="https://doi.org/10.1049/PBCE117E_ch12">10.1049/PBCE117E_ch12</a>. |
| Vivek, B. S., Mopuri, K. R. & Babu, R. V. Gray-box adversarial training. in vol. 11219 LNCS 213–228 (2018). |
| Wabersich, K., Hewing, L., Carron, A. & Zeilinger, M. Probabilistic Model Predictive Safety Certification for Learning-Based Control. <i>IEEE TRANSACTIONS ON AUTOMATIC CONTROL</i> <b>67</b>, 176–188 (2022). |
| Wada, Y., Su, S., Kumagai, W. & Kanamori, T. Robust label prediction via label propagation and geodesic k-nearest neighbor in online semi-supervised learning. <i>IEICE Transactions on Information and Systems</i> <b>E102D</b>, 1537–1545 (2019). |
| Wang, H., Huang, Z., Wu, X. & Xing, E. Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation. in 1846–1856 (2022). doi:<a href="https://doi.org/10.1145/3534678.3539438">10.1145/3534678.3539438</a>. |
| Wang, H., He, Z., Lipton, Z. C. & Xing, E. P. Learning Robust Representations by Projecting Superficial Statistics Out. Preprint at <a href="http://arxiv.org/abs/1903.06256">http://arxiv.org/abs/1903.06256</a> (2019). |
| Wang, J. T. & Jia, R. Data Banzhaf: A Robust Data Valuation Framework for Machine Learning. in vol. 206 6388–6421 (2023). |
| Wang, K. & Guo, P. A Robust Automated Machine Learning System with Pseudoinverse Learning. <i>Cognitive Computation</i> <b>13</b>, 724–735 (2021). |
| Wang, K., Variengien, A., Conmy, A., Shlegeris, B. & Steinhardt, J. Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small. Preprint at <a href="https://doi.org/10.48550/arXiv.2211.00593">https://doi.org/10.48550/arXiv.2211.00593</a> (2022). |
| Wang, M. <i>et al.</i> Joint Adversarial Domain Adaptation With Structural Graph Alignment. <i>IEEE Transactions on Network Science and Engineering</i> 1–10 (2023) doi:<a href="https://doi.org/10.1109/TNSE.2023.3302574">10.1109/TNSE.2023.3302574</a>. |
| Wang, S. <i>et al.</i> Adversarial Robustness of Deep Reinforcement Learning Based Dynamic Recommender Systems. <i>Frontiers in Big Data</i> <b>5</b>, (2022). |
| Wang, W. <i>et al.</i> A Safe and Self-Recoverable Reinforcement Learning Framework for Autonomous Robots. in (eds. Li, Z. & Sun, J.) 3878–3883 (2022). |
| Wang, Y., Wang, Y., Zhou, Y., Velasquez, A. & Zou, S. Data-Driven Robust Multi-Agent Reinforcement Learning. in vols 2022-August (2022). |
| Wang, Y. & Zou, S. Online Robust Reinforcement Learning with Model Uncertainty. in vol. 9 7193–7206 (2021). |
| Wang, Z., Chen, C. & Dong, D. A Dirichlet Process Mixture of Robust Task Models for Scalable Lifelong Reinforcement Learning. <i>IEEE Transactions on Cybernetics</i> 1–12 (2022) doi:<a href="https://doi.org/10.1109/TCYB.2022.3170485">10.1109/TCYB.2022.3170485</a>. |
| Ward, F. R. & Habli, I. An Assurance Case Pattern for the Interpretability of Machine Learning in Safety-Critical Systems. in vol. 12235 LNCS 395–407 (2020). |
| Wei, C.-Y., Dann, C. & Zimmert, J. A Model Selection Approach for Corruption Robust Reinforcement Learning. in vol. 167 1043–1096 (2022). |
| Weld, D. & Etzioni, O. The First Law of Robotics. in <i>Safety and Security in Multiagent Systems</i> (eds. Barley, M. et al.) 90–100 (Springer, Berlin, Heidelberg, 2009). doi:<a href="https://doi.org/10.1007/978-3-642-04879-1_7">10.1007/978-3-642-04879-1_7</a>. |
| Wen, M. & Topcu, U. Constrained Cross-Entropy Method for Safe Reinforcement Learning. <i>IEEE TRANSACTIONS ON AUTOMATIC CONTROL</i> <b>66</b>, 3123–3137 (2021). |
| Wen, Z., Xu, H. & Ying, S. JSMix: a holistic algorithm for learning with label noise. <i>Neural Computing and Applications</i> <b>35</b>, 1519–1533 (2023). |
| Wong, E. & Kolter, Z. Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope. in <i>Proceedings of the 35th International Conference on Machine Learning</i> 5286–5295 (PMLR, 2018). |
| Wozniak, E., Cârlan, C., Acar-Celik, E. & Putzer, H. J. A Safety Case Pattern for Systems with Machine Learning Components. in vol. 12235 LNCS 370–382 (2020). |
| Wu, G., Hashemi, M. & Srinivasa, C. PUMA: Performance Unchanged Model Augmentation for Training Data Removal. Preprint at <a href="https://doi.org/10.48550/arXiv.2203.00846">https://doi.org/10.48550/arXiv.2203.00846</a> (2022). |
| Wu, J., Zhou, Y., Yang, H., Huang, Z. & Lv, C. Human-Guided Reinforcement Learning With Sim-to-Real Transfer for Autonomous Navigation. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i> 1–15 (2023) doi:<a href="https://doi.org/10.1109/TPAMI.2023.3314762">10.1109/TPAMI.2023.3314762</a>. |
| Wu, T. <i>et al.</i> DiscrimLoss: A Universal Loss for Hard Samples and Incorrect Samples Discrimination. <i>IEEE Transactions on Multimedia</i> 1–12 (2023) doi:<a href="https://doi.org/10.1109/TMM.2023.3290477">10.1109/TMM.2023.3290477</a>. |
| Wu, Y., Dobriban, E. & Davidson, S. DeltaGrad: Rapid retraining of machine learning models. in <i>Proceedings of the 37th International Conference on Machine Learning</i> 10355–10366 (PMLR, 2020). |
| Xin, R., Kar, S. & Khan, U. A. Decentralized Stochastic Optimization and Machine Learning: A Unified Variance-Reduction Framework for Robust Performance and Fast Convergence. <i>IEEE Signal Processing Magazine</i> <b>37</b>, 102–113 (2020). |
| Xiong, P. <i>et al.</i> Towards a robust and trustworthy machine learning system development: An engineering perspective. <i>Journal of Information Security and Applications</i> <b>65</b>, (2022). |
| Xiong, Z., Agarwal, I. & Jagannathan, S. HiSaRL: A Hierarchical Framework for Safe Reinforcement Learning. in vol. 3087 (2022). |
| Xu, M. <i>et al.</i> Group Distributionally Robust Reinforcement Learning with Hierarchical Latent Variables. in vol. 206 2677–2703 (2023). |
| Xu, Y. <i>et al.</i> Look Before You Leap: Safe Model-Based Reinforcement Learning with Human Intervention. in vol. 164 332–341 (2021). |
| Yampolskiy, R. & Fox, J. Safety Engineering for Artificial General Intelligence. <i>Topoi</i> (2012) doi:<a href="https://doi.org/10.1007/s11245-012-9128-9">10.1007/s11245-012-9128-9</a>. |
| Yampolskiy, R. V. Leakproofing the Singularity: Artificial intelligence confinement problem. <i>Journal of Consciousness Studies</i> <b>19</b>, 194–214 (2012). |
| Yampolskiy, R. V. Artificial Intelligence Safety Engineering: Why Machine Ethics Is a Wrong Approach. in <i>Philosophy and Theory of Artificial Intelligence</i> (ed. Müller, V. C.) 389–396 (Springer, Berlin, Heidelberg, 2013). doi:<a href="https://doi.org/10.1007/978-3-642-31674-6_29">10.1007/978-3-642-31674-6_29</a>. |
| Yampolskiy, R. V. Utility function security in artificially intelligent agents. <i>Journal of Experimental &amp; Theoretical Artificial Intelligence</i> <b>26</b>, 373–389 (2014). |
| Yampolskiy, R. V. Taxonomy of Pathways to Dangerous AI. Preprint at <a href="https://doi.org/10.48550/arXiv.1511.03246">https://doi.org/10.48550/arXiv.1511.03246</a> (2015). |
| Yang, H. <i>et al.</i> Learning From Noisy Labels Via Dynamic Loss Thresholding. <i>IEEE Transactions on Knowledge and Data Engineering</i> 1–14 (2023) doi:<a href="https://doi.org/10.1109/TKDE.2023.3313604">10.1109/TKDE.2023.3313604</a>. |
| Yang, Q., Simão, T. D., Jansen, N., Tindemans, S. H. & Spaan, M. T. J. Training and Transferring Safe Policies in Reinforcement Learning. in (2022). |
| Yang, Q., Simao, T., Tindemans, S. & Spaan, M. Safety-constrained reinforcement learning with a distributional safety critic. <i>MACHINE LEARNING</i> <b>112</b>, 859–887 (2023). |
| Yang, X. & He, H. Adaptive Critic Designs for Event-Triggered Robust Control of Nonlinear Systems with Unknown Dynamics. <i>IEEE Transactions on Cybernetics</i> <b>49</b>, 2255–2267 (2019). |
| Yang, Y., Jiang, Y., Liu, Y., Chen, J. & Li, S. Model-Free Safe Reinforcement Learning Through Neural Barrier Certificate. <i>IEEE ROBOTICS AND AUTOMATION LETTERS</i> <b>8</b>, 1295–1302 (2023). |
| Yasuda, T., Araki, K. & Ohkura, K. Improving the robustness of instance-based reinforcement learning robots by metalearning. <i>Journal of Advanced Computational Intelligence and Intelligent Informatics</i> <b>15</b>, 1065–1072 (2011). |
| Yavas, M., Kumbasar, T. & Ure, N. A Real-World Reinforcement Learning Framework for Safe and Human-Like Tactical Decision-Making. <i>IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS</i> (2023) doi:<a href="https://doi.org/10.1109/TITS.2023.3292981">10.1109/TITS.2023.3292981</a>. |
| Yu, J., Gehring, C., Schäfer, F. & Anandkumar, A. Robust Reinforcement Learning: A Constrained Game-theoretic Approach. in vol. 144 1242–1254 (2021). |
| Yu, S., Zhu, Z., Liu, B., Jain, A. K. & Zhou, J. Robust Unsupervised Domain Adaptation from A Corrupted Source. in vols 2022-November 1299–1304 (2022). |
| Yuan, Y., Li, Y., Zhu, Z., Li, R. & Gu, X. Joint Domain Adaptation Based on Adversarial Dynamic Parameter Learning. <i>IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE</i> <b>5</b>, 714–723 (2021). |
| Zanella-Béguelin, S. <i>et al.</i> Analyzing Information Leakage of Updates to Natural Language Models. in <i>Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security</i> 363–375 (2020). doi:<a href="https://doi.org/10.1145/3372297.3417880">10.1145/3372297.3417880</a>. |
| Zemel, R., Wu, Y., Swersky, K., Pitassi, T. & Dwork, C. Learning Fair Representations. in <i>Proceedings of the 30th International Conference on Machine Learning</i> 325–333 (PMLR, 2013). |
| Zha, L., Ma, K., Li, G., Fang, Q. & Hu, X. A robust double-parallel extreme learning machine based on an improved M-estimation algorithm. <i>Advanced Engineering Informatics</i> <b>52</b>, (2022). |
| Zhang, B., Titov, I. & Sennrich, R. Sparse Attention with Linear Units. Preprint at <a href="https://doi.org/10.48550/arXiv.2104.07012">https://doi.org/10.48550/arXiv.2104.07012</a> (2021). |
| Zhang, C. <i>et al.</i> Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness. in vol. 13690 LNCS 725–742 (2022). |
| Zhang, F., Chen, S., Hong, Z., Shan, B. & Xu, Q. A Robust Extreme Learning Machine Based on Adaptive Loss Function for Regression Modeling. <i>Neural Processing Letters</i> (2023) doi:<a href="https://doi.org/10.1007/s11063-023-11340-y">10.1007/s11063-023-11340-y</a>. |
| Zhang, H. <i>et al.</i> Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models. Preprint at <a href="https://doi.org/10.48550/arXiv.2311.04378">https://doi.org/10.48550/arXiv.2311.04378</a> (2023). |
| Zhang, J., Tao, H., Luo, T. & Hou, C. Safe incomplete label distribution learning. <i>PATTERN RECOGNITION</i> <b>125</b>, (2022). |
| Zhang, L. <i>et al.</i> Evaluating Model-Free Reinforcement Learning toward Safety-Critical Tasks. in vol. 37 15313–15321 (2023). |
| Zhang, X., Wu, L., Li, Z. & Liu, H. A Robust Method to Measure the Global Feature Importance of Complex Prediction Models. <i>IEEE Access</i> <b>9</b>, 7885–7893 (2021). |
| Zhang, Y., Gao, T. & Mi, Q. A Robust Offline Reinforcement Learning Algorithm Based on Behavior Regularization Methods. in 150–154 (2022). doi:<a href="https://doi.org/10.1109/IAICT55358.2022.9887435">10.1109/IAICT55358.2022.9887435</a>. |
| Zhang, Y. <i>et al.</i> Barrier Lyapunov Function-Based Safe Reinforcement Learning for Autonomous Vehicles With Optimized Backstepping. <i>IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</i> (2022) doi:<a href="https://doi.org/10.1109/TNNLS.2022.3186528">10.1109/TNNLS.2022.3186528</a>. |
| Zhao, J., Liu, N. & Malov, A. Safe semi-supervised classification algorithm combined with active learning sampling strategy. <i>JOURNAL OF INTELLIGENT &amp; FUZZY SYSTEMS</i> <b>35</b>, 4001–4010 (2018). |
| Zhao, W., He, T. & Liu, C. Probabilistic Safeguard for Reinforcement Learning Using Safety Index Guided Gaussian Process Models. in vol. 211 783–796 (2023). |
| Zhao, X., Huang, W., Schewe, S., Dong, Y. & Huang, X. Detecting Operational Adversarial Examples for Reliable Deep Learning. in 5–6 (2021). doi:<a href="https://doi.org/10.1109/DSN-S52858.2021.00013">10.1109/DSN-S52858.2021.00013</a>. |
| Zhou, Z., Liu, G. & Zhou, M. A Robust Mean-Field Actor-Critic Reinforcement Learning Against Adversarial Perturbations on Agent States. <i>IEEE Transactions on Neural Networks and Learning Systems</i> 1–12 (2023) doi:<a href="https://doi.org/10.1109/TNNLS.2023.3278715">10.1109/TNNLS.2023.3278715</a>. |
| Zhou, Z., Oguz, O., Leibold, M. & Buss, M. Learning a Low-Dimensional Representation of a Safe Region for Safe Reinforcement Learning on Dynamical Systems. <i>IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</i> <b>34</b>, 2513–2527 (2023). |
| Zhu, J., Wang, L. & Han, X. Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software Deployment. in (2022). doi:<a href="https://doi.org/10.1145/3551349.3556906">10.1145/3551349.3556906</a>. |
| Zhu, X., Kang, S., Chen, J., & IEEE. A Contact-Safe Reinforcement Learning Framework for Contact-Rich Robot Manipulation. in 2476–2482 (2022). doi:<a href="https://doi.org/10.1109/IROS47612.2022.9981185">10.1109/IROS47612.2022.9981185</a>. |
| Zhuo, J., Wang, S., Zhang, W. & Huang, Q. Deep Unsupervised Convolutional Domain Adaptation. in 261–269 (2017). doi:<a href="https://doi.org/10.1145/3123266.3123292">10.1145/3123266.3123292</a>. |
| Zou, A. <i>et al.</i> Universal and Transferable Adversarial Attacks on Aligned Language Models. Preprint at <a href="https://doi.org/10.48550/arXiv.2307.15043">https://doi.org/10.48550/arXiv.2307.15043</a> (2023). |
| Zwane, S. <i>et al.</i> Safe Trajectory Sampling in Model-Based Reinforcement Learning. in vols 2023-August (2023). |

